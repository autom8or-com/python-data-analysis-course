{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "time_series_title"
      },
      "source": [
        "# Week 7: Advanced EDA with Business Intelligence - Part 3: Time Series Patterns in Order Data\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this session, you will be able to:\n",
        "- Conduct comprehensive time series analysis for business intelligence\n",
        "- Identify seasonal patterns, trends, and cyclical behaviors in order data\n",
        "- Apply advanced temporal analytics for forecasting and planning\n",
        "- Create time-based customer and product insights\n",
        "- Build predictive models for business forecasting\n",
        "\n",
        "## Business Context\n",
        "Completing our advanced EDA trilogy, we focus on **temporal patterns and time series analysis** to understand:\n",
        "- **Seasonal Business Cycles**: When do customers buy the most?\n",
        "- **Growth Trends**: How is the business evolving over time?\n",
        "- **Demand Forecasting**: What can we expect in future periods?\n",
        "- **Operational Planning**: How to optimize inventory and staffing?\n",
        "\n",
        "**Key Business Questions:**\n",
        "- What are the seasonal patterns in our sales data?\n",
        "- How do different customer segments behave over time?\n",
        "- Which time periods drive the highest revenue and why?\n",
        "- How can we forecast future demand and plan accordingly?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section_timeseries"
      },
      "source": [
        "## 1. Environment Setup and Secure Data Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "timeseries_setup"
      },
      "outputs": [],
      "source": [
        "# Essential imports for time series analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Time series specific libraries\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from scipy import stats\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "# Advanced analytics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Interactive visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.offline as pyo\n",
        "pyo.init_notebook_mode(connected=True)\n",
        "\n",
        "# Database connection (secure)\n",
        "import os\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Display and plotting settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (15, 8)\n",
        "\n",
        "print(\"✅ Environment setup complete for time series analysis!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "secure_timeseries_connection"
      },
      "outputs": [],
      "source": [
        "# Secure Database Connection Using Environment Variables\n",
        "# Best practice: Never expose credentials in code\n",
        "\n",
        "# Set up environment variables securely\n",
        "if 'SUPABASE_DB_HOST' not in os.environ:\n",
        "    # For educational purposes only - in production, set these at system level\n",
        "    os.environ['SUPABASE_DB_HOST'] = 'aws-0-us-east-1.pooler.supabase.com'\n",
        "    os.environ['SUPABASE_DB_PORT'] = '6543'\n",
        "    os.environ['SUPABASE_DB_NAME'] = 'postgres'\n",
        "    os.environ['SUPABASE_DB_USER'] = 'postgres.pzykoxdiwsyclwfqfiii'\n",
        "    os.environ['SUPABASE_DB_PASSWORD'] = 'L3tMeQuery123!'\n",
        "\n",
        "# Construct secure database URL\n",
        "DATABASE_URL = f\"postgresql://{os.environ['SUPABASE_DB_USER']}:{os.environ['SUPABASE_DB_PASSWORD']}@{os.environ['SUPABASE_DB_HOST']}:{os.environ['SUPABASE_DB_PORT']}/{os.environ['SUPABASE_DB_NAME']}\"\n",
        "\n",
        "# Create database engine\n",
        "engine = create_engine(DATABASE_URL)\n",
        "\n",
        "# Test connection\n",
        "try:\n",
        "    with engine.connect() as conn:\n",
        "        result = conn.execute(\"SELECT 1 as connection_test\")\n",
        "        print(\"✅ Secure database connection established!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Connection failed: {e}\")\n",
        "\n",
        "print(\"🔒 Security Note: Database credentials loaded from environment variables\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timeseries_data_loading"
      },
      "source": [
        "## 2. Comprehensive Time Series Data Loading\n",
        "\n",
        "Load temporal data optimized for time series analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_timeseries_data"
      },
      "outputs": [],
      "source": [
        "# Comprehensive Time Series Dataset Loading\n",
        "print(\"🔄 Loading comprehensive time series dataset...\")\n",
        "\n",
        "# Time series optimized query\n",
        "timeseries_query = \"\"\"\n",
        "WITH daily_orders AS (\n",
        "    SELECT \n",
        "        DATE(o.order_purchase_timestamp) as order_date,\n",
        "        o.order_id,\n",
        "        o.customer_id,\n",
        "        o.order_purchase_timestamp,\n",
        "        EXTRACT(YEAR FROM o.order_purchase_timestamp) as order_year,\n",
        "        EXTRACT(MONTH FROM o.order_purchase_timestamp) as order_month,\n",
        "        EXTRACT(DAY FROM o.order_purchase_timestamp) as order_day,\n",
        "        EXTRACT(DOW FROM o.order_purchase_timestamp) as order_dow,\n",
        "        EXTRACT(WEEK FROM o.order_purchase_timestamp) as order_week,\n",
        "        EXTRACT(QUARTER FROM o.order_purchase_timestamp) as order_quarter,\n",
        "        EXTRACT(HOUR FROM o.order_purchase_timestamp) as order_hour,\n",
        "        \n",
        "        c.customer_state,\n",
        "        c.customer_city,\n",
        "        \n",
        "        oi.product_id,\n",
        "        oi.price,\n",
        "        oi.freight_value,\n",
        "        (oi.price + oi.freight_value) as total_order_value,\n",
        "        \n",
        "        p.product_category_name,\n",
        "        COALESCE(pt.product_category_name_english, p.product_category_name) as category_english,\n",
        "        \n",
        "        r.review_score,\n",
        "        \n",
        "        -- Calculate days since business start for trend analysis\n",
        "        DATE_PART('day', o.order_purchase_timestamp - \n",
        "                 (SELECT MIN(order_purchase_timestamp) FROM olist_sales_data_set.olist_orders_dataset)) as days_since_start\n",
        "        \n",
        "    FROM olist_sales_data_set.olist_orders_dataset o\n",
        "    JOIN olist_sales_data_set.olist_customers_dataset c ON o.customer_id = c.customer_id\n",
        "    JOIN olist_sales_data_set.olist_order_items_dataset oi ON o.order_id = oi.order_id\n",
        "    JOIN olist_sales_data_set.olist_products_dataset p ON oi.product_id = p.product_id\n",
        "    LEFT JOIN olist_sales_data_set.product_category_name_translation pt \n",
        "        ON p.product_category_name = pt.product_category_name\n",
        "    LEFT JOIN olist_sales_data_set.olist_order_reviews_dataset r ON o.order_id = r.order_id\n",
        "    \n",
        "    WHERE o.order_status = 'delivered'\n",
        "    AND oi.price > 0\n",
        "    AND o.order_purchase_timestamp IS NOT NULL\n",
        ")\n",
        "SELECT * FROM daily_orders\n",
        "ORDER BY order_purchase_timestamp\n",
        "LIMIT 35000;\n",
        "\"\"\"\n",
        "\n",
        "# Load the time series data\n",
        "ts_df = pd.read_sql(timeseries_query, engine)\n",
        "\n",
        "# Data preprocessing for time series analysis\n",
        "ts_df['order_purchase_timestamp'] = pd.to_datetime(ts_df['order_purchase_timestamp'])\n",
        "ts_df['order_date'] = pd.to_datetime(ts_df['order_date'])\n",
        "ts_df['category_clean'] = ts_df['category_english'].fillna('Unknown').str.title()\n",
        "\n",
        "# Create additional time-based features\n",
        "ts_df['is_weekend'] = ts_df['order_dow'].isin([0, 6])  # Sunday=0, Saturday=6\n",
        "ts_df['month_name'] = ts_df['order_purchase_timestamp'].dt.month_name()\n",
        "ts_df['day_name'] = ts_df['order_purchase_timestamp'].dt.day_name()\n",
        "ts_df['is_holiday_season'] = ts_df['order_month'].isin([11, 12])  # Nov-Dec\n",
        "\n",
        "# Define business periods\n",
        "def get_business_period(hour):\n",
        "    if 6 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 18:\n",
        "        return 'Afternoon'\n",
        "    elif 18 <= hour < 22:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "ts_df['business_period'] = ts_df['order_hour'].apply(get_business_period)\n",
        "\n",
        "# Calculate analysis period\n",
        "analysis_start = ts_df['order_date'].min()\n",
        "analysis_end = ts_df['order_date'].max()\n",
        "total_days = (analysis_end - analysis_start).days\n",
        "\n",
        "print(f\"✅ Time series dataset loaded successfully!\")\n",
        "print(f\"   📊 Total records: {len(ts_df):,}\")\n",
        "print(f\"   📅 Analysis period: {analysis_start.date()} to {analysis_end.date()} ({total_days} days)\")\n",
        "print(f\"   🛒 Unique orders: {ts_df['order_id'].nunique():,}\")\n",
        "print(f\"   👥 Unique customers: {ts_df['customer_id'].nunique():,}\")\n",
        "print(f\"   🏷️ Product categories: {ts_df['category_clean'].nunique()}\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\n📋 Sample Time Series Data:\")\n",
        "display(ts_df[['order_date', 'order_hour', 'day_name', 'category_clean', \n",
        "              'total_order_value', 'business_period']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "time_aggregation_section"
      },
      "source": [
        "## 3. Time Series Aggregation and Basic Patterns\n",
        "\n",
        "Create different temporal aggregations to identify patterns at various time scales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "time_aggregation_analysis"
      },
      "outputs": [],
      "source": [
        "# Time Series Aggregation and Pattern Analysis\n",
        "print(\"📊 Time Series Aggregation and Pattern Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def create_time_aggregations(data):\n",
        "    \"\"\"\n",
        "    Create multiple time-based aggregations for analysis\n",
        "    \"\"\"\n",
        "    aggregations = {}\n",
        "    \n",
        "    # Daily aggregation\n",
        "    daily_agg = data.groupby('order_date').agg({\n",
        "        'order_id': 'nunique',\n",
        "        'customer_id': 'nunique',\n",
        "        'total_order_value': ['sum', 'mean', 'count'],\n",
        "        'review_score': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Flatten column names\n",
        "    daily_agg.columns = ['date', 'unique_orders', 'unique_customers', \n",
        "                        'total_revenue', 'avg_order_value', 'total_items', 'avg_review']\n",
        "    \n",
        "    # Weekly aggregation\n",
        "    weekly_agg = data.groupby([data['order_purchase_timestamp'].dt.to_period('W')]).agg({\n",
        "        'order_id': 'nunique',\n",
        "        'customer_id': 'nunique',\n",
        "        'total_order_value': ['sum', 'mean'],\n",
        "        'review_score': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    weekly_agg.columns = ['week', 'unique_orders', 'unique_customers', \n",
        "                         'total_revenue', 'avg_order_value', 'avg_review']\n",
        "    weekly_agg['week_start'] = weekly_agg['week'].dt.start_time\n",
        "    \n",
        "    # Monthly aggregation\n",
        "    monthly_agg = data.groupby([data['order_purchase_timestamp'].dt.to_period('M')]).agg({\n",
        "        'order_id': 'nunique',\n",
        "        'customer_id': 'nunique',\n",
        "        'total_order_value': ['sum', 'mean'],\n",
        "        'review_score': 'mean',\n",
        "        'category_clean': 'nunique'\n",
        "    }).reset_index()\n",
        "    \n",
        "    monthly_agg.columns = ['month', 'unique_orders', 'unique_customers', \n",
        "                          'total_revenue', 'avg_order_value', 'avg_review', 'categories_sold']\n",
        "    monthly_agg['month_start'] = monthly_agg['month'].dt.start_time\n",
        "    \n",
        "    # Hourly patterns\n",
        "    hourly_agg = data.groupby('order_hour').agg({\n",
        "        'order_id': 'nunique',\n",
        "        'total_order_value': ['sum', 'mean'],\n",
        "        'customer_id': 'nunique'\n",
        "    }).reset_index()\n",
        "    \n",
        "    hourly_agg.columns = ['hour', 'unique_orders', 'total_revenue', 'avg_order_value', 'unique_customers']\n",
        "    \n",
        "    # Day of week patterns\n",
        "    dow_agg = data.groupby(['order_dow', 'day_name']).agg({\n",
        "        'order_id': 'nunique',\n",
        "        'total_order_value': ['sum', 'mean'],\n",
        "        'customer_id': 'nunique'\n",
        "    }).reset_index()\n",
        "    \n",
        "    dow_agg.columns = ['dow_num', 'day_name', 'unique_orders', 'total_revenue', 'avg_order_value', 'unique_customers']\n",
        "    \n",
        "    aggregations['daily'] = daily_agg\n",
        "    aggregations['weekly'] = weekly_agg\n",
        "    aggregations['monthly'] = monthly_agg\n",
        "    aggregations['hourly'] = hourly_agg\n",
        "    aggregations['day_of_week'] = dow_agg\n",
        "    \n",
        "    return aggregations\n",
        "\n",
        "# Create time aggregations\n",
        "time_aggregations = create_time_aggregations(ts_df)\n",
        "\n",
        "# Display aggregation summaries\n",
        "print(f\"📈 Time Series Aggregation Summary:\")\n",
        "for period, agg_data in time_aggregations.items():\n",
        "    if period not in ['hourly', 'day_of_week']:\n",
        "        print(f\"   • {period.title()}: {len(agg_data)} periods\")\n",
        "        print(f\"     Revenue range: R$ {agg_data['total_revenue'].min():.2f} - R$ {agg_data['total_revenue'].max():.2f}\")\n",
        "        print(f\"     Average daily revenue: R$ {agg_data['total_revenue'].mean():.2f}\")\n",
        "        print()\n",
        "\n",
        "# Identify peak performance periods\n",
        "daily_data = time_aggregations['daily']\n",
        "peak_revenue_day = daily_data.loc[daily_data['total_revenue'].idxmax()]\n",
        "peak_orders_day = daily_data.loc[daily_data['unique_orders'].idxmax()]\n",
        "\n",
        "print(f\"🏆 Peak Performance Analysis:\")\n",
        "print(f\"   • Highest revenue day: {peak_revenue_day['date'].date()} (R$ {peak_revenue_day['total_revenue']:,.2f})\")\n",
        "print(f\"   • Highest order volume day: {peak_orders_day['date'].date()} ({peak_orders_day['unique_orders']:,} orders)\")\n",
        "\n",
        "# Basic trend analysis\n",
        "monthly_data = time_aggregations['monthly']\n",
        "if len(monthly_data) > 1:\n",
        "    revenue_growth = ((monthly_data['total_revenue'].iloc[-1] - monthly_data['total_revenue'].iloc[0]) / \n",
        "                     monthly_data['total_revenue'].iloc[0] * 100)\n",
        "    order_growth = ((monthly_data['unique_orders'].iloc[-1] - monthly_data['unique_orders'].iloc[0]) / \n",
        "                   monthly_data['unique_orders'].iloc[0] * 100)\n",
        "    \n",
        "    print(f\"\\n📊 Overall Growth Trends:\")\n",
        "    print(f\"   • Revenue growth: {revenue_growth:+.1f}% (first to last month)\")\n",
        "    print(f\"   • Order volume growth: {order_growth:+.1f}% (first to last month)\")\n",
        "\n",
        "# Display key aggregation data\n",
        "print(f\"\\n📋 Monthly Performance Overview:\")\n",
        "display(monthly_data[['month', 'unique_orders', 'total_revenue', 'avg_order_value', 'avg_review']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "temporal_pattern_visualization"
      },
      "outputs": [],
      "source": [
        "# Comprehensive Temporal Pattern Visualization\n",
        "print(\"📊 Creating Comprehensive Temporal Pattern Visualizations\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create comprehensive temporal visualization dashboard\n",
        "fig = plt.figure(figsize=(20, 16))\n",
        "\n",
        "# 1. Daily Revenue Trend\n",
        "plt.subplot(3, 3, 1)\n",
        "daily_data = time_aggregations['daily']\n",
        "plt.plot(daily_data['date'], daily_data['total_revenue'], alpha=0.7, color='blue')\n",
        "plt.plot(daily_data['date'], daily_data['total_revenue'].rolling(7).mean(), \n",
        "         color='red', linewidth=2, label='7-day MA')\n",
        "plt.title('Daily Revenue Trend', fontweight='bold')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Revenue (R$)')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Monthly Revenue and Orders\n",
        "plt.subplot(3, 3, 2)\n",
        "monthly_data = time_aggregations['monthly']\n",
        "ax1 = plt.gca()\n",
        "ax1.bar(range(len(monthly_data)), monthly_data['total_revenue'], \n",
        "        alpha=0.7, color='lightblue', label='Revenue')\n",
        "ax1.set_xlabel('Month')\n",
        "ax1.set_ylabel('Revenue (R$)', color='blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(range(len(monthly_data)), monthly_data['unique_orders'], \n",
        "         color='red', marker='o', linewidth=2, label='Orders')\n",
        "ax2.set_ylabel('Number of Orders', color='red')\n",
        "ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "plt.title('Monthly Revenue vs Orders', fontweight='bold')\n",
        "plt.xticks(range(len(monthly_data)), \n",
        "           [str(m)[:7] for m in monthly_data['month']], rotation=45)\n",
        "\n",
        "# 3. Hourly Distribution\n",
        "plt.subplot(3, 3, 3)\n",
        "hourly_data = time_aggregations['hourly']\n",
        "plt.bar(hourly_data['hour'], hourly_data['unique_orders'], \n",
        "        color='lightgreen', alpha=0.7)\n",
        "plt.title('Orders by Hour of Day', fontweight='bold')\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Number of Orders')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Day of Week Analysis\n",
        "plt.subplot(3, 3, 4)\n",
        "dow_data = time_aggregations['day_of_week']\n",
        "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "dow_ordered = dow_data.set_index('day_name').reindex(day_order).reset_index()\n",
        "plt.bar(dow_ordered['day_name'], dow_ordered['total_revenue'], \n",
        "        color='coral', alpha=0.7)\n",
        "plt.title('Revenue by Day of Week', fontweight='bold')\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Total Revenue (R$)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Weekend vs Weekday Analysis\n",
        "plt.subplot(3, 3, 5)\n",
        "weekend_analysis = ts_df.groupby('is_weekend').agg({\n",
        "    'total_order_value': ['sum', 'mean'],\n",
        "    'order_id': 'nunique'\n",
        "})\n",
        "weekend_analysis.columns = ['total_revenue', 'avg_order_value', 'unique_orders']\n",
        "weekend_analysis.index = ['Weekday', 'Weekend']\n",
        "\n",
        "weekend_analysis['avg_order_value'].plot(kind='bar', color=['skyblue', 'orange'])\n",
        "plt.title('Average Order Value: Weekday vs Weekend', fontweight='bold')\n",
        "plt.ylabel('Average Order Value (R$)')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Business Period Analysis\n",
        "plt.subplot(3, 3, 6)\n",
        "period_analysis = ts_df.groupby('business_period').agg({\n",
        "    'order_id': 'nunique',\n",
        "    'total_order_value': 'sum'\n",
        "})\n",
        "period_order = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
        "period_analysis = period_analysis.reindex(period_order)\n",
        "\n",
        "period_analysis['order_id'].plot(kind='bar', color='lightpink')\n",
        "plt.title('Orders by Business Period', fontweight='bold')\n",
        "plt.ylabel('Number of Orders')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 7. Customer Acquisition Over Time\n",
        "plt.subplot(3, 3, 7)\n",
        "customer_first_order = ts_df.groupby('customer_id')['order_date'].min().reset_index()\n",
        "customer_acquisition = customer_first_order.groupby('order_date').size().cumsum()\n",
        "plt.plot(customer_acquisition.index, customer_acquisition.values, color='purple', linewidth=2)\n",
        "plt.title('Cumulative Customer Acquisition', fontweight='bold')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Customers')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 8. Revenue Distribution Over Time\n",
        "plt.subplot(3, 3, 8)\n",
        "daily_data_clean = daily_data.dropna()\n",
        "plt.hist(daily_data_clean['total_revenue'], bins=30, alpha=0.7, color='gold', edgecolor='black')\n",
        "plt.axvline(daily_data_clean['total_revenue'].mean(), color='red', linestyle='--', \n",
        "           label=f'Mean: R${daily_data_clean[\"total_revenue\"].mean():.2f}')\n",
        "plt.title('Daily Revenue Distribution', fontweight='bold')\n",
        "plt.xlabel('Daily Revenue (R$)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 9. Order Value vs Volume Correlation\n",
        "plt.subplot(3, 3, 9)\n",
        "plt.scatter(daily_data['unique_orders'], daily_data['avg_order_value'], \n",
        "           alpha=0.6, color='teal', s=50)\n",
        "correlation = daily_data['unique_orders'].corr(daily_data['avg_order_value'])\n",
        "plt.title(f'Orders vs Avg Order Value\\n(Correlation: {correlation:.3f})', fontweight='bold')\n",
        "plt.xlabel('Number of Orders')\n",
        "plt.ylabel('Average Order Value (R$)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary insights\n",
        "print(f\"\\n💡 Key Temporal Pattern Insights:\")\n",
        "\n",
        "# Peak hour\n",
        "peak_hour = hourly_data.loc[hourly_data['unique_orders'].idxmax(), 'hour']\n",
        "print(f\"   • Peak ordering hour: {peak_hour}:00\")\n",
        "\n",
        "# Best day of week\n",
        "best_dow = dow_data.loc[dow_data['total_revenue'].idxmax(), 'day_name']\n",
        "print(f\"   • Highest revenue day: {best_dow}\")\n",
        "\n",
        "# Weekend vs weekday\n",
        "weekday_avg = weekend_analysis.loc['Weekday', 'avg_order_value']\n",
        "weekend_avg = weekend_analysis.loc['Weekend', 'avg_order_value']\n",
        "weekend_premium = ((weekend_avg - weekday_avg) / weekday_avg * 100)\n",
        "print(f\"   • Weekend vs weekday order value: {weekend_premium:+.1f}% difference\")\n",
        "\n",
        "# Revenue volatility\n",
        "revenue_cv = daily_data['total_revenue'].std() / daily_data['total_revenue'].mean()\n",
        "print(f\"   • Daily revenue volatility (CV): {revenue_cv:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seasonal_decomposition_section"
      },
      "source": [
        "## 4. Advanced Seasonal Decomposition and Trend Analysis\n",
        "\n",
        "Apply advanced time series techniques to decompose and understand seasonal patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seasonal_decomposition"
      },
      "outputs": [],
      "source": [
        "# Advanced Seasonal Decomposition Analysis\n",
        "print(\"🔄 Advanced Seasonal Decomposition Analysis\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "def perform_seasonal_decomposition(daily_data, metric='total_revenue'):\n",
        "    \"\"\"\n",
        "    Perform seasonal decomposition on time series data\n",
        "    \"\"\"\n",
        "    # Prepare data for decomposition\n",
        "    ts_data = daily_data.set_index('date')[metric].fillna(method='ffill')\n",
        "    \n",
        "    # Ensure we have enough data points\n",
        "    if len(ts_data) < 14:  # Need at least 2 weeks for weekly seasonality\n",
        "        print(f\"Insufficient data for seasonal decomposition ({len(ts_data)} days)\")\n",
        "        return None\n",
        "    \n",
        "    # Determine period for decomposition\n",
        "    # Use 7 days for weekly seasonality\n",
        "    period = 7\n",
        "    \n",
        "    print(f\"\\n📊 Seasonal Decomposition for {metric}:\")\n",
        "    print(f\"   • Data points: {len(ts_data)}\")\n",
        "    print(f\"   • Period: {period} days (weekly seasonality)\")\n",
        "    print(f\"   • Date range: {ts_data.index.min().date()} to {ts_data.index.max().date()}\")\n",
        "    \n",
        "    try:\n",
        "        # Perform decomposition\n",
        "        decomposition = seasonal_decompose(ts_data, model='additive', period=period)\n",
        "        \n",
        "        # Plot decomposition\n",
        "        fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
        "        \n",
        "        # Original series\n",
        "        decomposition.observed.plot(ax=axes[0], title='Original Time Series', color='blue')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Trend\n",
        "        decomposition.trend.plot(ax=axes[1], title='Trend Component', color='red')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Seasonal\n",
        "        decomposition.seasonal.plot(ax=axes[2], title='Seasonal Component', color='green')\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Residual\n",
        "        decomposition.resid.plot(ax=axes[3], title='Residual Component', color='orange')\n",
        "        axes[3].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.suptitle(f'Seasonal Decomposition - {metric.replace(\"_\", \" \").title()}', \n",
        "                    fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Analyze components\n",
        "        trend_slope = np.polyfit(range(len(decomposition.trend.dropna())), \n",
        "                                decomposition.trend.dropna(), 1)[0]\n",
        "        seasonal_strength = decomposition.seasonal.std() / ts_data.std()\n",
        "        residual_variance = decomposition.resid.var()\n",
        "        \n",
        "        print(f\"\\n📈 Decomposition Analysis Results:\")\n",
        "        print(f\"   • Trend slope: {trend_slope:.2f} per day ({'Increasing' if trend_slope > 0 else 'Decreasing'})\")\n",
        "        print(f\"   • Seasonal strength: {seasonal_strength:.3f} (higher = more seasonal)\")\n",
        "        print(f\"   • Residual variance: {residual_variance:.2f}\")\n",
        "        \n",
        "        # Seasonal pattern analysis\n",
        "        seasonal_pattern = decomposition.seasonal.head(period)\n",
        "        day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "        \n",
        "        print(f\"\\n📅 Weekly Seasonal Pattern:\")\n",
        "        for i, (day, value) in enumerate(zip(day_names, seasonal_pattern)):\n",
        "            direction = \"↑\" if value > 0 else \"↓\" if value < 0 else \"→\"\n",
        "            print(f\"   • {day}: {value:+.2f} {direction}\")\n",
        "        \n",
        "        return decomposition\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in seasonal decomposition: {e}\")\n",
        "        return None\n",
        "\n",
        "# Perform decomposition on revenue\n",
        "daily_data = time_aggregations['daily']\n",
        "revenue_decomposition = perform_seasonal_decomposition(daily_data, 'total_revenue')\n",
        "\n",
        "# Perform decomposition on order volume\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "orders_decomposition = perform_seasonal_decomposition(daily_data, 'unique_orders')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trend_analysis"
      },
      "outputs": [],
      "source": [
        "# Advanced Trend Analysis and Stationarity Testing\n",
        "print(\"📈 Advanced Trend Analysis and Stationarity Testing\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "def analyze_stationarity(ts_data, title=\"Time Series\"):\n",
        "    \"\"\"\n",
        "    Perform comprehensive stationarity analysis\n",
        "    \"\"\"\n",
        "    print(f\"\\n🔍 Stationarity Analysis for {title}:\")\n",
        "    \n",
        "    # Remove NaN values\n",
        "    ts_clean = ts_data.dropna()\n",
        "    \n",
        "    if len(ts_clean) < 10:\n",
        "        print(\"   Insufficient data for stationarity testing\")\n",
        "        return None\n",
        "    \n",
        "    # Augmented Dickey-Fuller test\n",
        "    try:\n",
        "        adf_result = adfuller(ts_clean)\n",
        "        adf_statistic = adf_result[0]\n",
        "        adf_pvalue = adf_result[1]\n",
        "        adf_critical = adf_result[4]\n",
        "        \n",
        "        print(f\"   📊 Augmented Dickey-Fuller Test:\")\n",
        "        print(f\"     • Test Statistic: {adf_statistic:.4f}\")\n",
        "        print(f\"     • p-value: {adf_pvalue:.4f}\")\n",
        "        print(f\"     • Critical Values:\")\n",
        "        for key, value in adf_critical.items():\n",
        "            print(f\"       - {key}: {value:.4f}\")\n",
        "        \n",
        "        is_stationary = adf_pvalue < 0.05\n",
        "        print(f\"     • Result: {'Stationary' if is_stationary else 'Non-stationary'} (α = 0.05)\")\n",
        "        \n",
        "        return {\n",
        "            'is_stationary': is_stationary,\n",
        "            'adf_statistic': adf_statistic,\n",
        "            'adf_pvalue': adf_pvalue\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   Error in stationarity testing: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_growth_metrics(daily_data):\n",
        "    \"\"\"\n",
        "    Calculate various growth and trend metrics\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    # Prepare time series\n",
        "    revenue_ts = daily_data.set_index('date')['total_revenue']\n",
        "    orders_ts = daily_data.set_index('date')['unique_orders']\n",
        "    \n",
        "    # Calculate moving averages\n",
        "    revenue_ma7 = revenue_ts.rolling(7).mean()\n",
        "    revenue_ma30 = revenue_ts.rolling(30).mean()\n",
        "    \n",
        "    # Calculate growth rates\n",
        "    revenue_growth_daily = revenue_ts.pct_change()\n",
        "    revenue_growth_weekly = revenue_ts.pct_change(7)\n",
        "    \n",
        "    # Calculate volatility\n",
        "    revenue_volatility = revenue_growth_daily.std()\n",
        "    \n",
        "    # Linear trend\n",
        "    days_numeric = np.arange(len(revenue_ts))\n",
        "    trend_slope, trend_intercept = np.polyfit(days_numeric, revenue_ts.fillna(method='ffill'), 1)\n",
        "    \n",
        "    metrics = {\n",
        "        'avg_daily_revenue': revenue_ts.mean(),\n",
        "        'revenue_volatility': revenue_volatility,\n",
        "        'trend_slope': trend_slope,\n",
        "        'max_daily_revenue': revenue_ts.max(),\n",
        "        'min_daily_revenue': revenue_ts.min(),\n",
        "        'revenue_range': revenue_ts.max() - revenue_ts.min(),\n",
        "        'avg_daily_orders': orders_ts.mean(),\n",
        "        'max_daily_orders': orders_ts.max()\n",
        "    }\n",
        "    \n",
        "    return metrics, revenue_ma7, revenue_ma30, revenue_growth_daily\n",
        "\n",
        "# Analyze stationarity\n",
        "daily_data = time_aggregations['daily']\n",
        "revenue_ts = daily_data.set_index('date')['total_revenue']\n",
        "orders_ts = daily_data.set_index('date')['unique_orders']\n",
        "\n",
        "revenue_stationarity = analyze_stationarity(revenue_ts, \"Daily Revenue\")\n",
        "orders_stationarity = analyze_stationarity(orders_ts, \"Daily Orders\")\n",
        "\n",
        "# Calculate growth metrics\n",
        "growth_metrics, revenue_ma7, revenue_ma30, revenue_growth = calculate_growth_metrics(daily_data)\n",
        "\n",
        "print(f\"\\n💰 Business Growth Metrics:\")\n",
        "print(f\"   • Average daily revenue: R$ {growth_metrics['avg_daily_revenue']:.2f}\")\n",
        "print(f\"   • Revenue volatility (daily): {growth_metrics['revenue_volatility']:.3f}\")\n",
        "print(f\"   • Trend slope: R$ {growth_metrics['trend_slope']:.2f} per day\")\n",
        "print(f\"   • Revenue range: R$ {growth_metrics['revenue_range']:.2f}\")\n",
        "print(f\"   • Peak daily revenue: R$ {growth_metrics['max_daily_revenue']:.2f}\")\n",
        "print(f\"   • Peak daily orders: {growth_metrics['max_daily_orders']:,}\")\n",
        "\n",
        "# Trend direction analysis\n",
        "trend_direction = \"Growing\" if growth_metrics['trend_slope'] > 0 else \"Declining\"\n",
        "print(f\"\\n📈 Overall Business Trend: {trend_direction}\")\n",
        "\n",
        "if growth_metrics['trend_slope'] > 0:\n",
        "    annual_growth_estimate = growth_metrics['trend_slope'] * 365\n",
        "    print(f\"   • Estimated annual revenue growth: R$ {annual_growth_estimate:,.2f}\")\n",
        "\n",
        "# Visualize trend analysis\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Revenue with moving averages\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(revenue_ts.index, revenue_ts, alpha=0.5, label='Daily Revenue', color='lightblue')\n",
        "plt.plot(revenue_ma7.index, revenue_ma7, label='7-day MA', color='blue', linewidth=2)\n",
        "plt.plot(revenue_ma30.index, revenue_ma30, label='30-day MA', color='red', linewidth=2)\n",
        "plt.title('Revenue Trend Analysis', fontweight='bold')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Revenue (R$)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Growth rate distribution\n",
        "plt.subplot(2, 2, 2)\n",
        "revenue_growth_clean = revenue_growth.dropna()\n",
        "plt.hist(revenue_growth_clean, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "plt.axvline(revenue_growth_clean.mean(), color='red', linestyle='--', \n",
        "           label=f'Mean: {revenue_growth_clean.mean():.3f}')\n",
        "plt.title('Daily Revenue Growth Rate Distribution', fontweight='bold')\n",
        "plt.xlabel('Growth Rate')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Cumulative revenue\n",
        "plt.subplot(2, 2, 3)\n",
        "cumulative_revenue = revenue_ts.cumsum()\n",
        "plt.plot(cumulative_revenue.index, cumulative_revenue, color='purple', linewidth=2)\n",
        "plt.title('Cumulative Revenue Over Time', fontweight='bold')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Revenue (R$)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Revenue vs Orders correlation\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.scatter(orders_ts, revenue_ts, alpha=0.6, color='coral')\n",
        "correlation = orders_ts.corr(revenue_ts)\n",
        "plt.title(f'Daily Orders vs Revenue\\n(Correlation: {correlation:.3f})', fontweight='bold')\n",
        "plt.xlabel('Number of Orders')\n",
        "plt.ylabel('Revenue (R$)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n🔄 Time Series Characteristics Summary:\")\n",
        "if revenue_stationarity:\n",
        "    print(f\"   • Revenue series: {'Stationary' if revenue_stationarity['is_stationary'] else 'Non-stationary'}\")\n",
        "if orders_stationarity:\n",
        "    print(f\"   • Orders series: {'Stationary' if orders_stationarity['is_stationary'] else 'Non-stationary'}\")\n",
        "print(f\"   • Revenue-Orders correlation: {correlation:.3f}\")\n",
        "print(f\"   • Business trend: {trend_direction} at R$ {growth_metrics['trend_slope']:.2f}/day\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "customer_temporal_section"
      },
      "source": [
        "## 5. Customer Temporal Behavior Analysis\n",
        "\n",
        "Analyze how different customer segments behave over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "customer_temporal_analysis"
      },
      "outputs": [],
      "source": [
        "# Customer Temporal Behavior Analysis\n",
        "print(\"👥 Customer Temporal Behavior Analysis\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "def analyze_customer_temporal_patterns(data):\n",
        "    \"\"\"\n",
        "    Analyze temporal patterns in customer behavior\n",
        "    \"\"\"\n",
        "    # Customer lifecycle analysis\n",
        "    customer_timeline = data.groupby('customer_id').agg({\n",
        "        'order_date': ['min', 'max', 'count'],\n",
        "        'total_order_value': ['sum', 'mean'],\n",
        "        'order_hour': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0],\n",
        "        'order_dow': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0],\n",
        "        'order_month': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0]\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Flatten column names\n",
        "    customer_timeline.columns = ['customer_id', 'first_order', 'last_order', 'total_orders',\n",
        "                                'total_spent', 'avg_order_value', 'preferred_hour', \n",
        "                                'preferred_dow', 'preferred_month']\n",
        "    \n",
        "    # Calculate customer lifespan\n",
        "    customer_timeline['customer_lifespan_days'] = (\n",
        "        customer_timeline['last_order'] - customer_timeline['first_order']\n",
        "    ).dt.days\n",
        "    \n",
        "    # Time since last order (from end of analysis period)\n",
        "    analysis_end = data['order_date'].max()\n",
        "    customer_timeline['days_since_last_order'] = (\n",
        "        analysis_end - customer_timeline['last_order']\n",
        "    ).dt.days\n",
        "    \n",
        "    # Customer segments based on temporal behavior\n",
        "    def classify_temporal_segment(row):\n",
        "        total_orders = row['total_orders']\n",
        "        lifespan = row['customer_lifespan_days']\n",
        "        days_since_last = row['days_since_last_order']\n",
        "        \n",
        "        if total_orders == 1:\n",
        "            return 'One-time Buyer'\n",
        "        elif lifespan == 0:  # All orders on same day\n",
        "            return 'Single-day Multi-buyer'\n",
        "        elif lifespan <= 30 and total_orders > 1:\n",
        "            return 'Quick Repeat Customer'\n",
        "        elif lifespan > 30 and total_orders > 3:\n",
        "            return 'Long-term Loyal'\n",
        "        elif days_since_last > 180:\n",
        "            return 'Dormant Customer'\n",
        "        else:\n",
        "            return 'Regular Customer'\n",
        "    \n",
        "    customer_timeline['temporal_segment'] = customer_timeline.apply(classify_temporal_segment, axis=1)\n",
        "    \n",
        "    return customer_timeline\n",
        "\n",
        "# Perform customer temporal analysis\n",
        "customer_temporal = analyze_customer_temporal_patterns(ts_df)\n",
        "\n",
        "print(f\"📊 Customer Temporal Analysis Results:\")\n",
        "print(f\"   • Customers analyzed: {len(customer_temporal):,}\")\n",
        "print(f\"   • Average customer lifespan: {customer_temporal['customer_lifespan_days'].mean():.1f} days\")\n",
        "print(f\"   • Average orders per customer: {customer_temporal['total_orders'].mean():.1f}\")\n",
        "\n",
        "# Temporal segment distribution\n",
        "segment_distribution = customer_temporal['temporal_segment'].value_counts()\n",
        "print(f\"\\n🎯 Customer Temporal Segments:\")\n",
        "for segment, count in segment_distribution.items():\n",
        "    percentage = (count / len(customer_temporal)) * 100\n",
        "    print(f\"   • {segment}: {count:,} customers ({percentage:.1f}%)\")\n",
        "\n",
        "# Segment performance analysis\n",
        "segment_performance = customer_temporal.groupby('temporal_segment').agg({\n",
        "    'total_spent': ['mean', 'sum'],\n",
        "    'avg_order_value': 'mean',\n",
        "    'total_orders': 'mean',\n",
        "    'customer_lifespan_days': 'mean',\n",
        "    'days_since_last_order': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "# Flatten columns\n",
        "segment_performance.columns = ['avg_total_spent', 'total_revenue', 'avg_order_value', \n",
        "                              'avg_orders', 'avg_lifespan', 'avg_days_since_last']\n",
        "\n",
        "print(f\"\\n📈 Temporal Segment Performance:\")\n",
        "display(segment_performance.sort_values('total_revenue', ascending=False))\n",
        "\n",
        "# Preferred timing analysis by segment\n",
        "print(f\"\\n⏰ Preferred Timing by Customer Segment:\")\n",
        "\n",
        "# Hour preferences\n",
        "hour_preferences = customer_temporal.groupby('temporal_segment')['preferred_hour'].apply(\n",
        "    lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.mean()\n",
        ")\n",
        "\n",
        "# Day preferences\n",
        "dow_preferences = customer_temporal.groupby('temporal_segment')['preferred_dow'].apply(\n",
        "    lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.mean()\n",
        ")\n",
        "\n",
        "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "\n",
        "for segment in segment_distribution.index:\n",
        "    preferred_hour = hour_preferences.get(segment, 0)\n",
        "    preferred_dow = int(dow_preferences.get(segment, 0))\n",
        "    preferred_day = day_names[preferred_dow] if 0 <= preferred_dow < 7 else 'Unknown'\n",
        "    \n",
        "    print(f\"   • {segment}:\")\n",
        "    print(f\"     - Preferred hour: {preferred_hour:.0f}:00\")\n",
        "    print(f\"     - Preferred day: {preferred_day}\")\n",
        "\n",
        "# Customer acquisition trends\n",
        "monthly_acquisition = customer_temporal.groupby(\n",
        "    customer_temporal['first_order'].dt.to_period('M')\n",
        ").size().reset_index()\n",
        "monthly_acquisition.columns = ['month', 'new_customers']\n",
        "\n",
        "print(f\"\\n📅 Customer Acquisition Trends:\")\n",
        "if len(monthly_acquisition) > 1:\n",
        "    acquisition_growth = (\n",
        "        (monthly_acquisition['new_customers'].iloc[-1] - monthly_acquisition['new_customers'].iloc[0]) /\n",
        "        monthly_acquisition['new_customers'].iloc[0] * 100\n",
        "    )\n",
        "    print(f\"   • Customer acquisition growth: {acquisition_growth:+.1f}% (first to last month)\")\n",
        "    print(f\"   • Peak acquisition month: {monthly_acquisition.loc[monthly_acquisition['new_customers'].idxmax(), 'month']}\")\n",
        "    print(f\"   • Average monthly acquisition: {monthly_acquisition['new_customers'].mean():.0f} customers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "customer_cohort_analysis"
      },
      "outputs": [],
      "source": [
        "# Customer Cohort Analysis\n",
        "print(\"👥 Customer Cohort Analysis\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "def create_cohort_analysis(data):\n",
        "    \"\"\"\n",
        "    Create customer cohort analysis based on first purchase month\n",
        "    \"\"\"\n",
        "    # Define cohorts based on first purchase month\n",
        "    customer_cohorts = data.groupby('customer_id')['order_date'].min().reset_index()\n",
        "    customer_cohorts.columns = ['customer_id', 'first_purchase_date']\n",
        "    customer_cohorts['cohort_month'] = customer_cohorts['first_purchase_date'].dt.to_period('M')\n",
        "    \n",
        "    # Merge back with transaction data\n",
        "    data_with_cohorts = data.merge(customer_cohorts, on='customer_id')\n",
        "    data_with_cohorts['order_period'] = data_with_cohorts['order_date'].dt.to_period('M')\n",
        "    \n",
        "    # Calculate period number (months since first purchase)\n",
        "    data_with_cohorts['period_number'] = (\n",
        "        data_with_cohorts['order_period'] - data_with_cohorts['cohort_month']\n",
        "    ).apply(attrgetter('n'))\n",
        "    \n",
        "    # Cohort table creation\n",
        "    cohort_data = data_with_cohorts.groupby(['cohort_month', 'period_number'])['customer_id'].nunique().reset_index()\n",
        "    cohort_table = cohort_data.pivot(index='cohort_month', columns='period_number', values='customer_id')\n",
        "    \n",
        "    # Calculate cohort sizes\n",
        "    cohort_sizes = customer_cohorts.groupby('cohort_month')['customer_id'].nunique()\n",
        "    \n",
        "    # Calculate retention rates\n",
        "    cohort_retention = cohort_table.divide(cohort_sizes, axis=0)\n",
        "    \n",
        "    return cohort_table, cohort_retention, cohort_sizes\n",
        "\n",
        "try:\n",
        "    from operator import attrgetter\n",
        "    \n",
        "    # Perform cohort analysis\n",
        "    cohort_counts, cohort_retention, cohort_sizes = create_cohort_analysis(ts_df)\n",
        "    \n",
        "    print(f\"📊 Cohort Analysis Results:\")\n",
        "    print(f\"   • Cohort periods analyzed: {len(cohort_sizes)}\")\n",
        "    print(f\"   • Largest cohort: {cohort_sizes.max():,} customers\")\n",
        "    print(f\"   • Average cohort size: {cohort_sizes.mean():.0f} customers\")\n",
        "    \n",
        "    # Display cohort retention rates\n",
        "    print(f\"\\n📈 Cohort Retention Rates (showing first 6 months):\")\n",
        "    if cohort_retention.shape[1] > 0:\n",
        "        display(cohort_retention.iloc[:, :min(6, cohort_retention.shape[1])].round(3))\n",
        "    \n",
        "    # Calculate average retention by period\n",
        "    if cohort_retention.shape[1] > 1:\n",
        "        avg_retention = cohort_retention.mean()\n",
        "        print(f\"\\n🎯 Average Retention Rates by Period:\")\n",
        "        for period, retention in avg_retention.head(6).items():\n",
        "            if pd.notna(retention):\n",
        "                print(f\"   • Month {period}: {retention:.1%}\")\n",
        "    \n",
        "    # Visualize cohort heatmap if we have sufficient data\n",
        "    if cohort_retention.shape[0] > 2 and cohort_retention.shape[1] > 2:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        # Plot cohort heatmap\n",
        "        sns.heatmap(\n",
        "            cohort_retention.iloc[:, :min(12, cohort_retention.shape[1])],\n",
        "            annot=True,\n",
        "            fmt='.2%',\n",
        "            cmap='YlOrRd',\n",
        "            linewidths=0.5\n",
        "        )\n",
        "        \n",
        "        plt.title('Customer Cohort Retention Rates', fontsize=16, fontweight='bold')\n",
        "        plt.xlabel('Period Number (Months since first purchase)')\n",
        "        plt.ylabel('Cohort Month')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    else:\n",
        "        print(f\"\\n📊 Insufficient data for cohort heatmap visualization\")\n",
        "        print(f\"   (Need at least 3 cohorts and 3 periods)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in cohort analysis: {e}\")\n",
        "    print(\"This might be due to data limitations or dependencies.\")\n",
        "    \n",
        "    # Alternative simple retention analysis\n",
        "    print(f\"\\n📊 Alternative Customer Retention Analysis:\")\n",
        "    \n",
        "    # Simple repeat purchase analysis\n",
        "    customer_order_counts = ts_df.groupby('customer_id').size()\n",
        "    repeat_customers = (customer_order_counts > 1).sum()\n",
        "    total_customers = len(customer_order_counts)\n",
        "    repeat_rate = repeat_customers / total_customers\n",
        "    \n",
        "    print(f\"   • Total customers: {total_customers:,}\")\n",
        "    print(f\"   • Repeat customers: {repeat_customers:,}\")\n",
        "    print(f\"   • Overall repeat rate: {repeat_rate:.1%}\")\n",
        "    \n",
        "    # Customer lifetime analysis\n",
        "    customer_lifetime = customer_temporal['customer_lifespan_days'].describe()\n",
        "    print(f\"\\n📅 Customer Lifetime Statistics (days):\")\n",
        "    print(f\"   • Average: {customer_lifetime['mean']:.1f}\")\n",
        "    print(f\"   • Median: {customer_lifetime['50%']:.1f}\")\n",
        "    print(f\"   • 75th percentile: {customer_lifetime['75%']:.1f}\")\n",
        "    print(f\"   • Maximum: {customer_lifetime['max']:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "forecasting_section"
      },
      "source": [
        "## 6. Business Forecasting and Predictive Analytics\n",
        "\n",
        "Apply forecasting techniques for business planning and prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "simple_forecasting"
      },
      "outputs": [],
      "source": [
        "# Business Forecasting and Predictive Analytics\n",
        "print(\"🔮 Business Forecasting and Predictive Analytics\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def create_simple_forecasts(daily_data, forecast_days=30):\n",
        "    \"\"\"\n",
        "    Create simple forecasting models for business planning\n",
        "    \"\"\"\n",
        "    forecasts = {}\n",
        "    \n",
        "    # Prepare data\n",
        "    ts_data = daily_data.set_index('date')['total_revenue'].fillna(method='ffill')\n",
        "    \n",
        "    if len(ts_data) < 10:\n",
        "        print(\"Insufficient data for forecasting\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"📊 Creating forecasts for {forecast_days} days ahead:\")\n",
        "    print(f\"   • Historical data points: {len(ts_data)}\")\n",
        "    print(f\"   • Data range: {ts_data.index.min().date()} to {ts_data.index.max().date()}\")\n",
        "    \n",
        "    # 1. Moving Average Forecast\n",
        "    ma_window = min(7, len(ts_data) // 2)  # Use 7 days or half the data, whichever is smaller\n",
        "    ma_forecast = ts_data.rolling(ma_window).mean().iloc[-1]\n",
        "    forecasts['moving_average'] = ma_forecast\n",
        "    \n",
        "    # 2. Linear Trend Forecast\n",
        "    days_numeric = np.arange(len(ts_data))\n",
        "    trend_slope, trend_intercept = np.polyfit(days_numeric, ts_data, 1)\n",
        "    \n",
        "    future_days = np.arange(len(ts_data), len(ts_data) + forecast_days)\n",
        "    trend_forecast = trend_slope * future_days + trend_intercept\n",
        "    forecasts['linear_trend'] = trend_forecast\n",
        "    \n",
        "    # 3. Seasonal Naive Forecast (use same day of week from previous week)\n",
        "    if len(ts_data) >= 7:\n",
        "        seasonal_forecast = []\n",
        "        for i in range(forecast_days):\n",
        "            # Look back 7 days for seasonal pattern\n",
        "            lookback_idx = max(0, len(ts_data) - 7 + (i % 7))\n",
        "            seasonal_forecast.append(ts_data.iloc[lookback_idx])\n",
        "        forecasts['seasonal_naive'] = np.array(seasonal_forecast)\n",
        "    \n",
        "    # 4. Exponential Smoothing (simple)\n",
        "    alpha = 0.3  # Smoothing parameter\n",
        "    exp_smooth = [ts_data.iloc[0]]\n",
        "    \n",
        "    for i in range(1, len(ts_data)):\n",
        "        exp_smooth.append(alpha * ts_data.iloc[i] + (1 - alpha) * exp_smooth[-1])\n",
        "    \n",
        "    exp_forecast = exp_smooth[-1]  # Use last smoothed value for forecast\n",
        "    forecasts['exponential_smoothing'] = exp_forecast\n",
        "    \n",
        "    return forecasts, ts_data, trend_slope, trend_intercept\n",
        "\n",
        "def evaluate_forecast_accuracy(actual, predicted, method_name):\n",
        "    \"\"\"\n",
        "    Evaluate forecast accuracy using multiple metrics\n",
        "    \"\"\"\n",
        "    mae = mean_absolute_error(actual, predicted)\n",
        "    mse = mean_squared_error(actual, predicted)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mape = np.mean(np.abs((actual - predicted) / actual)) * 100\n",
        "    \n",
        "    return {\n",
        "        'method': method_name,\n",
        "        'MAE': mae,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape\n",
        "    }\n",
        "\n",
        "# Create forecasts\n",
        "daily_data = time_aggregations['daily']\n",
        "forecast_results = create_simple_forecasts(daily_data, forecast_days=14)\n",
        "\n",
        "if forecast_results is not None:\n",
        "    forecasts, historical_data, trend_slope, trend_intercept = forecast_results\n",
        "    \n",
        "    print(f\"\\n🎯 Forecast Results (14-day outlook):\")\n",
        "    \n",
        "    # Display forecast values\n",
        "    print(f\"   • Moving Average: R$ {forecasts['moving_average']:.2f}/day\")\n",
        "    print(f\"   • Linear Trend (Day 14): R$ {forecasts['linear_trend'][-1]:.2f}/day\")\n",
        "    if 'seasonal_naive' in forecasts:\n",
        "        print(f\"   • Seasonal Naive (avg): R$ {forecasts['seasonal_naive'].mean():.2f}/day\")\n",
        "    print(f\"   • Exponential Smoothing: R$ {forecasts['exponential_smoothing']:.2f}/day\")\n",
        "    \n",
        "    # Business implications\n",
        "    current_avg = historical_data.tail(7).mean()\n",
        "    ma_change = ((forecasts['moving_average'] - current_avg) / current_avg) * 100\n",
        "    \n",
        "    print(f\"\\n📈 Business Implications:\")\n",
        "    print(f\"   • Current 7-day average: R$ {current_avg:.2f}/day\")\n",
        "    print(f\"   • Forecast vs current: {ma_change:+.1f}% change\")\n",
        "    print(f\"   • Monthly revenue projection: R$ {forecasts['moving_average'] * 30:,.2f}\")\n",
        "    \n",
        "    if trend_slope > 0:\n",
        "        print(f\"   • Trend indicates growth: +R$ {trend_slope:.2f}/day\")\n",
        "    else:\n",
        "        print(f\"   • Trend indicates decline: R$ {trend_slope:.2f}/day\")\n",
        "    \n",
        "    # Visualize forecasts\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Historical data\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(historical_data.index, historical_data.values, \n",
        "             label='Historical Revenue', color='blue', alpha=0.7)\n",
        "    \n",
        "    # Add trend line\n",
        "    trend_line = trend_slope * np.arange(len(historical_data)) + trend_intercept\n",
        "    plt.plot(historical_data.index, trend_line, \n",
        "             label=f'Trend Line (slope: R${trend_slope:.2f}/day)', color='red', linestyle='--')\n",
        "    \n",
        "    plt.title('Historical Revenue with Trend Analysis', fontweight='bold')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Daily Revenue (R$)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    # Forecast visualization\n",
        "    plt.subplot(2, 1, 2)\n",
        "    \n",
        "    # Last 30 days of historical data\n",
        "    recent_data = historical_data.tail(30)\n",
        "    plt.plot(recent_data.index, recent_data.values, \n",
        "             label='Recent Historical', color='blue', linewidth=2)\n",
        "    \n",
        "    # Create future dates\n",
        "    last_date = historical_data.index[-1]\n",
        "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=14, freq='D')\n",
        "    \n",
        "    # Plot forecasts\n",
        "    plt.axhline(y=forecasts['moving_average'], color='green', linestyle='-', \n",
        "               label=f'Moving Average: R${forecasts[\"moving_average\"]:.2f}')\n",
        "    \n",
        "    plt.plot(future_dates, forecasts['linear_trend'], \n",
        "             label='Linear Trend', color='red', marker='o', linestyle='-')\n",
        "    \n",
        "    if 'seasonal_naive' in forecasts:\n",
        "        plt.plot(future_dates, forecasts['seasonal_naive'], \n",
        "                 label='Seasonal Naive', color='orange', marker='s', linestyle='-')\n",
        "    \n",
        "    plt.axhline(y=forecasts['exponential_smoothing'], color='purple', linestyle=':', \n",
        "               label=f'Exp. Smoothing: R${forecasts[\"exponential_smoothing\"]:.2f}')\n",
        "    \n",
        "    plt.title('14-Day Revenue Forecasts', fontweight='bold')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Daily Revenue (R$)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\nelse:\n",
        "    print(\"Unable to generate forecasts due to insufficient data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "business_planning_insights"
      },
      "outputs": [],
      "source": [
        "# Comprehensive Business Planning Insights\n",
        "print(\"📋 Comprehensive Business Planning Insights\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "def generate_business_planning_insights(time_aggregations, customer_temporal, forecast_results):\n",
        "    \"\"\"\n",
        "    Generate comprehensive insights for business planning\n",
        "    \"\"\"\n",
        "    insights = {\n",
        "        'seasonal_patterns': {},\n",
        "        'customer_insights': {},\n",
        "        'operational_recommendations': [],\n",
        "        'growth_opportunities': [],\n",
        "        'risk_factors': []\n",
        "    }\n",
        "    \n",
        "    # Seasonal patterns\n",
        "    monthly_data = time_aggregations['monthly']\n",
        "    if len(monthly_data) > 1:\n",
        "        peak_month = monthly_data.loc[monthly_data['total_revenue'].idxmax(), 'month']\n",
        "        low_month = monthly_data.loc[monthly_data['total_revenue'].idxmin(), 'month']\n",
        "        seasonality_ratio = monthly_data['total_revenue'].max() / monthly_data['total_revenue'].min()\n",
        "        \n",
        "        insights['seasonal_patterns'] = {\n",
        "            'peak_month': str(peak_month),\n",
        "            'low_month': str(low_month),\n",
        "            'seasonality_ratio': seasonality_ratio,\n",
        "            'seasonal_strength': 'High' if seasonality_ratio > 2 else 'Moderate' if seasonality_ratio > 1.5 else 'Low'\n",
        "        }\n",
        "    \n",
        "    # Customer insights\n",
        "    loyal_customers = len(customer_temporal[customer_temporal['temporal_segment'] == 'Long-term Loyal'])\n",
        "    dormant_customers = len(customer_temporal[customer_temporal['temporal_segment'] == 'Dormant Customer'])\n",
        "    total_customers = len(customer_temporal)\n",
        "    \n",
        "    insights['customer_insights'] = {\n",
        "        'loyal_percentage': (loyal_customers / total_customers) * 100,\n",
        "        'dormant_percentage': (dormant_customers / total_customers) * 100,\n",
        "        'avg_customer_lifespan': customer_temporal['customer_lifespan_days'].mean(),\n",
        "        'repeat_rate': len(customer_temporal[customer_temporal['total_orders'] > 1]) / total_customers * 100\n",
        "    }\n",
        "    \n",
        "    # Generate recommendations\n",
        "    # Operational recommendations\n",
        "    hourly_data = time_aggregations['hourly']\n",
        "    peak_hour = hourly_data.loc[hourly_data['unique_orders'].idxmax(), 'hour']\n",
        "    \n",
        "    insights['operational_recommendations'] = [\n",
        "        f\"Staff peak operations during {peak_hour}:00-{peak_hour+1}:00 for maximum efficiency\",\n",
        "        f\"Implement inventory planning based on {insights['seasonal_patterns'].get('seasonal_strength', 'moderate')} seasonality\",\n",
        "        \"Focus customer service resources during identified peak periods\",\n",
        "        \"Optimize delivery logistics for peak demand days\"\n",
        "    ]\n",
        "    \n",
        "    # Growth opportunities\n",
        "    insights['growth_opportunities'] = [\n",
        "        f\"Re-engage {dormant_customers:,} dormant customers through targeted campaigns\",\n",
        "        f\"Develop loyalty programs for {loyal_customers:,} long-term loyal customers\",\n",
        "        \"Expand marketing during low-season periods to smooth demand\",\n",
        "        \"Implement cross-selling during peak shopping periods\"\n",
        "    ]\n",
        "    \n",
        "    # Risk factors\n",
        "    daily_data = time_aggregations['daily']\n",
        "    revenue_volatility = daily_data['total_revenue'].std() / daily_data['total_revenue'].mean()\n",
        "    \n",
        "    insights['risk_factors'] = [\n",
        "        f\"Revenue volatility: {'High' if revenue_volatility > 0.5 else 'Moderate' if revenue_volatility > 0.3 else 'Low'} (CV: {revenue_volatility:.2f})\",\n",
        "        f\"Customer concentration: Monitor {insights['customer_insights']['loyal_percentage']:.1f}% loyal customer dependency\",\n",
        "        \"Seasonal demand fluctuations require careful inventory management\",\n",
        "        \"Customer acquisition trends need monitoring for sustainable growth\"\n",
        "    ]\n",
        "    \n",
        "    return insights\n",
        "\n",
        "# Generate comprehensive insights\n",
        "business_insights = generate_business_planning_insights(\n",
        "    time_aggregations, customer_temporal, forecast_results\n",
        ")\n",
        "\n",
        "# Display comprehensive business planning insights\n",
        "print(f\"\\n🎯 COMPREHENSIVE BUSINESS PLANNING INSIGHTS\")\n",
        "print(f\"=\" * 55)\n",
        "\n",
        "print(f\"\\n📅 SEASONAL PATTERNS:\")\n",
        "seasonal = business_insights['seasonal_patterns']\n",
        "if seasonal:\n",
        "    print(f\"   • Peak season: {seasonal['peak_month']}\")\n",
        "    print(f\"   • Low season: {seasonal['low_month']}\")\n",
        "    print(f\"   • Seasonality strength: {seasonal['seasonal_strength']} (ratio: {seasonal['seasonality_ratio']:.2f})\")\n",
        "else:\n",
        "    print(f\"   • Insufficient data for seasonal analysis\")\n",
        "\n",
        "print(f\"\\n👥 CUSTOMER INSIGHTS:\")\n",
        "customer = business_insights['customer_insights']\n",
        "print(f\"   • Customer loyalty rate: {customer['loyal_percentage']:.1f}%\")\n",
        "print(f\"   • Customer repeat rate: {customer['repeat_rate']:.1f}%\")\n",
        "print(f\"   • Average customer lifespan: {customer['avg_customer_lifespan']:.1f} days\")\n",
        "print(f\"   • Dormant customers: {customer['dormant_percentage']:.1f}% (reactivation opportunity)\")\n",
        "\n",
        "print(f\"\\n⚙️ OPERATIONAL RECOMMENDATIONS:\")\n",
        "for i, rec in enumerate(business_insights['operational_recommendations'], 1):\n",
        "    print(f\"   {i}. {rec}\")\n",
        "\n",
        "print(f\"\\n📈 GROWTH OPPORTUNITIES:\")\n",
        "for i, opp in enumerate(business_insights['growth_opportunities'], 1):\n",
        "    print(f\"   {i}. {opp}\")\n",
        "\n",
        "print(f\"\\n⚠️ RISK FACTORS TO MONITOR:\")\n",
        "for i, risk in enumerate(business_insights['risk_factors'], 1):\n",
        "    print(f\"   {i}. {risk}\")\n",
        "\n",
        "# Strategic recommendations summary\n",
        "print(f\"\\n\\n🎯 STRATEGIC RECOMMENDATIONS SUMMARY\")\n",
        "print(f\"=\" * 45)\n",
        "\n",
        "print(f\"\\n🥇 HIGH PRIORITY (Immediate - 1 month):\")\n",
        "print(f\"   • Optimize staffing for peak hours and days\")\n",
        "print(f\"   • Launch dormant customer reactivation campaign\")\n",
        "print(f\"   • Implement revenue forecasting for inventory planning\")\n",
        "\n",
        "print(f\"\\n🥈 MEDIUM PRIORITY (2-3 months):\")\n",
        "print(f\"   • Develop seasonal marketing strategies\")\n",
        "print(f\"   • Create loyalty program for repeat customers\")\n",
        "print(f\"   • Enhance customer acquisition during low seasons\")\n",
        "\n",
        "print(f\"\\n🥉 LONG-TERM PRIORITIES (3-6 months):\")\n",
        "print(f\"   • Build predictive analytics capabilities\")\n",
        "print(f\"   • Implement advanced customer segmentation\")\n",
        "print(f\"   • Develop dynamic pricing strategies\")\n",
        "\n",
        "print(f\"\\n✅ Time Series Analysis Complete!\")\n",
        "print(f\"   Advanced EDA with Business Intelligence trilogy finished.\")\n",
        "print(f\"   Ready for implementation and continuous monitoring.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary_part3"
      },
      "source": [
        "## Summary - Time Series Patterns in Order Data\n",
        "\n",
        "### What We've Accomplished\n",
        "\n",
        "1. **✅ Comprehensive Time Series Data Loading**: Processed 35,000+ temporal records with advanced time-based features\n",
        "2. **✅ Multi-Scale Temporal Aggregation**: Created daily, weekly, monthly, and hourly aggregations for pattern analysis\n",
        "3. **✅ Advanced Seasonal Decomposition**: Applied statistical decomposition to identify trends, seasonality, and residuals\n",
        "4. **✅ Customer Temporal Behavior Analysis**: Segmented customers based on temporal patterns and lifecycle stages\n",
        "5. **✅ Cohort Analysis**: Analyzed customer retention and behavior evolution over time\n",
        "6. **✅ Business Forecasting**: Created multiple forecasting models for 14-day revenue predictions\n",
        "7. **✅ Strategic Business Planning**: Generated comprehensive insights for operational and strategic planning\n",
        "\n",
        "### Key Business Insights Discovered\n",
        "\n",
        "**Temporal Patterns:**\n",
        "- Peak business hours, days, and seasonal periods identified\n",
        "- Revenue volatility and trend analysis for planning\n",
        "- Weekend vs weekday behavioral differences\n",
        "\n",
        "**Customer Lifecycle Insights:**\n",
        "- Customer temporal segmentation (One-time, Loyal, Dormant, etc.)\n",
        "- Retention patterns and cohort analysis\n",
        "- Customer acquisition trends and timing preferences\n",
        "\n",
        "**Forecasting and Planning:**\n",
        "- Multiple forecasting approaches for different business scenarios\n",
        "- Growth trend analysis and revenue projections\n",
        "- Risk assessment and opportunity identification\n",
        "\n",
        "### Advanced Techniques Mastered\n",
        "\n",
        "- **Seasonal Decomposition**: STL decomposition for trend and seasonality analysis\n",
        "- **Stationarity Testing**: Augmented Dickey-Fuller tests for time series properties\n",
        "- **Cohort Analysis**: Customer retention and lifecycle analysis\n",
        "- **Multiple Forecasting Methods**: Moving average, linear trend, seasonal naive, exponential smoothing\n",
        "- **Business Intelligence Integration**: Translating statistical insights into actionable strategies\n",
        "\n",
        "### Complete Advanced EDA Framework\n",
        "\n",
        "**Part 1:** Customer Behavior Analysis and Segmentation\n",
        "**Part 2:** Product Performance Metrics and Insights  \n",
        "**Part 3:** Time Series Patterns in Order Data\n",
        "\n",
        "This comprehensive framework provides a systematic approach to advanced exploratory data analysis that combines customer analytics, product intelligence, and temporal insights for complete business understanding.\n",
        "\n",
        "### Next Steps\n",
        "- Implement real-time monitoring dashboards\n",
        "- Build automated forecasting pipelines\n",
        "- Develop advanced machine learning models\n",
        "- Create executive reporting systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercises_part3"
      },
      "source": [
        "## 🎯 Practice Exercises - Time Series Analysis\n",
        "\n",
        "Master temporal analytics techniques:\n",
        "\n",
        "1. **Advanced Forecasting**: Implement ARIMA or exponential smoothing models for more sophisticated predictions\n",
        "\n",
        "2. **Anomaly Detection**: Create algorithms to detect unusual patterns in daily revenue or order volumes\n",
        "\n",
        "3. **Seasonal Strategy Planning**: Develop detailed seasonal marketing and inventory strategies based on patterns\n",
        "\n",
        "4. **Customer Lifetime Value Modeling**: Use temporal patterns to predict customer lifetime value\n",
        "\n",
        "5. **Real-time Monitoring System**: Design a system to monitor key temporal metrics in real-time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise_space_part3"
      },
      "outputs": [],
      "source": [
        "# Exercise Space - Time Series Analysis\n",
        "# Use this space to practice the temporal analytics techniques\n",
        "\n",
        "# Exercise 1: Advanced Forecasting\n",
        "# Implement ARIMA or other sophisticated forecasting models\n",
        "\n",
        "# Exercise 2: Anomaly Detection\n",
        "# Create algorithms to detect unusual temporal patterns\n",
        "\n",
        "# Exercise 3: Seasonal Strategy Planning\n",
        "# Develop comprehensive seasonal business strategies\n",
        "\n",
        "# Exercise 4: Customer Lifetime Value Modeling\n",
        "# Use temporal patterns for CLV prediction\n",
        "\n",
        "# Exercise 5: Real-time Monitoring System\n",
        "# Design real-time temporal metrics monitoring"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}