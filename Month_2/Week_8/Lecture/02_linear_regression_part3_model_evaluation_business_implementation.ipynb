{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_evaluation_header"
   },
   "source": [
    "# Linear Regression Part 3: Model Evaluation & Business Implementation\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will be able to:\n",
    "- Implement advanced model validation techniques\n",
    "- Design A/B testing frameworks for model deployment\n",
    "- Monitor model performance in production environments\n",
    "- Handle model drift and retraining strategies\n",
    "- Create business-ready model deployment pipelines\n",
    "- Communicate model results effectively to stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "environment_setup"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "Enhanced setup for model evaluation and deployment scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_setup"
   },
   "outputs": [],
   "source": [
    "# Standard data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Statistical and machine learning libraries\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, validation_curve, \n",
    "    TimeSeriesSplit, KFold\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, mean_absolute_error,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Model interpretation and evaluation\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Database connectivity\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Visualization styling\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Environment setup complete for model evaluation and deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "database_connection"
   },
   "source": [
    "## Database Connection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db_config"
   },
   "outputs": [],
   "source": [
    "# Database configuration\n",
    "DATABASE_CONFIG = {\n",
    "    'host': os.getenv('SUPABASE_HOST'),\n",
    "    'port': os.getenv('SUPABASE_PORT', '5432'),\n",
    "    'database': os.getenv('SUPABASE_DATABASE'),\n",
    "    'user': os.getenv('SUPABASE_USER'),\n",
    "    'password': os.getenv('SUPABASE_PASSWORD')\n",
    "}\n",
    "\n",
    "def create_database_connection():\n",
    "    \"\"\"\n",
    "    Create a SQLAlchemy engine for database connections.\n",
    "    \n",
    "    Returns:\n",
    "        sqlalchemy.engine.Engine: Database engine for executing queries\n",
    "    \"\"\"\n",
    "    connection_string = f\"postgresql://{DATABASE_CONFIG['user']}:{DATABASE_CONFIG['password']}@{DATABASE_CONFIG['host']}:{DATABASE_CONFIG['port']}/{DATABASE_CONFIG['database']}\"\n",
    "    engine = create_engine(connection_string, pool_size=5, max_overflow=10)\n",
    "    return engine\n",
    "\n",
    "# Test database connection\n",
    "try:\n",
    "    engine = create_database_connection()\n",
    "    test_query = \"SELECT 1 as test\"\n",
    "    test_result = pd.read_sql(test_query, engine)\n",
    "    print(\"✅ Database connection successful!\")\n",
    "    print(f\"Test result: {test_result.iloc[0, 0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Database connection failed: {str(e)}\")\n",
    "    print(\"Please check your .env file and database credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comprehensive_evaluation"
   },
   "source": [
    "# Comprehensive Model Evaluation Framework\n",
    "\n",
    "## Advanced Validation Techniques\n",
    "\n",
    "We'll implement a comprehensive evaluation framework that goes beyond basic train-test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation_data_prep"
   },
   "outputs": [],
   "source": [
    "def prepare_evaluation_dataset():\n",
    "    \"\"\"\n",
    "    Prepare a comprehensive dataset for model evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Evaluation dataset with temporal information\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH order_metrics AS (\n",
    "        SELECT \n",
    "            o.order_id,\n",
    "            o.customer_id,\n",
    "            o.order_purchase_timestamp,\n",
    "            c.customer_state,\n",
    "            \n",
    "            -- Target: Order value\n",
    "            SUM(oi.price + oi.freight_value) as order_value,\n",
    "            \n",
    "            -- Features: Order characteristics\n",
    "            COUNT(oi.order_item_id) as item_count,\n",
    "            AVG(oi.price) as avg_item_price,\n",
    "            SUM(oi.freight_value) as total_freight,\n",
    "            COUNT(DISTINCT p.product_category_name_english) as category_count,\n",
    "            AVG(p.product_weight_g) as avg_product_weight,\n",
    "            \n",
    "            -- Features: Customer history (at time of order)\n",
    "            COUNT(DISTINCT o_hist.order_id) as customer_previous_orders,\n",
    "            COALESCE(SUM(oi_hist.price + oi_hist.freight_value), 0) as customer_previous_spending,\n",
    "            \n",
    "            -- Features: Temporal\n",
    "            EXTRACT(MONTH FROM o.order_purchase_timestamp) as order_month,\n",
    "            EXTRACT(DOW FROM o.order_purchase_timestamp) as order_dow,\n",
    "            EXTRACT(QUARTER FROM o.order_purchase_timestamp) as order_quarter,\n",
    "            \n",
    "            -- Features: Market context\n",
    "            COUNT(*) OVER (\n",
    "                PARTITION BY DATE_TRUNC('month', o.order_purchase_timestamp)\n",
    "            ) as monthly_order_volume\n",
    "            \n",
    "        FROM \"olist_sales_data_set\".\"olist_orders_dataset\" o\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_customers_dataset\" c \n",
    "            ON o.customer_id = c.customer_id\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_order_items_dataset\" oi \n",
    "            ON o.order_id = oi.order_id\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_products_dataset\" p \n",
    "            ON oi.product_id = p.product_id\n",
    "        \n",
    "        -- Customer history (orders before current order)\n",
    "        LEFT JOIN \"olist_sales_data_set\".\"olist_orders_dataset\" o_hist \n",
    "            ON o.customer_id = o_hist.customer_id \n",
    "            AND o_hist.order_purchase_timestamp < o.order_purchase_timestamp\n",
    "            AND o_hist.order_status = 'delivered'\n",
    "        LEFT JOIN \"olist_sales_data_set\".\"olist_order_items_dataset\" oi_hist \n",
    "            ON o_hist.order_id = oi_hist.order_id\n",
    "            \n",
    "        WHERE o.order_status = 'delivered'\n",
    "            AND p.product_weight_g IS NOT NULL\n",
    "            AND o.order_purchase_timestamp >= '2017-01-01'  -- Focus on complete year\n",
    "            AND o.order_purchase_timestamp < '2019-01-01'\n",
    "        GROUP BY \n",
    "            o.order_id, o.customer_id, o.order_purchase_timestamp, c.customer_state\n",
    "        HAVING SUM(oi.price + oi.freight_value) > 0\n",
    "            AND SUM(oi.price + oi.freight_value) < 1000  -- Remove extreme outliers\n",
    "    ),\n",
    "    enhanced_metrics AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            -- Customer value features\n",
    "            CASE \n",
    "                WHEN customer_previous_orders = 0 THEN 0\n",
    "                ELSE customer_previous_spending / customer_previous_orders\n",
    "            END as customer_avg_order_value,\n",
    "            \n",
    "            CASE \n",
    "                WHEN customer_previous_orders = 0 THEN 'New'\n",
    "                WHEN customer_previous_orders = 1 THEN 'Returning'\n",
    "                ELSE 'Loyal'\n",
    "            END as customer_segment,\n",
    "            \n",
    "            -- Regional mapping\n",
    "            CASE \n",
    "                WHEN customer_state IN ('SP', 'RJ', 'ES', 'MG') THEN 'Southeast'\n",
    "                WHEN customer_state IN ('PR', 'SC', 'RS') THEN 'South'\n",
    "                WHEN customer_state IN ('GO', 'MT', 'MS', 'DF') THEN 'Center-West'\n",
    "                WHEN customer_state IN ('BA', 'SE', 'PE', 'AL', 'PB', 'RN', 'CE', 'PI', 'MA') THEN 'Northeast'\n",
    "                WHEN customer_state IN ('AM', 'RR', 'AP', 'PA', 'TO', 'RO', 'AC') THEN 'North'\n",
    "                ELSE 'Other'\n",
    "            END as region,\n",
    "            \n",
    "            -- Time-based features for temporal validation\n",
    "            DATE_TRUNC('month', order_purchase_timestamp) as order_month_date\n",
    "            \n",
    "        FROM order_metrics\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM enhanced_metrics\n",
    "    WHERE region != 'Other'\n",
    "    ORDER BY order_purchase_timestamp\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.read_sql(query, engine)\n",
    "\n",
    "# Load evaluation dataset\n",
    "eval_df = prepare_evaluation_dataset()\n",
    "\n",
    "print(f\"Loaded {len(eval_df):,} orders for comprehensive evaluation\")\n",
    "print(f\"Date range: {eval_df['order_purchase_timestamp'].min()} to {eval_df['order_purchase_timestamp'].max()}\")\n",
    "print(f\"\\nCustomer segments:\")\n",
    "print(eval_df['customer_segment'].value_counts())\n",
    "\n",
    "# Visualize data distribution over time\n",
    "monthly_orders = eval_df.groupby('order_month_date').agg({\n",
    "    'order_id': 'count',\n",
    "    'order_value': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "axes[0].plot(monthly_orders['order_month_date'], monthly_orders['order_id'], marker='o')\n",
    "axes[0].set_title('Monthly Order Volume Over Time')\n",
    "axes[0].set_ylabel('Number of Orders')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(monthly_orders['order_month_date'], monthly_orders['order_value'], marker='s', color='orange')\n",
    "axes[1].set_title('Average Order Value Over Time')\n",
    "axes[1].set_ylabel('Average Order Value (R$)')\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "advanced_validation"
   },
   "source": [
    "## Advanced Cross-Validation Strategies\n",
    "\n",
    "We'll implement multiple validation strategies appropriate for business contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validation_strategies"
   },
   "outputs": [],
   "source": [
    "def comprehensive_model_validation(data, target_col='order_value'):\n",
    "    \"\"\"\n",
    "    Implement comprehensive model validation strategies.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataset for validation\n",
    "        target_col (str): Target variable column\n",
    "    \n",
    "    Returns:\n",
    "        dict: Validation results across different strategies\n",
    "    \"\"\"\n",
    "    print(\"COMPREHENSIVE MODEL VALIDATION\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Prepare features\n",
    "    feature_columns = [\n",
    "        'item_count', 'avg_item_price', 'total_freight',\n",
    "        'category_count', 'avg_product_weight',\n",
    "        'customer_previous_orders', 'customer_avg_order_value',\n",
    "        'order_month', 'order_dow', 'monthly_order_volume'\n",
    "    ]\n",
    "    \n",
    "    # Add categorical variables\n",
    "    categorical_features = ['customer_segment', 'region']\n",
    "    for cat_feature in categorical_features:\n",
    "        dummies = pd.get_dummies(data[cat_feature], prefix=cat_feature)\n",
    "        data = pd.concat([data, dummies], axis=1)\n",
    "        feature_columns.extend(dummies.columns.tolist())\n",
    "    \n",
    "    # Prepare clean dataset\n",
    "    clean_data = data[feature_columns + [target_col, 'order_purchase_timestamp']].dropna()\n",
    "    X = clean_data[feature_columns]\n",
    "    y = clean_data[target_col]\n",
    "    timestamps = clean_data['order_purchase_timestamp']\n",
    "    \n",
    "    print(f\"Features: {len(feature_columns)}\")\n",
    "    print(f\"Sample size: {len(clean_data):,}\")\n",
    "    \n",
    "    # Models to evaluate\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge (α=1.0)': Ridge(alpha=1.0),\n",
    "        'Ridge (α=10.0)': Ridge(alpha=10.0),\n",
    "        'Lasso (α=0.1)': Lasso(alpha=0.1)\n",
    "    }\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # 1. Traditional K-Fold Cross-Validation\n",
    "    print(\"\\n1. K-FOLD CROSS-VALIDATION (k=5)\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        cv_scores = cross_val_score(model, X, y, cv=kfold, scoring='r2')\n",
    "        validation_results[f'{name}_kfold'] = {\n",
    "            'mean_r2': cv_scores.mean(),\n",
    "            'std_r2': cv_scores.std(),\n",
    "            'scores': cv_scores\n",
    "        }\n",
    "        print(f\"{name:<20} R² = {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "    \n",
    "    # 2. Time Series Cross-Validation\n",
    "    print(\"\\n2. TIME SERIES CROSS-VALIDATION\")\n",
    "    print(\"-\" * 32)\n",
    "    \n",
    "    # Sort data by timestamp\n",
    "    time_sorted_idx = timestamps.sort_values().index\n",
    "    X_time_sorted = X.loc[time_sorted_idx]\n",
    "    y_time_sorted = y.loc[time_sorted_idx]\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        cv_scores = cross_val_score(model, X_time_sorted, y_time_sorted, cv=tscv, scoring='r2')\n",
    "        validation_results[f'{name}_timeseries'] = {\n",
    "            'mean_r2': cv_scores.mean(),\n",
    "            'std_r2': cv_scores.std(),\n",
    "            'scores': cv_scores\n",
    "        }\n",
    "        print(f\"{name:<20} R² = {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "    \n",
    "    # 3. Customer-based Cross-Validation (Group-based)\n",
    "    print(\"\\n3. CUSTOMER-BASED VALIDATION\")\n",
    "    print(\"-\" * 27)\n",
    "    \n",
    "    # Use customer segments as groups\n",
    "    customer_groups = clean_data['customer_segment']\n",
    "    \n",
    "    # Manual customer-based split\n",
    "    unique_customers = clean_data.groupby('customer_segment').apply(\n",
    "        lambda x: x.sample(frac=0.8, random_state=42).index\n",
    "    ).explode().values\n",
    "    \n",
    "    train_idx = clean_data.index.isin(unique_customers)\n",
    "    test_idx = ~train_idx\n",
    "    \n",
    "    X_train_cust = X[train_idx]\n",
    "    X_test_cust = X[test_idx]\n",
    "    y_train_cust = y[train_idx]\n",
    "    y_test_cust = y[test_idx]\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_cust, y_train_cust)\n",
    "        y_pred_cust = model.predict(X_test_cust)\n",
    "        r2_cust = r2_score(y_test_cust, y_pred_cust)\n",
    "        \n",
    "        validation_results[f'{name}_customer'] = {\n",
    "            'r2': r2_cust,\n",
    "            'predictions': y_pred_cust,\n",
    "            'actuals': y_test_cust\n",
    "        }\n",
    "        print(f\"{name:<20} R² = {r2_cust:.4f}\")\n",
    "    \n",
    "    # 4. Baseline Model Comparison\n",
    "    print(\"\\n4. BASELINE MODEL COMPARISON\")\n",
    "    print(\"-\" * 27)\n",
    "    \n",
    "    baseline_models = {\n",
    "        'Mean Predictor': DummyRegressor(strategy='mean'),\n",
    "        'Median Predictor': DummyRegressor(strategy='median')\n",
    "    }\n",
    "    \n",
    "    for name, baseline in baseline_models.items():\n",
    "        cv_scores = cross_val_score(baseline, X, y, cv=kfold, scoring='r2')\n",
    "        validation_results[f'{name}_baseline'] = {\n",
    "            'mean_r2': cv_scores.mean(),\n",
    "            'std_r2': cv_scores.std()\n",
    "        }\n",
    "        print(f\"{name:<20} R² = {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"VALIDATION STRATEGY COMPARISON:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Compare best model across strategies\n",
    "    best_model_per_strategy = {}\n",
    "    \n",
    "    for strategy in ['kfold', 'timeseries', 'customer']:\n",
    "        strategy_results = {k: v for k, v in validation_results.items() if strategy in k and 'baseline' not in k}\n",
    "        \n",
    "        if strategy == 'customer':\n",
    "            best_model = max(strategy_results.items(), key=lambda x: x[1]['r2'])\n",
    "            best_model_per_strategy[strategy] = (best_model[0], best_model[1]['r2'])\n",
    "        else:\n",
    "            best_model = max(strategy_results.items(), key=lambda x: x[1]['mean_r2'])\n",
    "            best_model_per_strategy[strategy] = (best_model[0], best_model[1]['mean_r2'])\n",
    "    \n",
    "    for strategy, (model_name, score) in best_model_per_strategy.items():\n",
    "        print(f\"{strategy.title():<15}: {model_name.split('_')[0]:<20} R² = {score:.4f}\")\n",
    "    \n",
    "    return validation_results, feature_columns, clean_data\n",
    "\n",
    "# Run comprehensive validation\n",
    "validation_results, feature_columns, clean_data = comprehensive_model_validation(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_interpretation"
   },
   "source": [
    "## Model Interpretation and Feature Importance\n",
    "\n",
    "Understanding which features drive model predictions is crucial for business implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_importance_analysis"
   },
   "outputs": [],
   "source": [
    "def comprehensive_feature_importance_analysis(data, feature_columns, target_col='order_value'):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using multiple methods.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Clean dataset\n",
    "        feature_columns (list): List of feature column names\n",
    "        target_col (str): Target variable column\n",
    "    \n",
    "    Returns:\n",
    "        dict: Feature importance results\n",
    "    \"\"\"\n",
    "    print(\"COMPREHENSIVE FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    X = data[feature_columns]\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # Split data for consistent analysis\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    importance_results = {}\n",
    "    \n",
    "    # 1. Linear Regression Coefficients\n",
    "    print(\"\\n1. LINEAR REGRESSION COEFFICIENTS\")\n",
    "    print(\"-\" * 33)\n",
    "    \n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    lr_importance = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Coefficient': lr_model.coef_,\n",
    "        'Abs_Coefficient': np.abs(lr_model.coef_)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    importance_results['linear_regression'] = lr_importance\n",
    "    \n",
    "    print(\"Top 10 Features by Coefficient Magnitude:\")\n",
    "    for _, row in lr_importance.head(10).iterrows():\n",
    "        direction = \"↑\" if row['Coefficient'] > 0 else \"↓\"\n",
    "        print(f\"{row['Feature']:<30} {row['Coefficient']:<10.3f} {direction}\")\n",
    "    \n",
    "    # 2. Regularized Model Feature Selection\n",
    "    print(\"\\n2. LASSO REGULARIZATION (Feature Selection)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Try different alpha values to see feature selection\n",
    "    alphas = [0.01, 0.1, 1.0, 10.0]\n",
    "    lasso_results = {}\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        lasso_model = Lasso(alpha=alpha)\n",
    "        lasso_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Count non-zero coefficients\n",
    "        non_zero_features = np.sum(lasso_model.coef_ != 0)\n",
    "        r2_score_val = r2_score(y_test, lasso_model.predict(X_test))\n",
    "        \n",
    "        lasso_results[alpha] = {\n",
    "            'non_zero_features': non_zero_features,\n",
    "            'r2_score': r2_score_val,\n",
    "            'coefficients': lasso_model.coef_\n",
    "        }\n",
    "        \n",
    "        print(f\"Alpha = {alpha:<6} → {non_zero_features:>2} features selected, R² = {r2_score_val:.4f}\")\n",
    "    \n",
    "    # Select best alpha and show selected features\n",
    "    best_alpha = max(lasso_results.keys(), key=lambda a: lasso_results[a]['r2_score'])\n",
    "    best_lasso_coef = lasso_results[best_alpha]['coefficients']\n",
    "    \n",
    "    selected_features = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Coefficient': best_lasso_coef,\n",
    "        'Selected': best_lasso_coef != 0\n",
    "    })\n",
    "    \n",
    "    importance_results['lasso_selection'] = selected_features\n",
    "    \n",
    "    print(f\"\\nBest Alpha = {best_alpha} - Selected Features:\")\n",
    "    selected_only = selected_features[selected_features['Selected']].sort_values(\n",
    "        'Coefficient', key=abs, ascending=False\n",
    "    )\n",
    "    for _, row in selected_only.iterrows():\n",
    "        direction = \"↑\" if row['Coefficient'] > 0 else \"↓\"\n",
    "        print(f\"  {row['Feature']:<30} {row['Coefficient']:<10.3f} {direction}\")\n",
    "    \n",
    "    # 3. Permutation Importance\n",
    "    print(\"\\n3. PERMUTATION IMPORTANCE\")\n",
    "    print(\"-\" * 24)\n",
    "    \n",
    "    # Use Ridge model for permutation importance\n",
    "    ridge_model = Ridge(alpha=1.0)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    \n",
    "    perm_importance = permutation_importance(\n",
    "        ridge_model, X_test, y_test, \n",
    "        n_repeats=10, random_state=42, scoring='r2'\n",
    "    )\n",
    "    \n",
    "    perm_results = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Importance_Mean': perm_importance.importances_mean,\n",
    "        'Importance_Std': perm_importance.importances_std\n",
    "    }).sort_values('Importance_Mean', ascending=False)\n",
    "    \n",
    "    importance_results['permutation'] = perm_results\n",
    "    \n",
    "    print(\"Top 10 Features by Permutation Importance:\")\n",
    "    for _, row in perm_results.head(10).iterrows():\n",
    "        print(f\"{row['Feature']:<30} {row['Importance_Mean']:<10.4f} (±{row['Importance_Std']:.4f})\")\n",
    "    \n",
    "    # 4. Correlation-based Importance\n",
    "    print(\"\\n4. CORRELATION-BASED IMPORTANCE\")\n",
    "    print(\"-\" * 31)\n",
    "    \n",
    "    correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "    \n",
    "    correlation_results = pd.DataFrame({\n",
    "        'Feature': correlations.index,\n",
    "        'Correlation': correlations.values\n",
    "    })\n",
    "    \n",
    "    importance_results['correlation'] = correlation_results\n",
    "    \n",
    "    print(\"Top 10 Features by Correlation:\")\n",
    "    for _, row in correlation_results.head(10).iterrows():\n",
    "        print(f\"{row['Feature']:<30} {row['Correlation']:<10.4f}\")\n",
    "    \n",
    "    # Create comprehensive importance visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "    \n",
    "    # Plot 1: Linear Regression Coefficients\n",
    "    top_lr = lr_importance.head(15)\n",
    "    axes[0, 0].barh(range(len(top_lr)), top_lr['Coefficient'])\n",
    "    axes[0, 0].set_yticks(range(len(top_lr)))\n",
    "    axes[0, 0].set_yticklabels(top_lr['Feature'], fontsize=10)\n",
    "    axes[0, 0].set_title('Linear Regression Coefficients')\n",
    "    axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 2: Lasso Feature Selection\n",
    "    selected_features_plot = selected_only.head(15)\n",
    "    colors = ['green' if c > 0 else 'red' for c in selected_features_plot['Coefficient']]\n",
    "    axes[0, 1].barh(range(len(selected_features_plot)), selected_features_plot['Coefficient'], color=colors)\n",
    "    axes[0, 1].set_yticks(range(len(selected_features_plot)))\n",
    "    axes[0, 1].set_yticklabels(selected_features_plot['Feature'], fontsize=10)\n",
    "    axes[0, 1].set_title(f'Lasso Selected Features (α={best_alpha})')\n",
    "    axes[0, 1].axvline(x=0, color='black', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 3: Permutation Importance\n",
    "    top_perm = perm_results.head(15)\n",
    "    axes[1, 0].barh(range(len(top_perm)), top_perm['Importance_Mean'])\n",
    "    axes[1, 0].set_yticks(range(len(top_perm)))\n",
    "    axes[1, 0].set_yticklabels(top_perm['Feature'], fontsize=10)\n",
    "    axes[1, 0].set_title('Permutation Importance')\n",
    "    \n",
    "    # Plot 4: Correlation\n",
    "    top_corr = correlation_results.head(15)\n",
    "    axes[1, 1].barh(range(len(top_corr)), top_corr['Correlation'])\n",
    "    axes[1, 1].set_yticks(range(len(top_corr)))\n",
    "    axes[1, 1].set_yticklabels(top_corr['Feature'], fontsize=10)\n",
    "    axes[1, 1].set_title('Feature-Target Correlation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Business insights summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BUSINESS INSIGHTS FROM FEATURE IMPORTANCE:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Find consistently important features across methods\n",
    "    top_features_lr = set(lr_importance.head(10)['Feature'])\n",
    "    top_features_perm = set(perm_results.head(10)['Feature'])\n",
    "    top_features_corr = set(correlation_results.head(10)['Feature'])\n",
    "    selected_features_set = set(selected_only['Feature'])\n",
    "    \n",
    "    consistent_features = top_features_lr.intersection(\n",
    "        top_features_perm\n",
    "    ).intersection(top_features_corr)\n",
    "    \n",
    "    print(f\"🎯 MOST CONSISTENT PREDICTORS (across all methods):\")\n",
    "    for feature in consistent_features:\n",
    "        print(f\"   • {feature.replace('_', ' ').title()}\")\n",
    "    \n",
    "    print(f\"\\n🔍 LASSO SELECTED FEATURES ({len(selected_only)} total):\")\n",
    "    for feature in selected_only['Feature'][:5]:\n",
    "        print(f\"   • {feature.replace('_', ' ').title()}\")\n",
    "    \n",
    "    return importance_results\n",
    "\n",
    "# Run feature importance analysis\n",
    "importance_results = comprehensive_feature_importance_analysis(\n",
    "    clean_data, feature_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab_testing_framework"
   },
   "source": [
    "# A/B Testing Framework for Model Deployment\n",
    "\n",
    "## Designing Production Model Testing\n",
    "\n",
    "We'll create a framework for safely deploying models in production through A/B testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab_testing_deployment"
   },
   "outputs": [],
   "source": [
    "def design_model_ab_test_framework(data, target_col='order_value'):\n",
    "    \"\"\"\n",
    "    Design an A/B testing framework for model deployment.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Historical data for simulation\n",
    "        target_col (str): Target variable\n",
    "    \n",
    "    Returns:\n",
    "        dict: A/B testing framework and results\n",
    "    \"\"\"\n",
    "    print(\"MODEL A/B TESTING FRAMEWORK\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Prepare models for comparison\n",
    "    X = data[feature_columns]\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # Split into train and \"production\" test sets\n",
    "    X_train, X_prod, y_train, y_prod = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train candidate models\n",
    "    models = {\n",
    "        'Current_Model': LinearRegression(),  # Baseline \"current\" model\n",
    "        'New_Model': Ridge(alpha=1.0)         # New \"challenger\" model\n",
    "    }\n",
    "    \n",
    "    trained_models = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[name] = model\n",
    "    \n",
    "    # Simulate A/B test on production data\n",
    "    print(\"\\n1. A/B TEST SIMULATION SETUP\")\n",
    "    print(\"-\" * 28)\n",
    "    \n",
    "    # Randomly assign production samples to A (current) or B (new) groups\n",
    "    np.random.seed(42)\n",
    "    test_assignment = np.random.choice(['A', 'B'], size=len(X_prod), p=[0.5, 0.5])\n",
    "    \n",
    "    group_A_idx = test_assignment == 'A'\n",
    "    group_B_idx = test_assignment == 'B'\n",
    "    \n",
    "    # Generate predictions for each group\n",
    "    group_A_pred = trained_models['Current_Model'].predict(X_prod[group_A_idx])\n",
    "    group_B_pred = trained_models['New_Model'].predict(X_prod[group_B_idx])\n",
    "    \n",
    "    group_A_actual = y_prod[group_A_idx]\n",
    "    group_B_actual = y_prod[group_B_idx]\n",
    "    \n",
    "    print(f\"Group A (Current Model): {len(group_A_pred):,} samples\")\n",
    "    print(f\"Group B (New Model): {len(group_B_pred):,} samples\")\n",
    "    \n",
    "    # Calculate performance metrics for each group\n",
    "    print(\"\\n2. PERFORMANCE COMPARISON\")\n",
    "    print(\"-\" * 24)\n",
    "    \n",
    "    metrics_A = {\n",
    "        'r2': r2_score(group_A_actual, group_A_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(group_A_actual, group_A_pred)),\n",
    "        'mae': mean_absolute_error(group_A_actual, group_A_pred),\n",
    "        'mape': mean_absolute_percentage_error(group_A_actual, group_A_pred) * 100\n",
    "    }\n",
    "    \n",
    "    metrics_B = {\n",
    "        'r2': r2_score(group_B_actual, group_B_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(group_B_actual, group_B_pred)),\n",
    "        'mae': mean_absolute_error(group_B_actual, group_B_pred),\n",
    "        'mape': mean_absolute_percentage_error(group_B_actual, group_B_pred) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"{'Metric':<15} {'Group A (Current)':<18} {'Group B (New)':<15} {'Improvement':<12}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for metric in metrics_A.keys():\n",
    "        value_A = metrics_A[metric]\n",
    "        value_B = metrics_B[metric]\n",
    "        \n",
    "        if metric == 'r2':\n",
    "            improvement = ((value_B - value_A) / abs(value_A)) * 100\n",
    "        else:\n",
    "            improvement = ((value_A - value_B) / value_A) * 100  # Lower is better\n",
    "        \n",
    "        print(f\"{metric.upper():<15} {value_A:<18.4f} {value_B:<15.4f} {improvement:>+8.2f}%\")\n",
    "    \n",
    "    # Statistical significance testing\n",
    "    print(\"\\n3. STATISTICAL SIGNIFICANCE TESTING\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Test for significant difference in prediction errors\n",
    "    errors_A = np.abs(group_A_actual - group_A_pred)\n",
    "    errors_B = np.abs(group_B_actual - group_B_pred)\n",
    "    \n",
    "    # Two-sample t-test for error differences\n",
    "    t_stat, p_value = stats.ttest_ind(errors_A, errors_B)\n",
    "    \n",
    "    print(f\"Two-sample t-test on absolute errors:\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.6f}\")\n",
    "    \n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        winner = \"Group B (New Model)\" if errors_B.mean() < errors_A.mean() else \"Group A (Current Model)\"\n",
    "        print(f\"  ✅ SIGNIFICANT difference detected (p < {alpha})\")\n",
    "        print(f\"  🏆 Winner: {winner}\")\n",
    "    else:\n",
    "        print(f\"  ❌ No significant difference (p >= {alpha})\")\n",
    "        print(f\"  → Continue with current model or extend test duration\")\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt(((len(errors_A) - 1) * errors_A.var() + \n",
    "                          (len(errors_B) - 1) * errors_B.var()) / \n",
    "                         (len(errors_A) + len(errors_B) - 2))\n",
    "    cohens_d = (errors_A.mean() - errors_B.mean()) / pooled_std\n",
    "    \n",
    "    print(f\"  Effect size (Cohen's d): {cohens_d:.4f}\")\n",
    "    \n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_interpretation = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_interpretation = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_interpretation = \"medium\"\n",
    "    else:\n",
    "        effect_interpretation = \"large\"\n",
    "    \n",
    "    print(f\"  Effect size interpretation: {effect_interpretation}\")\n",
    "    \n",
    "    # Business impact assessment\n",
    "    print(\"\\n4. BUSINESS IMPACT ASSESSMENT\")\n",
    "    print(\"-\" * 29)\n",
    "    \n",
    "    # Calculate potential business value\n",
    "    avg_order_value = y_prod.mean()\n",
    "    monthly_orders = 10000  # Assumption for business calculation\n",
    "    \n",
    "    mae_improvement = metrics_A['mae'] - metrics_B['mae']\n",
    "    mape_improvement = metrics_A['mape'] - metrics_B['mape']\n",
    "    \n",
    "    # Estimate business value of improved predictions\n",
    "    monthly_value_improvement = mae_improvement * monthly_orders\n",
    "    accuracy_improvement = mape_improvement\n",
    "    \n",
    "    print(f\"Business Impact Estimates:\")\n",
    "    print(f\"  Average Order Value: R$ {avg_order_value:.2f}\")\n",
    "    print(f\"  Assumed Monthly Orders: {monthly_orders:,}\")\n",
    "    print(f\"  MAE Improvement: R$ {mae_improvement:.2f} per prediction\")\n",
    "    print(f\"  MAPE Improvement: {accuracy_improvement:.2f} percentage points\")\n",
    "    print(f\"  Monthly Value Impact: R$ {monthly_value_improvement:,.2f}\")\n",
    "    \n",
    "    if mae_improvement > 0:\n",
    "        print(f\"  💰 Estimated annual value: R$ {monthly_value_improvement * 12:,.2f}\")\n",
    "        print(f\"  📈 Recommendation: Deploy new model\")\n",
    "    else:\n",
    "        print(f\"  ⚠️  New model shows no improvement\")\n",
    "        print(f\"  📉 Recommendation: Keep current model\")\n",
    "    \n",
    "    # Visualization of A/B test results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Prediction accuracy comparison\n",
    "    axes[0, 0].scatter(group_A_actual, group_A_pred, alpha=0.5, label='Group A (Current)', s=20)\n",
    "    axes[0, 0].scatter(group_B_actual, group_B_pred, alpha=0.5, label='Group B (New)', s=20)\n",
    "    axes[0, 0].plot([y_prod.min(), y_prod.max()], [y_prod.min(), y_prod.max()], 'r--', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Actual Order Value')\n",
    "    axes[0, 0].set_ylabel('Predicted Order Value')\n",
    "    axes[0, 0].set_title('Prediction Accuracy Comparison')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Error distribution comparison\n",
    "    axes[0, 1].hist(errors_A, bins=30, alpha=0.7, label='Group A (Current)', density=True)\n",
    "    axes[0, 1].hist(errors_B, bins=30, alpha=0.7, label='Group B (New)', density=True)\n",
    "    axes[0, 1].set_xlabel('Absolute Prediction Error')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].set_title('Error Distribution Comparison')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Residuals plot for Group A\n",
    "    residuals_A = group_A_actual - group_A_pred\n",
    "    axes[1, 0].scatter(group_A_pred, residuals_A, alpha=0.5, s=20)\n",
    "    axes[1, 0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[1, 0].set_xlabel('Predicted Values')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].set_title('Group A (Current Model) Residuals')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Residuals plot for Group B\n",
    "    residuals_B = group_B_actual - group_B_pred\n",
    "    axes[1, 1].scatter(group_B_pred, residuals_B, alpha=0.5, s=20)\n",
    "    axes[1, 1].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[1, 1].set_xlabel('Predicted Values')\n",
    "    axes[1, 1].set_ylabel('Residuals')\n",
    "    axes[1, 1].set_title('Group B (New Model) Residuals')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'metrics_A': metrics_A,\n",
    "        'metrics_B': metrics_B,\n",
    "        'statistical_test': {'t_stat': t_stat, 'p_value': p_value, 'cohens_d': cohens_d},\n",
    "        'business_impact': {\n",
    "            'mae_improvement': mae_improvement,\n",
    "            'monthly_value_improvement': monthly_value_improvement\n",
    "        },\n",
    "        'trained_models': trained_models\n",
    "    }\n",
    "\n",
    "# Run A/B testing framework\n",
    "ab_test_results = design_model_ab_test_framework(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "production_monitoring"
   },
   "source": [
    "# Production Model Monitoring Framework\n",
    "\n",
    "## Monitoring Model Performance and Drift\n",
    "\n",
    "We'll create a comprehensive framework for monitoring models in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "monitoring_framework"
   },
   "outputs": [],
   "source": [
    "def create_production_monitoring_framework(data, target_col='order_value'):\n",
    "    \"\"\"\n",
    "    Create a comprehensive production monitoring framework.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Historical data for monitoring simulation\n",
    "        target_col (str): Target variable\n",
    "    \n",
    "    Returns:\n",
    "        dict: Monitoring framework and alerts\n",
    "    \"\"\"\n",
    "    print(\"PRODUCTION MODEL MONITORING FRAMEWORK\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Simulate model deployment timeline\n",
    "    data_sorted = data.sort_values('order_purchase_timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Split into training, validation, and production periods\n",
    "    n_total = len(data_sorted)\n",
    "    n_train = int(0.6 * n_total)\n",
    "    n_val = int(0.2 * n_total)\n",
    "    \n",
    "    train_data = data_sorted[:n_train]\n",
    "    val_data = data_sorted[n_train:n_train + n_val]\n",
    "    prod_data = data_sorted[n_train + n_val:]\n",
    "    \n",
    "    print(f\"Training period: {len(train_data):,} samples\")\n",
    "    print(f\"Validation period: {len(val_data):,} samples\")\n",
    "    print(f\"Production period: {len(prod_data):,} samples\")\n",
    "    \n",
    "    # Train production model\n",
    "    X_train = train_data[feature_columns]\n",
    "    y_train = train_data[target_col]\n",
    "    \n",
    "    production_model = Ridge(alpha=1.0)\n",
    "    production_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Establish baseline performance on validation set\n",
    "    X_val = val_data[feature_columns]\n",
    "    y_val = val_data[target_col]\n",
    "    y_val_pred = production_model.predict(X_val)\n",
    "    \n",
    "    baseline_metrics = {\n",
    "        'r2': r2_score(y_val, y_val_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "        'mae': mean_absolute_error(y_val, y_val_pred),\n",
    "        'mape': mean_absolute_percentage_error(y_val, y_val_pred) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n1. BASELINE PERFORMANCE METRICS\")\n",
    "    print(\"-\" * 31)\n",
    "    for metric, value in baseline_metrics.items():\n",
    "        print(f\"{metric.upper()}: {value:.4f}\")\n",
    "    \n",
    "    # Define monitoring thresholds\n",
    "    monitoring_thresholds = {\n",
    "        'r2_min': baseline_metrics['r2'] * 0.9,  # 10% degradation threshold\n",
    "        'rmse_max': baseline_metrics['rmse'] * 1.2,  # 20% increase threshold\n",
    "        'mae_max': baseline_metrics['mae'] * 1.2,\n",
    "        'mape_max': baseline_metrics['mape'] * 1.2\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n2. MONITORING THRESHOLDS\")\n",
    "    print(\"-\" * 22)\n",
    "    print(f\"R² minimum: {monitoring_thresholds['r2_min']:.4f}\")\n",
    "    print(f\"RMSE maximum: {monitoring_thresholds['rmse_max']:.4f}\")\n",
    "    print(f\"MAE maximum: {monitoring_thresholds['mae_max']:.4f}\")\n",
    "    print(f\"MAPE maximum: {monitoring_thresholds['mape_max']:.2f}%\")\n",
    "    \n",
    "    # Simulate weekly monitoring during production\n",
    "    prod_data['week'] = (prod_data.index // (len(prod_data) // 10)).astype(int)  # 10 monitoring periods\n",
    "    \n",
    "    monitoring_results = []\n",
    "    alerts = []\n",
    "    \n",
    "    print(f\"\\n3. WEEKLY MONITORING RESULTS\")\n",
    "    print(\"-\" * 27)\n",
    "    print(f\"{'Week':<6} {'R²':<8} {'RMSE':<8} {'MAE':<8} {'MAPE%':<8} {'Alerts':<20}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for week in sorted(prod_data['week'].unique()):\n",
    "        week_data = prod_data[prod_data['week'] == week]\n",
    "        \n",
    "        if len(week_data) < 10:  # Skip weeks with insufficient data\n",
    "            continue\n",
    "        \n",
    "        X_week = week_data[feature_columns]\n",
    "        y_week = week_data[target_col]\n",
    "        y_week_pred = production_model.predict(X_week)\n",
    "        \n",
    "        week_metrics = {\n",
    "            'week': week,\n",
    "            'r2': r2_score(y_week, y_week_pred),\n",
    "            'rmse': np.sqrt(mean_squared_error(y_week, y_week_pred)),\n",
    "            'mae': mean_absolute_error(y_week, y_week_pred),\n",
    "            'mape': mean_absolute_percentage_error(y_week, y_week_pred) * 100,\n",
    "            'sample_size': len(week_data)\n",
    "        }\n",
    "        \n",
    "        # Check for threshold violations\n",
    "        week_alerts = []\n",
    "        if week_metrics['r2'] < monitoring_thresholds['r2_min']:\n",
    "            week_alerts.append('R² LOW')\n",
    "        if week_metrics['rmse'] > monitoring_thresholds['rmse_max']:\n",
    "            week_alerts.append('RMSE HIGH')\n",
    "        if week_metrics['mae'] > monitoring_thresholds['mae_max']:\n",
    "            week_alerts.append('MAE HIGH')\n",
    "        if week_metrics['mape'] > monitoring_thresholds['mape_max']:\n",
    "            week_alerts.append('MAPE HIGH')\n",
    "        \n",
    "        week_metrics['alerts'] = week_alerts\n",
    "        monitoring_results.append(week_metrics)\n",
    "        \n",
    "        if week_alerts:\n",
    "            alerts.extend([(week, alert) for alert in week_alerts])\n",
    "        \n",
    "        alert_str = ', '.join(week_alerts) if week_alerts else 'None'\n",
    "        status_icon = '🚨' if week_alerts else '✅'\n",
    "        \n",
    "        print(f\"{week:<6} {week_metrics['r2']:<8.4f} {week_metrics['rmse']:<8.2f} \"\n",
    "              f\"{week_metrics['mae']:<8.2f} {week_metrics['mape']:<8.2f} {status_icon} {alert_str}\")\n",
    "    \n",
    "    # Feature drift detection\n",
    "    print(f\"\\n4. FEATURE DRIFT DETECTION\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # Compare feature distributions between training and recent production data\n",
    "    recent_prod_data = prod_data[prod_data['week'] >= prod_data['week'].max() - 2]  # Last 2 weeks\n",
    "    \n",
    "    feature_drift_results = {}\n",
    "    significant_drifts = []\n",
    "    \n",
    "    # Test key numerical features for distribution changes\n",
    "    key_features = ['item_count', 'avg_item_price', 'total_freight', 'category_count']\n",
    "    \n",
    "    for feature in key_features:\n",
    "        if feature in train_data.columns and feature in recent_prod_data.columns:\n",
    "            train_values = train_data[feature].dropna()\n",
    "            prod_values = recent_prod_data[feature].dropna()\n",
    "            \n",
    "            if len(train_values) > 30 and len(prod_values) > 30:\n",
    "                # Kolmogorov-Smirnov test for distribution difference\n",
    "                ks_stat, ks_p_value = stats.ks_2samp(train_values, prod_values)\n",
    "                \n",
    "                feature_drift_results[feature] = {\n",
    "                    'ks_statistic': ks_stat,\n",
    "                    'p_value': ks_p_value,\n",
    "                    'drift_detected': ks_p_value < 0.05\n",
    "                }\n",
    "                \n",
    "                status = \"🚨 DRIFT\" if ks_p_value < 0.05 else \"✅ STABLE\"\n",
    "                print(f\"{feature:<20} KS-stat: {ks_stat:.4f}, p-value: {ks_p_value:.6f} {status}\")\n",
    "                \n",
    "                if ks_p_value < 0.05:\n",
    "                    significant_drifts.append(feature)\n",
    "    \n",
    "    # Summary and recommendations\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"MONITORING SUMMARY & RECOMMENDATIONS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total_weeks = len(monitoring_results)\n",
    "    weeks_with_alerts = len([r for r in monitoring_results if r['alerts']])\n",
    "    alert_rate = weeks_with_alerts / total_weeks * 100 if total_weeks > 0 else 0\n",
    "    \n",
    "    print(f\"📊 PERFORMANCE MONITORING:\")\n",
    "    print(f\"   • Weeks monitored: {total_weeks}\")\n",
    "    print(f\"   • Weeks with alerts: {weeks_with_alerts}\")\n",
    "    print(f\"   • Alert rate: {alert_rate:.1f}%\")\n",
    "    \n",
    "    if alert_rate > 30:\n",
    "        print(f\"   🚨 HIGH alert rate - Model requires attention\")\n",
    "    elif alert_rate > 10:\n",
    "        print(f\"   ⚠️  MODERATE alert rate - Monitor closely\")\n",
    "    else:\n",
    "        print(f\"   ✅ LOW alert rate - Model performing well\")\n",
    "    \n",
    "    print(f\"\\n🔄 FEATURE DRIFT:\")\n",
    "    if significant_drifts:\n",
    "        print(f\"   • Features with significant drift: {len(significant_drifts)}\")\n",
    "        for feature in significant_drifts:\n",
    "            print(f\"     - {feature.replace('_', ' ').title()}\")\n",
    "        print(f\"   🚨 RECOMMENDATION: Consider model retraining\")\n",
    "    else:\n",
    "        print(f\"   • No significant feature drift detected\")\n",
    "        print(f\"   ✅ RECOMMENDATION: Continue current monitoring\")\n",
    "    \n",
    "    print(f\"\\n📋 ACTION ITEMS:\")\n",
    "    if weeks_with_alerts > total_weeks * 0.3 or significant_drifts:\n",
    "        print(f\"   1. Schedule model retraining with recent data\")\n",
    "        print(f\"   2. Investigate root causes of performance degradation\")\n",
    "        print(f\"   3. Consider updating feature engineering pipeline\")\n",
    "        print(f\"   4. Review data quality and collection processes\")\n",
    "    else:\n",
    "        print(f\"   1. Continue weekly monitoring\")\n",
    "        print(f\"   2. Schedule quarterly model performance review\")\n",
    "        print(f\"   3. Maintain current alert thresholds\")\n",
    "    \n",
    "    # Create monitoring dashboard visualization\n",
    "    if monitoring_results:\n",
    "        monitoring_df = pd.DataFrame(monitoring_results)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot monitoring metrics over time\n",
    "        axes[0, 0].plot(monitoring_df['week'], monitoring_df['r2'], marker='o', label='Weekly R²')\n",
    "        axes[0, 0].axhline(y=monitoring_thresholds['r2_min'], color='red', linestyle='--', alpha=0.7, label='Threshold')\n",
    "        axes[0, 0].set_title('R² Over Time')\n",
    "        axes[0, 0].set_xlabel('Week')\n",
    "        axes[0, 0].set_ylabel('R²')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[0, 1].plot(monitoring_df['week'], monitoring_df['rmse'], marker='s', color='orange', label='Weekly RMSE')\n",
    "        axes[0, 1].axhline(y=monitoring_thresholds['rmse_max'], color='red', linestyle='--', alpha=0.7, label='Threshold')\n",
    "        axes[0, 1].set_title('RMSE Over Time')\n",
    "        axes[0, 1].set_xlabel('Week')\n",
    "        axes[0, 1].set_ylabel('RMSE')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1, 0].plot(monitoring_df['week'], monitoring_df['mae'], marker='^', color='green', label='Weekly MAE')\n",
    "        axes[1, 0].axhline(y=monitoring_thresholds['mae_max'], color='red', linestyle='--', alpha=0.7, label='Threshold')\n",
    "        axes[1, 0].set_title('MAE Over Time')\n",
    "        axes[1, 0].set_xlabel('Week')\n",
    "        axes[1, 0].set_ylabel('MAE')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1, 1].plot(monitoring_df['week'], monitoring_df['mape'], marker='d', color='purple', label='Weekly MAPE')\n",
    "        axes[1, 1].axhline(y=monitoring_thresholds['mape_max'], color='red', linestyle='--', alpha=0.7, label='Threshold')\n",
    "        axes[1, 1].set_title('MAPE Over Time')\n",
    "        axes[1, 1].set_xlabel('Week')\n",
    "        axes[1, 1].set_ylabel('MAPE (%)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'baseline_metrics': baseline_metrics,\n",
    "        'monitoring_thresholds': monitoring_thresholds,\n",
    "        'monitoring_results': monitoring_results,\n",
    "        'alerts': alerts,\n",
    "        'feature_drift_results': feature_drift_results,\n",
    "        'significant_drifts': significant_drifts,\n",
    "        'production_model': production_model\n",
    "    }\n",
    "\n",
    "# Create production monitoring framework\n",
    "monitoring_results = create_production_monitoring_framework(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "session_summary"
   },
   "source": [
    "# Session Summary\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "In this comprehensive model evaluation and business implementation session, we covered:\n",
    "\n",
    "### 1. Advanced Model Validation\n",
    "- **Multiple Cross-Validation Strategies**: K-fold, time series, and customer-based validation\n",
    "- **Baseline Model Comparison**: Established performance benchmarks with dummy predictors\n",
    "- **Robust Performance Assessment**: Evaluated models across different data splits and scenarios\n",
    "- **Business-Relevant Validation**: Used customer groupings and temporal splits for realistic evaluation\n",
    "\n",
    "### 2. Comprehensive Feature Importance Analysis\n",
    "- **Multiple Importance Methods**: Linear coefficients, LASSO selection, permutation importance, correlation analysis\n",
    "- **Feature Selection**: Identified consistently important predictors across methods\n",
    "- **Business Interpretation**: Translated statistical importance into actionable business insights\n",
    "- **Visual Analysis**: Created comprehensive importance visualizations\n",
    "\n",
    "### 3. A/B Testing Framework for Model Deployment\n",
    "- **Production-Ready Testing**: Simulated real-world model comparison scenarios\n",
    "- **Statistical Significance**: Implemented proper hypothesis testing for model differences\n",
    "- **Effect Size Analysis**: Measured practical significance using Cohen's d\n",
    "- **Business Impact Assessment**: Quantified monetary value of model improvements\n",
    "- **Risk Management**: Designed safe deployment strategies\n",
    "\n",
    "### 4. Production Monitoring Framework\n",
    "- **Performance Monitoring**: Established baseline metrics and alert thresholds\n",
    "- **Drift Detection**: Implemented feature drift monitoring using statistical tests\n",
    "- **Weekly Monitoring**: Simulated continuous performance tracking\n",
    "- **Alert System**: Created actionable alert mechanisms for model degradation\n",
    "- **Retraining Triggers**: Defined conditions for model updates\n",
    "\n",
    "## Key Business Skills Developed\n",
    "- **Model Deployment Strategy**: Safe, data-driven model rollout approaches\n",
    "- **Performance Monitoring**: Continuous model health assessment\n",
    "- **Business Value Quantification**: Translating model improvements to monetary impact\n",
    "- **Risk Management**: Identifying and mitigating model failure risks\n",
    "- **Stakeholder Communication**: Presenting technical results in business terms\n",
    "\n",
    "## Advanced Techniques Applied\n",
    "- **Time Series Cross-Validation**: Appropriate validation for temporal data\n",
    "- **Permutation Importance**: Model-agnostic feature importance\n",
    "- **Statistical Testing**: Hypothesis testing for model comparison\n",
    "- **Distribution Testing**: Kolmogorov-Smirnov tests for drift detection\n",
    "- **Effect Size Calculation**: Practical significance assessment\n",
    "\n",
    "## Production-Ready Frameworks\n",
    "- **Monitoring Dashboard**: Visual tracking of model performance metrics\n",
    "- **Alert System**: Automated detection of performance degradation\n",
    "- **Retraining Pipeline**: Data-driven model update decisions\n",
    "- **Documentation**: Comprehensive model evaluation reports\n",
    "\n",
    "## Business Implementation Insights\n",
    "- **Gradual Rollout**: Use A/B testing for safe model deployment\n",
    "- **Continuous Monitoring**: Implement automated performance tracking\n",
    "- **Threshold Management**: Set appropriate alert levels for business context\n",
    "- **Stakeholder Alignment**: Regular reporting of model performance and value\n",
    "\n",
    "## Week 8 Complete!\n",
    "\n",
    "You have now mastered:\n",
    "- **Statistical foundations** and hypothesis testing\n",
    "- **Comparative analysis** techniques and business applications\n",
    "- **Linear regression** fundamentals and assumptions\n",
    "- **Business modeling** applications (CLV, pricing, demand forecasting)\n",
    "- **Model evaluation** and production implementation\n",
    "\n",
    "These skills provide a complete framework for implementing statistical analysis and predictive modeling in business environments, from initial analysis through production deployment and monitoring."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}