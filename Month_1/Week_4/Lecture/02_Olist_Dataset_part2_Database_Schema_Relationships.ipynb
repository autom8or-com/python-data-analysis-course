{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to the Olist Dataset - Part 2: Database Schema and Relationships\n",
        "\n",
        "## Week 4, Day 2 (Thursday) - May 1st, 2025\n",
        "\n",
        "### Overview\n",
        "In Part 1, we explored the business context and overall structure of the Olist dataset. Now we'll dive deep into the technical details: examining each table's schema, understanding data types, and mapping the relationships that make complex analysis possible.\n",
        "\n",
        "### Learning Objectives\n",
        "By the end of this session, you will be able to:\n",
        "- Identify and explain the purpose of each field in all 9 Olist tables\n",
        "- Understand the data types and constraints for each column\n",
        "- Map primary and foreign key relationships between tables\n",
        "- Design multi-table queries using relationship understanding\n",
        "- Anticipate data quality issues based on field definitions\n",
        "- Create an entity relationship diagram for the dataset\n",
        "\n",
        "### Prerequisites\n",
        "- Part 1: Olist Dataset Overview (previous session)\n",
        "- SQL database concepts (primary keys, foreign keys, relationships)\n",
        "- Pandas DataFrame fundamentals\n",
        "- Data merging and joining concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup for the session\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_rows', 10)\n",
        "\n",
        "print(\"üóÑÔ∏è Database Schema Analysis Setup Complete\")\n",
        "print(\"Ready to examine the Olist database structure in detail!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Database Schema Overview\n",
        "\n",
        "The Olist dataset follows **relational database principles** with:\n",
        "- **Normalized structure** to minimize redundancy\n",
        "- **Primary keys** for unique record identification\n",
        "- **Foreign keys** to establish relationships\n",
        "- **Referential integrity** between related tables\n",
        "\n",
        "### Table Categories\n",
        "\n",
        "#### üèóÔ∏è **Core Transaction Tables**\n",
        "- **Orders**: Main transaction records\n",
        "- **Order Items**: Product details within orders\n",
        "- **Payments**: Payment method and installment information\n",
        "\n",
        "#### üë• **Entity Tables**\n",
        "- **Customers**: Customer information and location\n",
        "- **Sellers**: Seller information and location\n",
        "- **Products**: Product characteristics and categorization\n",
        "\n",
        "#### üìä **Supplementary Tables**\n",
        "- **Reviews**: Customer feedback and ratings\n",
        "- **Geolocation**: Coordinate and location data\n",
        "- **Category Translation**: Portuguese-English mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Core Transaction Tables\n",
        "\n",
        "### üõí Table 1: Orders (`olist_orders_dataset.csv`)\n",
        "\n",
        "**Purpose**: Central table containing all order information and status tracking.\n",
        "\n",
        "#### Schema Definition\n",
        "\n",
        "| Column Name | Data Type | Description | Constraints |\n",
        "|-------------|-----------|-------------|-------------|\n",
        "| `order_id` | STRING | Unique order identifier | PRIMARY KEY, NOT NULL |\n",
        "| `customer_id` | STRING | Customer identifier | FOREIGN KEY ‚Üí customers.customer_id |\n",
        "| `order_status` | STRING | Current order status | NOT NULL, Limited values |\n",
        "| `order_purchase_timestamp` | DATETIME | When order was placed | NOT NULL |\n",
        "| `order_approved_at` | DATETIME | When payment was approved | CAN BE NULL |\n",
        "| `order_delivered_carrier_date` | DATETIME | When shipped to carrier | CAN BE NULL |\n",
        "| `order_delivered_customer_date` | DATETIME | When delivered to customer | CAN BE NULL |\n",
        "| `order_estimated_delivery_date` | DATETIME | Estimated delivery date | CAN BE NULL |\n",
        "\n",
        "#### Order Status Values\n",
        "- `'delivered'` - Successfully completed orders\n",
        "- `'shipped'` - In transit to customer\n",
        "- `'processing'` - Being prepared for shipment\n",
        "- `'invoiced'` - Payment confirmed, preparing order\n",
        "- `'canceled'` - Order canceled\n",
        "- `'unavailable'` - Product unavailable\n",
        "- `'approved'` - Payment approved, starting fulfillment\n",
        "- `'created'` - Order created but not yet processed\n",
        "\n",
        "#### Business Logic\n",
        "- **Order Lifecycle**: created ‚Üí approved ‚Üí invoiced ‚Üí processing ‚Üí shipped ‚Üí delivered\n",
        "- **Timestamps**: Enable calculation of processing times and delivery performance\n",
        "- **Status Tracking**: Allows analysis of order fulfillment efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample orders table structure for demonstration\n",
        "sample_orders = pd.DataFrame({\n",
        "    'order_id': ['e481f51cbdc54678b7cc49136f2d6af7', '53cdb2fc8bc7dce0b6741e2150273451', \n",
        "                 '47770eb9100c2d0c44946d9cf07ec65d'],\n",
        "    'customer_id': ['9ef432eb6251297304e76186b10a928d', 'b0830fb4747a6c6d20dea0b8c802d7ef',\n",
        "                    '41ce2a54c0b03bf3443c3d931a367089'],\n",
        "    'order_status': ['delivered', 'delivered', 'delivered'],\n",
        "    'order_purchase_timestamp': ['2017-10-02 10:56:33', '2018-07-24 20:41:37', '2018-08-08 08:38:49'],\n",
        "    'order_approved_at': ['2017-10-02 11:07:15', '2018-07-26 03:24:27', '2018-08-08 08:55:23'],\n",
        "    'order_delivered_carrier_date': ['2017-10-04 19:55:00', '2018-07-26 14:31:00', '2018-08-08 13:50:00'],\n",
        "    'order_delivered_customer_date': ['2017-10-10 21:25:13', '2018-08-07 15:27:45', '2018-08-17 18:03:12'],\n",
        "    'order_estimated_delivery_date': ['2017-10-18 00:00:00', '2018-08-13 00:00:00', '2018-09-04 00:00:00']\n",
        "})\n",
        "\n",
        "# Convert to datetime\n",
        "datetime_cols = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date',\n",
        "                'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
        "\n",
        "for col in datetime_cols:\n",
        "    sample_orders[col] = pd.to_datetime(sample_orders[col])\n",
        "\n",
        "print(\"üìã Sample Orders Table Structure:\")\n",
        "print(sample_orders.info())\n",
        "print(\"\\nüìä Sample Data:\")\n",
        "print(sample_orders.head())\n",
        "\n",
        "# Calculate some derived metrics\n",
        "sample_orders['approval_time_hours'] = (\n",
        "    sample_orders['order_approved_at'] - sample_orders['order_purchase_timestamp']\n",
        ").dt.total_seconds() / 3600\n",
        "\n",
        "sample_orders['delivery_time_days'] = (\n",
        "    sample_orders['order_delivered_customer_date'] - sample_orders['order_purchase_timestamp']\n",
        ").dt.days\n",
        "\n",
        "print(\"\\n‚è±Ô∏è Derived Metrics:\")\n",
        "print(f\"Average approval time: {sample_orders['approval_time_hours'].mean():.1f} hours\")\n",
        "print(f\"Average delivery time: {sample_orders['delivery_time_days'].mean():.1f} days\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üì¶ Table 2: Order Items (`olist_order_items_dataset.csv`)\n",
        "\n",
        "**Purpose**: Details of individual products within each order, including pricing and seller information.\n",
        "\n",
        "#### Schema Definition\n",
        "\n",
        "| Column Name | Data Type | Description | Constraints |\n",
        "|-------------|-----------|-------------|-------------|\n",
        "| `order_id` | STRING | Order identifier | FOREIGN KEY ‚Üí orders.order_id |\n",
        "| `order_item_id` | INTEGER | Item sequence within order | NOT NULL, starts at 1 |\n",
        "| `product_id` | STRING | Product identifier | FOREIGN KEY ‚Üí products.product_id |\n",
        "| `seller_id` | STRING | Seller identifier | FOREIGN KEY ‚Üí sellers.seller_id |\n",
        "| `shipping_limit_date` | DATETIME | Latest shipping date promised | NOT NULL |\n",
        "| `price` | DECIMAL | Item price (before shipping) | NOT NULL, > 0 |\n",
        "| `freight_value` | DECIMAL | Shipping cost for this item | NOT NULL, >= 0 |\n",
        "\n",
        "#### Key Relationships\n",
        "- **Composite Primary Key**: `order_id` + `order_item_id`\n",
        "- **One-to-Many**: One order can have multiple items\n",
        "- **Many-to-One**: Multiple items can reference same product/seller\n",
        "\n",
        "#### Business Logic\n",
        "- **Item Sequencing**: `order_item_id` shows order of items in cart\n",
        "- **Pricing**: Separate tracking of product price vs. shipping costs\n",
        "- **Seller Attribution**: Each item tracks which seller fulfilled it\n",
        "- **Shipping Promises**: `shipping_limit_date` for customer expectations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample order items table\n",
        "sample_order_items = pd.DataFrame({\n",
        "    'order_id': ['e481f51cbdc54678b7cc49136f2d6af7', 'e481f51cbdc54678b7cc49136f2d6af7',\n",
        "                 '53cdb2fc8bc7dce0b6741e2150273451', '47770eb9100c2d0c44946d9cf07ec65d'],\n",
        "    'order_item_id': [1, 2, 1, 1],\n",
        "    'product_id': ['4244733e06e7ecb4970a6e2683c13e61', 'e5f2d52b802189ee658865ca93d83a8f',\n",
        "                   'c777355d18b72b67abbeef9df44fd0fd', '7634da152a4610f1595efa32f14722fc'],\n",
        "    'seller_id': ['48436dade18ac8b2bce089ec2a041202', '48436dade18ac8b2bce089ec2a041202',\n",
        "                  'dd7ddc04e1b6c2c614352b383efe2d36', '1f50f920176fa81dab994f9023523100'],\n",
        "    'shipping_limit_date': ['2017-10-09 10:56:33', '2017-10-09 10:56:33',\n",
        "                           '2018-07-31 20:41:37', '2018-08-15 08:38:49'],\n",
        "    'price': [58.90, 239.90, 199.00, 129.90],\n",
        "    'freight_value': [13.29, 19.93, 17.87, 14.78]\n",
        "})\n",
        "\n",
        "sample_order_items['shipping_limit_date'] = pd.to_datetime(sample_order_items['shipping_limit_date'])\n",
        "\n",
        "print(\"üì¶ Sample Order Items Table:\")\n",
        "print(sample_order_items)\n",
        "\n",
        "# Demonstrate relationship analysis\n",
        "print(\"\\nüîç Relationship Analysis:\")\n",
        "print(f\"Total items: {len(sample_order_items)}\")\n",
        "print(f\"Unique orders: {sample_order_items['order_id'].nunique()}\")\n",
        "print(f\"Items per order: {len(sample_order_items) / sample_order_items['order_id'].nunique():.1f} average\")\n",
        "\n",
        "# Show multi-item order\n",
        "multi_item_order = sample_order_items[sample_order_items['order_id'] == 'e481f51cbdc54678b7cc49136f2d6af7']\n",
        "print(\"\\nüõí Multi-item order example:\")\n",
        "print(multi_item_order[['order_item_id', 'price', 'freight_value']])\n",
        "print(f\"Total order value: ${multi_item_order['price'].sum():.2f} + ${multi_item_order['freight_value'].sum():.2f} shipping\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí≥ Table 3: Payments (`olist_order_payments_dataset.csv`)\n",
        "\n",
        "**Purpose**: Payment method details and installment information for orders.\n",
        "\n",
        "#### Schema Definition\n",
        "\n",
        "| Column Name | Data Type | Description | Constraints |\n",
        "|-------------|-----------|-------------|-------------|\n",
        "| `order_id` | STRING | Order identifier | FOREIGN KEY ‚Üí orders.order_id |\n",
        "| `payment_sequential` | INTEGER | Payment sequence number | NOT NULL, starts at 1 |\n",
        "| `payment_type` | STRING | Payment method used | NOT NULL, Limited values |\n",
        "| `payment_installments` | INTEGER | Number of installments | NOT NULL, >= 1 |\n",
        "| `payment_value` | DECIMAL | Payment amount | NOT NULL, > 0 |\n",
        "\n",
        "#### Payment Types\n",
        "- `'credit_card'` - Credit card payments (most common)\n",
        "- `'boleto'` - Brazilian bank slip payment\n",
        "- `'voucher'` - Gift cards or vouchers\n",
        "- `'debit_card'` - Debit card payments\n",
        "- `'not_defined'` - Unknown payment method\n",
        "\n",
        "#### Business Logic\n",
        "- **Multiple Payments**: One order can have multiple payment methods\n",
        "- **Installments**: Very common in Brazilian e-commerce\n",
        "- **Sequential Tracking**: `payment_sequential` for multiple payments\n",
        "- **Cultural Context**: Boleto is uniquely Brazilian payment method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample payments table\n",
        "sample_payments = pd.DataFrame({\n",
        "    'order_id': ['e481f51cbdc54678b7cc49136f2d6af7', '53cdb2fc8bc7dce0b6741e2150273451',\n",
        "                 '47770eb9100c2d0c44946d9cf07ec65d', '47770eb9100c2d0c44946d9cf07ec65d'],\n",
        "    'payment_sequential': [1, 1, 1, 2],\n",
        "    'payment_type': ['credit_card', 'credit_card', 'credit_card', 'voucher'],\n",
        "    'payment_installments': [8, 1, 4, 1],\n",
        "    'payment_value': [298.83, 216.87, 119.90, 24.78]\n",
        "})\n",
        "\n",
        "print(\"üí≥ Sample Payments Table:\")\n",
        "print(sample_payments)\n",
        "\n",
        "# Analyze payment patterns\n",
        "print(\"\\nüìä Payment Analysis:\")\n",
        "print(\"Payment type distribution:\")\n",
        "print(sample_payments['payment_type'].value_counts())\n",
        "\n",
        "print(\"\\nInstallment patterns:\")\n",
        "installment_analysis = sample_payments.groupby('payment_installments').agg({\n",
        "    'payment_value': ['count', 'mean'],\n",
        "    'order_id': 'nunique'\n",
        "})\n",
        "print(installment_analysis)\n",
        "\n",
        "# Show mixed payment order\n",
        "mixed_payment_order = sample_payments[sample_payments['order_id'] == '47770eb9100c2d0c44946d9cf07ec65d']\n",
        "print(\"\\nüîÑ Mixed payment method example:\")\n",
        "print(mixed_payment_order)\n",
        "print(f\"Total payment: ${mixed_payment_order['payment_value'].sum():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Entity Tables\n",
        "\n",
        "### üë• Table 4: Customers (`olist_customers_dataset.csv`)\n",
        "\n",
        "**Purpose**: Customer information and geographic location data.\n",
        "\n",
        "#### Schema Definition\n",
        "\n",
        "| Column Name | Data Type | Description | Constraints |\n",
        "|-------------|-----------|-------------|-------------|\n",
        "| `customer_id` | STRING | Unique customer identifier | PRIMARY KEY, NOT NULL |\n",
        "| `customer_unique_id` | STRING | Real customer identifier (anonymized) | NOT NULL |\n",
        "| `customer_zip_code_prefix` | INTEGER | First 5 digits of ZIP code | NOT NULL |\n",
        "| `customer_city` | STRING | Customer city name | NOT NULL |\n",
        "| `customer_state` | STRING | Brazilian state abbreviation | NOT NULL, 2 characters |\n",
        "\n",
        "#### Geographic Context\n",
        "- **Brazilian States**: 27 total states (26 states + 1 federal district)\n",
        "- **ZIP Code System**: 8-digit system, first 5 digits indicate region\n",
        "- **Privacy Protection**: Real customer identity anonymized\n",
        "\n",
        "#### Key Insights\n",
        "- **Customer Uniqueness**: `customer_id` vs `customer_unique_id` distinction\n",
        "- **Geographic Analysis**: Enables location-based customer segmentation\n",
        "- **Market Reach**: Shows Olist's geographic penetration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample customers table\n",
        "sample_customers = pd.DataFrame({\n",
        "    'customer_id': ['9ef432eb6251297304e76186b10a928d', 'b0830fb4747a6c6d20dea0b8c802d7ef',\n",
        "                    '41ce2a54c0b03bf3443c3d931a367089', '8d50f5eadf9050cf04b64645b8f2b075'],\n",
        "    'customer_unique_id': ['861eff4711a542e4b93843c6dd7febb0', '290c77bc529b7ac935b93aa66c333dc3',\n",
        "                          '060e732b5b29e8181a18229c7b0b2b5e', '854daefd58154449b40e90b8faf8b916'],\n",
        "    'customer_zip_code_prefix': [14409, 9790, 1151, 8775],\n",
        "    'customer_city': ['franca', 'sao bernardo do campo', 'sao paulo', 'mogi das cruzes'],\n",
        "    'customer_state': ['SP', 'SP', 'SP', 'SP']\n",
        "})\n",
        "\n",
        "print(\"üë• Sample Customers Table:\")\n",
        "print(sample_customers)\n",
        "\n",
        "# Geographic analysis\n",
        "print(\"\\nüó∫Ô∏è Geographic Distribution:\")\n",
        "state_dist = sample_customers['customer_state'].value_counts()\n",
        "print(f\"States represented: {state_dist}\")\n",
        "\n",
        "print(\"\\nüìç ZIP Code Analysis:\")\n",
        "print(f\"ZIP code range: {sample_customers['customer_zip_code_prefix'].min()} - {sample_customers['customer_zip_code_prefix'].max()}\")\n",
        "\n",
        "# Brazilian state context\n",
        "print(\"\\nüáßüá∑ Brazilian State Context:\")\n",
        "print(\"SP = S√£o Paulo (Brazil's most populous state)\")\n",
        "print(\"Major economic center with highest e-commerce activity\")\n",
        "\n",
        "# Show privacy protection\n",
        "print(\"\\nüîí Privacy Protection:\")\n",
        "print(\"customer_id ‚â† customer_unique_id (anonymization layer)\")\n",
        "print(\"Real identity mapped through customer_unique_id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üè™ Table 5: Sellers (`olist_sellers_dataset.csv`)\n",
        "\n",
        "**Purpose**: Seller information and geographic distribution.\n",
        "\n",
        "#### Schema Definition\n",
        "\n",
        "| Column Name | Data Type | Description | Constraints |\n",
        "|-------------|-----------|-------------|-------------|\n",
        "| `seller_id` | STRING | Unique seller identifier | PRIMARY KEY, NOT NULL |\n",
        "| `seller_zip_code_prefix` | INTEGER | First 5 digits of ZIP code | NOT NULL |\n",
        "| `seller_city` | STRING | Seller city name | NOT NULL |\n",
        "| `seller_state` | STRING | Brazilian state abbreviation | NOT NULL, 2 characters |\n",
        "\n",
        "#### Business Context\n",
        "- **Seller Network**: Distributed across Brazil\n",
        "- **Geographic Strategy**: Logistics and shipping optimization\n",
        "- **Market Reach**: Local sellers serving broader markets\n",
        "\n",
        "#### Analysis Opportunities\n",
        "- **Seller Performance by Location**: Regional success patterns\n",
        "- **Logistics Analysis**: Distance between sellers and customers\n",
        "- **Market Penetration**: Coverage across Brazilian regions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìã Table 6: Products (`olist_products_dataset.csv`)\n",
        "\n",
        "**Purpose**: Product characteristics and categorization information.\n",
        "\n",
        "#### Schema Definition\n",
        "\n",
        "| Column Name | Data Type | Description | Constraints |\n",
        "|-------------|-----------|-------------|-------------|\n",
        "| `product_id` | STRING | Unique product identifier | PRIMARY KEY, NOT NULL |\n",
        "| `product_category_name` | STRING | Category name (Portuguese) | CAN BE NULL |\n",
        "| `product_name_lenght` | INTEGER | Length of product name | CAN BE NULL, >= 0 |\n",
        "| `product_description_lenght` | INTEGER | Length of description | CAN BE NULL, >= 0 |\n",
        "| `product_photos_qty` | INTEGER | Number of product photos | CAN BE NULL, >= 0 |\n",
        "| `product_weight_g` | INTEGER | Product weight in grams | CAN BE NULL, > 0 |\n",
        "| `product_length_cm` | INTEGER | Product length in cm | CAN BE NULL, > 0 |\n",
        "| `product_height_cm` | INTEGER | Product height in cm | CAN BE NULL, > 0 |\n",
        "| `product_width_cm` | INTEGER | Product width in cm | CAN BE NULL, > 0 |\n",
        "\n",
        "#### Key Insights\n",
        "- **Physical Characteristics**: Enable shipping cost calculations\n",
        "- **Content Quality**: Name/description length as quality indicators\n",
        "- **Visual Appeal**: Photo quantity impacts conversion\n",
        "- **Portuguese Categories**: Require translation for international analysis\n",
        "\n",
        "#### Data Quality Notes\n",
        "- **Missing Values**: Common in physical dimensions\n",
        "- **Spelling Error**: \"lenght\" instead of \"length\" in column names\n",
        "- **Measurement Units**: Consistent metric system usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample products table\n",
        "sample_products = pd.DataFrame({\n",
        "    'product_id': ['4244733e06e7ecb4970a6e2683c13e61', 'e5f2d52b802189ee658865ca93d83a8f',\n",
        "                   'c777355d18b72b67abbeef9df44fd0fd', '7634da152a4610f1595efa32f14722fc'],\n",
        "    'product_category_name': ['beleza_saude', 'perfumaria', 'esporte_lazer', 'informatica_acessorios'],\n",
        "    'product_name_lenght': [58, 42, 35, 67],\n",
        "    'product_description_lenght': [315, 287, 542, 128],\n",
        "    'product_photos_qty': [4, 3, 7, 2],\n",
        "    'product_weight_g': [650, 350, 1200, 89],\n",
        "    'product_length_cm': [18, 12, 35, 15],\n",
        "    'product_height_cm': [6, 8, 15, 2],\n",
        "    'product_width_cm': [12, 8, 25, 10]\n",
        "})\n",
        "\n",
        "print(\"üìã Sample Products Table:\")\n",
        "print(sample_products)\n",
        "\n",
        "# Analyze product characteristics\n",
        "print(\"\\nüìä Product Analysis:\")\n",
        "print(\"Category distribution:\")\n",
        "print(sample_products['product_category_name'].value_counts())\n",
        "\n",
        "print(\"\\nüì∏ Content Quality Metrics:\")\n",
        "content_metrics = sample_products[['product_name_lenght', 'product_description_lenght', 'product_photos_qty']].describe()\n",
        "print(content_metrics)\n",
        "\n",
        "# Calculate shipping volume (for logistics)\n",
        "sample_products['volume_cm3'] = (sample_products['product_length_cm'] * \n",
        "                                sample_products['product_height_cm'] * \n",
        "                                sample_products['product_width_cm'])\n",
        "\n",
        "print(\"\\nüì¶ Shipping Characteristics:\")\n",
        "shipping_analysis = sample_products[['product_weight_g', 'volume_cm3']].describe()\n",
        "print(shipping_analysis)\n",
        "\n",
        "# Category translation examples\n",
        "print(\"\\nüî§ Category Translation Examples:\")\n",
        "category_translations = {\n",
        "    'beleza_saude': 'health_beauty',\n",
        "    'perfumaria': 'perfumery',\n",
        "    'esporte_lazer': 'sports_leisure',\n",
        "    'informatica_acessorios': 'computers_accessories'\n",
        "}\n",
        "\n",
        "for pt, en in category_translations.items():\n",
        "    print(f\"{pt} ‚Üí {en}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Supplementary Tables\n",
        "\n",
        "### ‚≠ê Table 7: Reviews (`olist_order_reviews_dataset.csv`)\n",
        "\n",
        "**Purpose**: Customer feedback, ratings, and review text for orders.\n",
        "\n",
        "#### Schema Definition\n",
        "\n",
        "| Column Name | Data Type | Description | Constraints |\n",
        "|-------------|-----------|-------------|-------------|\n",
        "| `review_id` | STRING | Unique review identifier | PRIMARY KEY, NOT NULL |\n",
        "| `order_id` | STRING | Order being reviewed | FOREIGN KEY ‚Üí orders.order_id |\n",
        "| `review_score` | INTEGER | Rating from 1-5 stars | NOT NULL, 1 ‚â§ value ‚â§ 5 |\n",
        "| `review_comment_title` | STRING | Review title/summary | CAN BE NULL |\n",
        "| `review_comment_message` | STRING | Full review text | CAN BE NULL |\n",
        "| `review_creation_date` | DATETIME | When review was created | NOT NULL |\n",
        "| `review_answer_timestamp` | DATETIME | When seller/platform responded | CAN BE NULL |\n",
        "\n",
        "#### Rating Scale\n",
        "- **5 stars**: Excellent experience\n",
        "- **4 stars**: Good experience\n",
        "- **3 stars**: Average experience\n",
        "- **2 stars**: Poor experience\n",
        "- **1 star**: Very poor experience\n",
        "\n",
        "#### Analysis Applications\n",
        "- **Sentiment Analysis**: Text mining of review content\n",
        "- **Quality Metrics**: Correlation with delivery performance\n",
        "- **Product Success**: Rating impact on sales\n",
        "- **Response Analysis**: Seller engagement with feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üåç Table 8: Geolocation (`olist_geolocation_dataset.csv`)\n",
        "\n",
        "**Purpose**: Geographic coordinates and location data for Brazilian ZIP codes.\n",
        "\n",
        "#### Schema Definition\n",
        "\n",
        "| Column Name | Data Type | Description | Constraints |\n",
        "|-------------|-----------|-------------|-------------|\n",
        "| `geolocation_zip_code_prefix` | INTEGER | ZIP code prefix | NOT NULL |\n",
        "| `geolocation_lat` | DECIMAL | Latitude coordinate | NOT NULL |\n",
        "| `geolocation_lng` | DECIMAL | Longitude coordinate | NOT NULL |\n",
        "| `geolocation_city` | STRING | City name | NOT NULL |\n",
        "| `geolocation_state` | STRING | State abbreviation | NOT NULL |\n",
        "\n",
        "#### Geographic Analysis Applications\n",
        "- **Distance Calculations**: Between customers and sellers\n",
        "- **Logistics Optimization**: Delivery route planning\n",
        "- **Market Analysis**: Regional performance mapping\n",
        "- **Heat Maps**: Geographic visualization of business metrics\n",
        "\n",
        "#### Data Characteristics\n",
        "- **High Volume**: ~1 million records for detailed coverage\n",
        "- **Coordinate Precision**: Decimal degrees for accuracy\n",
        "- **Duplicate Handling**: Multiple entries per ZIP code possible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üè∑Ô∏è Table 9: Category Translation (`product_category_name_translation.csv`)\n",
        "\n",
        "**Purpose**: Mapping between Portuguese and English product category names.\n",
        "\n",
        "#### Schema Definition\n",
        "\n",
        "| Column Name | Data Type | Description | Constraints |\n",
        "|-------------|-----------|-------------|-------------|\n",
        "| `product_category_name` | STRING | Portuguese category name | PRIMARY KEY, NOT NULL |\n",
        "| `product_category_name_english` | STRING | English translation | NOT NULL |\n",
        "\n",
        "#### Key Categories (Examples)\n",
        "\n",
        "| Portuguese | English | Business Context |\n",
        "|------------|---------|-------------------|\n",
        "| `beleza_saude` | `health_beauty` | Personal care products |\n",
        "| `informatica_acessorios` | `computers_accessories` | Technology products |\n",
        "| `moveis_decoracao` | `furniture_decor` | Home improvement |\n",
        "| `esporte_lazer` | `sports_leisure` | Recreation products |\n",
        "| `casa_construcao` | `home_construction` | Building materials |\n",
        "\n",
        "#### Usage Importance\n",
        "- **International Analysis**: Makes data accessible globally\n",
        "- **Standardization**: Consistent category naming\n",
        "- **Business Intelligence**: Category performance comparison\n",
        "- **Cultural Bridge**: Understanding Brazilian market categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample category translation table\n",
        "sample_categories = pd.DataFrame({\n",
        "    'product_category_name': [\n",
        "        'beleza_saude', 'informatica_acessorios', 'moveis_decoracao',\n",
        "        'esporte_lazer', 'casa_construcao', 'eletronicos', 'telefonia',\n",
        "        'automotivo', 'livros_tecnicos', 'fashion_bolsas_e_acessorios'\n",
        "    ],\n",
        "    'product_category_name_english': [\n",
        "        'health_beauty', 'computers_accessories', 'furniture_decor',\n",
        "        'sports_leisure', 'home_construction', 'electronics', 'telephony',\n",
        "        'automotive', 'books_technical', 'fashion_bags_accessories'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"üè∑Ô∏è Sample Category Translation Table:\")\n",
        "print(sample_categories)\n",
        "\n",
        "# Show translation usage\n",
        "print(\"\\nüîÑ Translation Usage Example:\")\n",
        "print(\"Original products with Portuguese categories:\")\n",
        "products_with_translation = sample_products.merge(\n",
        "    sample_categories, \n",
        "    on='product_category_name', \n",
        "    how='left'\n",
        ")\n",
        "print(products_with_translation[['product_id', 'product_category_name', 'product_category_name_english']].head())\n",
        "\n",
        "print(\"\\nüìä Category Analysis Benefits:\")\n",
        "print(\"‚úÖ Enables international business analysis\")\n",
        "print(\"‚úÖ Facilitates category comparison studies\")\n",
        "print(\"‚úÖ Supports multi-language reporting\")\n",
        "print(\"‚úÖ Bridges cultural understanding gaps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Entity Relationship Diagram (ERD)\n",
        "\n",
        "Understanding the relationships between tables is crucial for effective analysis. Let's visualize the complete schema structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive Entity Relationship Diagram\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import FancyBboxPatch, ConnectionPatch\n",
        "import numpy as np\n",
        "\n",
        "# Create large figure for detailed ERD\n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
        "ax.set_xlim(0, 16)\n",
        "ax.set_ylim(0, 12)\n",
        "ax.axis('off')\n",
        "\n",
        "# Define table positions with detailed schema information\n",
        "tables_erd = {\n",
        "    'Orders': {\n",
        "        'pos': (8, 9),\n",
        "        'color': '#FF6B6B',\n",
        "        'pk': 'order_id',\n",
        "        'fk': ['customer_id'],\n",
        "        'size': (2.5, 1.5),\n",
        "        'fields': ['order_id (PK)', 'customer_id (FK)', 'order_status', 'purchase_timestamp']\n",
        "    },\n",
        "    'Order Items': {\n",
        "        'pos': (8, 6.5),\n",
        "        'color': '#4ECDC4',\n",
        "        'pk': 'order_id + order_item_id',\n",
        "        'fk': ['order_id', 'product_id', 'seller_id'],\n",
        "        'size': (2.5, 1.5),\n",
        "        'fields': ['order_id (FK)', 'order_item_id', 'product_id (FK)', 'seller_id (FK)', 'price']\n",
        "    },\n",
        "    'Customers': {\n",
        "        'pos': (4, 9),\n",
        "        'color': '#45B7D1',\n",
        "        'pk': 'customer_id',\n",
        "        'fk': [],\n",
        "        'size': (2.5, 1.5),\n",
        "        'fields': ['customer_id (PK)', 'customer_unique_id', 'zip_code_prefix', 'city', 'state']\n",
        "    },\n",
        "    'Products': {\n",
        "        'pos': (12, 6.5),\n",
        "        'color': '#FFEAA7',\n",
        "        'pk': 'product_id',\n",
        "        'fk': [],\n",
        "        'size': (2.5, 1.5),\n",
        "        'fields': ['product_id (PK)', 'category_name', 'weight_g', 'dimensions']\n",
        "    },\n",
        "    'Sellers': {\n",
        "        'pos': (4, 6.5),\n",
        "        'color': '#96CEB4',\n",
        "        'pk': 'seller_id',\n",
        "        'fk': [],\n",
        "        'size': (2.5, 1.5),\n",
        "        'fields': ['seller_id (PK)', 'zip_code_prefix', 'city', 'state']\n",
        "    },\n",
        "    'Payments': {\n",
        "        'pos': (12, 9),\n",
        "        'color': '#F7DC6F',\n",
        "        'pk': 'order_id + payment_sequential',\n",
        "        'fk': ['order_id'],\n",
        "        'size': (2.5, 1.5),\n",
        "        'fields': ['order_id (FK)', 'payment_sequential', 'payment_type', 'installments']\n",
        "    },\n",
        "    'Reviews': {\n",
        "        'pos': (8, 4),\n",
        "        'color': '#DDA0DD',\n",
        "        'pk': 'review_id',\n",
        "        'fk': ['order_id'],\n",
        "        'size': (2.5, 1.5),\n",
        "        'fields': ['review_id (PK)', 'order_id (FK)', 'review_score', 'comment_title']\n",
        "    },\n",
        "    'Geolocation': {\n",
        "        'pos': (2, 3),\n",
        "        'color': '#AED6F1',\n",
        "        'pk': 'zip_code_prefix',\n",
        "        'fk': [],\n",
        "        'size': (2.5, 1.5),\n",
        "        'fields': ['zip_code_prefix', 'latitude', 'longitude', 'city', 'state']\n",
        "    },\n",
        "    'Categories': {\n",
        "        'pos': (14, 3),\n",
        "        'color': '#F8C471',\n",
        "        'pk': 'product_category_name',\n",
        "        'fk': [],\n",
        "        'size': (2.5, 1.5),\n",
        "        'fields': ['category_name (PK)', 'category_name_english']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Draw tables with detailed information\n",
        "for table_name, info in tables_erd.items():\n",
        "    x, y = info['pos']\n",
        "    width, height = info['size']\n",
        "    \n",
        "    # Create table box\n",
        "    box = FancyBboxPatch(\n",
        "        (x - width/2, y - height/2),\n",
        "        width, height,\n",
        "        boxstyle=\"round,pad=0.05\",\n",
        "        facecolor=info['color'],\n",
        "        edgecolor='black',\n",
        "        linewidth=2,\n",
        "        alpha=0.8\n",
        "    )\n",
        "    ax.add_patch(box)\n",
        "    \n",
        "    # Add table name (header)\n",
        "    ax.text(x, y + height/2 - 0.2, table_name, \n",
        "            ha='center', va='center', fontsize=11, fontweight='bold')\n",
        "    \n",
        "    # Add fields\n",
        "    field_y_start = y + 0.1\n",
        "    for i, field in enumerate(info['fields'][:4]):  # Show first 4 fields\n",
        "        field_y = field_y_start - (i * 0.2)\n",
        "        ax.text(x, field_y, field, ha='center', va='center', \n",
        "                fontsize=8, family='monospace')\n",
        "\n",
        "# Define relationships with cardinality\n",
        "relationships = [\n",
        "    ('Orders', 'Order Items', '1:M', 'order_id'),\n",
        "    ('Orders', 'Customers', 'M:1', 'customer_id'),\n",
        "    ('Orders', 'Payments', '1:M', 'order_id'),\n",
        "    ('Orders', 'Reviews', '1:1', 'order_id'),\n",
        "    ('Order Items', 'Products', 'M:1', 'product_id'),\n",
        "    ('Order Items', 'Sellers', 'M:1', 'seller_id'),\n",
        "    ('Products', 'Categories', 'M:1', 'category_name'),\n",
        "    ('Customers', 'Geolocation', 'M:1', 'zip_code_prefix'),\n",
        "    ('Sellers', 'Geolocation', 'M:1', 'zip_code_prefix')\n",
        "]\n",
        "\n",
        "# Draw relationships\n",
        "for start_table, end_table, cardinality, join_field in relationships:\n",
        "    start_pos = tables_erd[start_table]['pos']\n",
        "    end_pos = tables_erd[end_table]['pos']\n",
        "    \n",
        "    # Calculate arrow positions\n",
        "    dx = end_pos[0] - start_pos[0]\n",
        "    dy = end_pos[1] - start_pos[1]\n",
        "    \n",
        "    # Adjust start and end points to table edges\n",
        "    start_size = tables_erd[start_table]['size']\n",
        "    end_size = tables_erd[end_table]['size']\n",
        "    \n",
        "    # Calculate edge points\n",
        "    if abs(dx) > abs(dy):  # Horizontal connection\n",
        "        if dx > 0:  # Right connection\n",
        "            start_point = (start_pos[0] + start_size[0]/2, start_pos[1])\n",
        "            end_point = (end_pos[0] - end_size[0]/2, end_pos[1])\n",
        "        else:  # Left connection\n",
        "            start_point = (start_pos[0] - start_size[0]/2, start_pos[1])\n",
        "            end_point = (end_pos[0] + end_size[0]/2, end_pos[1])\n",
        "    else:  # Vertical connection\n",
        "        if dy > 0:  # Up connection\n",
        "            start_point = (start_pos[0], start_pos[1] + start_size[1]/2)\n",
        "            end_point = (end_pos[0], end_pos[1] - end_size[1]/2)\n",
        "        else:  # Down connection\n",
        "            start_point = (start_pos[0], start_pos[1] - start_size[1]/2)\n",
        "            end_point = (end_pos[0], end_pos[1] + end_size[1]/2)\n",
        "    \n",
        "    # Draw arrow\n",
        "    ax.annotate('', xy=end_point, xytext=start_point,\n",
        "                arrowprops=dict(arrowstyle='->', lw=1.5, color='gray', alpha=0.8))\n",
        "    \n",
        "    # Add cardinality label\n",
        "    mid_x = (start_point[0] + end_point[0]) / 2\n",
        "    mid_y = (start_point[1] + end_point[1]) / 2\n",
        "    ax.text(mid_x, mid_y, cardinality, ha='center', va='center',\n",
        "            fontsize=8, bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.8))\n",
        "\n",
        "# Add title and legend\n",
        "ax.text(8, 11.5, 'Olist Dataset - Entity Relationship Diagram', \n",
        "        ha='center', va='center', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Add legend\n",
        "legend_x = 1\n",
        "legend_y = 10.5\n",
        "ax.text(legend_x, legend_y, 'Legend:', fontsize=12, fontweight='bold')\n",
        "ax.text(legend_x, legend_y - 0.3, 'PK = Primary Key', fontsize=9)\n",
        "ax.text(legend_x, legend_y - 0.6, 'FK = Foreign Key', fontsize=9)\n",
        "ax.text(legend_x, legend_y - 0.9, '1:M = One to Many', fontsize=9)\n",
        "ax.text(legend_x, legend_y - 1.2, 'M:1 = Many to One', fontsize=9)\n",
        "ax.text(legend_x, legend_y - 1.5, '1:1 = One to One', fontsize=9)\n",
        "\n",
        "# Add business context notes\n",
        "notes_x = 1\n",
        "notes_y = 7.5\n",
        "ax.text(notes_x, notes_y, 'Business Context:', fontsize=10, fontweight='bold')\n",
        "ax.text(notes_x, notes_y - 0.3, '‚Ä¢ Orders are the central entity', fontsize=8)\n",
        "ax.text(notes_x, notes_y - 0.5, '‚Ä¢ Each order can have multiple items', fontsize=8)\n",
        "ax.text(notes_x, notes_y - 0.7, '‚Ä¢ Items connect to products & sellers', fontsize=8)\n",
        "ax.text(notes_x, notes_y - 0.9, '‚Ä¢ Geographic data supports logistics', fontsize=8)\n",
        "ax.text(notes_x, notes_y - 1.1, '‚Ä¢ Reviews provide quality feedback', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üóÇÔ∏è Complete Entity Relationship Diagram\")\n",
        "print(\"\\nKey Relationship Patterns:\")\n",
        "print(\"1. Orders ‚Üí Order Items (1:Many) - Each order has multiple line items\")\n",
        "print(\"2. Order Items ‚Üí Products (Many:1) - Many items reference same product\")\n",
        "print(\"3. Order Items ‚Üí Sellers (Many:1) - Multiple items from same seller\")\n",
        "print(\"4. Orders ‚Üí Customers (Many:1) - Customer can have multiple orders\")\n",
        "print(\"5. Geographic tables support location-based analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Primary and Foreign Key Relationships\n",
        "\n",
        "### üîë Key Relationship Summary\n",
        "\n",
        "Understanding the key relationships is crucial for joining tables correctly:\n",
        "\n",
        "#### Primary Keys (Unique Identifiers)\n",
        "| Table | Primary Key | Description |\n",
        "|-------|-------------|-------------|\n",
        "| Orders | `order_id` | Unique order identifier |\n",
        "| Order Items | `order_id` + `order_item_id` | Composite key for line items |\n",
        "| Customers | `customer_id` | Unique customer identifier |\n",
        "| Sellers | `seller_id` | Unique seller identifier |\n",
        "| Products | `product_id` | Unique product identifier |\n",
        "| Reviews | `review_id` | Unique review identifier |\n",
        "| Payments | `order_id` + `payment_sequential` | Composite key for payments |\n",
        "| Geolocation | `geolocation_zip_code_prefix` | ZIP code (with duplicates) |\n",
        "| Categories | `product_category_name` | Category name in Portuguese |\n",
        "\n",
        "#### Foreign Key Relationships\n",
        "| Child Table | Foreign Key | Parent Table | Parent Key | Relationship Type |\n",
        "|-------------|-------------|--------------|------------|-------------------|\n",
        "| Orders | `customer_id` | Customers | `customer_id` | Many-to-One |\n",
        "| Order Items | `order_id` | Orders | `order_id` | Many-to-One |\n",
        "| Order Items | `product_id` | Products | `product_id` | Many-to-One |\n",
        "| Order Items | `seller_id` | Sellers | `seller_id` | Many-to-One |\n",
        "| Payments | `order_id` | Orders | `order_id` | Many-to-One |\n",
        "| Reviews | `order_id` | Orders | `order_id` | One-to-One* |\n",
        "\n",
        "*Note: Reviews to Orders is typically One-to-One, but some orders may not have reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate key relationship concepts with examples\n",
        "print(\"üîó Key Relationship Examples\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Example 1: One-to-Many (Orders to Order Items)\n",
        "print(\"\\n1Ô∏è‚É£ One-to-Many: Orders ‚Üí Order Items\")\n",
        "print(\"One order can contain multiple products:\")\n",
        "print(\"\")\n",
        "print(\"Order ORD001:\")\n",
        "print(\"  ‚îú‚îÄ‚îÄ Item 1: Laptop (Seller A)\")\n",
        "print(\"  ‚îú‚îÄ‚îÄ Item 2: Mouse (Seller B)\")\n",
        "print(\"  ‚îî‚îÄ‚îÄ Item 3: Keyboard (Seller A)\")\n",
        "print(\"\")\n",
        "\n",
        "# Example 2: Many-to-One (Order Items to Products)\n",
        "print(\"2Ô∏è‚É£ Many-to-One: Order Items ‚Üí Products\")\n",
        "print(\"Multiple order items can reference the same product:\")\n",
        "print(\"\")\n",
        "print(\"Product PROD123 (iPhone):\")\n",
        "print(\"  ‚Üê Order ORD001, Item 1 (Customer A)\")\n",
        "print(\"  ‚Üê Order ORD002, Item 1 (Customer B)\")\n",
        "print(\"  ‚Üê Order ORD003, Item 2 (Customer C)\")\n",
        "print(\"\")\n",
        "\n",
        "# Example 3: Composite Keys\n",
        "print(\"3Ô∏è‚É£ Composite Keys: Order Items\")\n",
        "print(\"Order items identified by order_id + order_item_id:\")\n",
        "print(\"\")\n",
        "print(\"Primary Key Examples:\")\n",
        "print(\"  ‚Ä¢ (ORD001, 1) ‚Üí First item in order ORD001\")\n",
        "print(\"  ‚Ä¢ (ORD001, 2) ‚Üí Second item in order ORD001\")\n",
        "print(\"  ‚Ä¢ (ORD002, 1) ‚Üí First item in order ORD002\")\n",
        "print(\"\")\n",
        "\n",
        "# SQL to Pandas translation examples\n",
        "print(\"4Ô∏è‚É£ SQL to Pandas Translation:\")\n",
        "print(\"\")\n",
        "print(\"SQL JOIN:\")\n",
        "print(\"SELECT o.order_id, c.customer_city\")\n",
        "print(\"FROM orders o\")\n",
        "print(\"JOIN customers c ON o.customer_id = c.customer_id\")\n",
        "print(\"\")\n",
        "print(\"Pandas equivalent:\")\n",
        "print(\"pd.merge(orders, customers, on='customer_id')\")\n",
        "print(\"\")\n",
        "\n",
        "# Common join patterns\n",
        "print(\"5Ô∏è‚É£ Common Join Patterns:\")\n",
        "print(\"\")\n",
        "join_patterns = [\n",
        "    \"orders + customers ‚Üí Customer order analysis\",\n",
        "    \"orders + order_items ‚Üí Order detail analysis\",\n",
        "    \"order_items + products ‚Üí Product sales analysis\",\n",
        "    \"order_items + sellers ‚Üí Seller performance analysis\",\n",
        "    \"orders + reviews ‚Üí Customer satisfaction analysis\",\n",
        "    \"orders + payments ‚Üí Payment method analysis\"\n",
        "]\n",
        "\n",
        "for pattern in join_patterns:\n",
        "    print(f\"  ‚Ä¢ {pattern}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Quality Considerations\n",
        "\n",
        "Understanding the schema helps anticipate data quality issues you'll encounter:\n",
        "\n",
        "### üö® Common Data Quality Issues\n",
        "\n",
        "#### **Missing Values (NULL)**\n",
        "- **Product dimensions**: Not all products have complete physical measurements\n",
        "- **Review comments**: Many reviews have ratings but no text\n",
        "- **Delivery dates**: Canceled orders won't have delivery timestamps\n",
        "- **Geographic coordinates**: Some ZIP codes may lack precise coordinates\n",
        "\n",
        "#### **Data Type Consistency**\n",
        "- **Date formats**: All timestamps should be consistent\n",
        "- **Numeric precision**: Price values may have different decimal places\n",
        "- **String formatting**: City names may have inconsistent capitalization\n",
        "\n",
        "#### **Referential Integrity**\n",
        "- **Orphaned records**: Order items without corresponding orders\n",
        "- **Missing references**: Products referenced but not in products table\n",
        "- **Cascade effects**: Deleted orders affecting related reviews/payments\n",
        "\n",
        "#### **Business Logic Violations**\n",
        "- **Negative values**: Prices or quantities that shouldn't be negative\n",
        "- **Date inconsistencies**: Delivery before order date\n",
        "- **Status mismatches**: Delivered orders without delivery dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create examples of data quality checks you'll need to perform\n",
        "print(\"üîç Data Quality Checklist for Olist Dataset\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "quality_checks = {\n",
        "    \"üèóÔ∏è Structural Checks\": [\n",
        "        \"Verify all expected columns are present\",\n",
        "        \"Check data types match schema definitions\",\n",
        "        \"Confirm primary key uniqueness\",\n",
        "        \"Validate foreign key references exist\"\n",
        "    ],\n",
        "    \n",
        "    \"üìä Completeness Checks\": [\n",
        "        \"Identify missing values in required fields\",\n",
        "        \"Calculate completion rates for optional fields\",\n",
        "        \"Check for empty strings vs. NULL values\",\n",
        "        \"Assess overall dataset coverage\"\n",
        "    ],\n",
        "    \n",
        "    \"‚úÖ Validity Checks\": [\n",
        "        \"Verify order status values are valid\",\n",
        "        \"Check review scores are between 1-5\",\n",
        "        \"Validate Brazilian state codes (27 states)\",\n",
        "        \"Confirm ZIP code format (5 digits)\"\n",
        "    ],\n",
        "    \n",
        "    \"üìÖ Temporal Checks\": [\n",
        "        \"Verify purchase_date ‚â§ approval_date\",\n",
        "        \"Check approval_date ‚â§ shipped_date\",\n",
        "        \"Validate shipped_date ‚â§ delivery_date\",\n",
        "        \"Ensure delivery_date ‚â§ estimated_date tolerance\"\n",
        "    ],\n",
        "    \n",
        "    \"üí∞ Business Logic Checks\": [\n",
        "        \"Confirm prices are positive values\",\n",
        "        \"Check freight costs are non-negative\",\n",
        "        \"Verify payment amounts match order totals\",\n",
        "        \"Validate installment counts are reasonable\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for category, checks in quality_checks.items():\n",
        "    print(f\"\\n{category}:\")\n",
        "    for check in checks:\n",
        "        print(f\"  ‚úì {check}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*55)\n",
        "print(\"üí° Pro Tips for Data Quality:\")\n",
        "print(\"  ‚Ä¢ Always profile data before analysis\")\n",
        "print(\"  ‚Ä¢ Document assumptions about missing data\")\n",
        "print(\"  ‚Ä¢ Create data quality reports for stakeholders\")\n",
        "print(\"  ‚Ä¢ Set up automated quality checks for ongoing analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. SQL to Pandas Query Planning\n",
        "\n",
        "### üó∫Ô∏è Query Planning Framework\n",
        "\n",
        "Before writing code, plan your multi-table queries:\n",
        "\n",
        "#### Step 1: Define Business Question\n",
        "- What specific insight are you seeking?\n",
        "- What metrics do you need to calculate?\n",
        "- What dimensions do you need to group by?\n",
        "\n",
        "#### Step 2: Identify Required Tables\n",
        "- Which entities contain your needed data?\n",
        "- What are the joining paths between tables?\n",
        "- Are there multiple ways to get the same data?\n",
        "\n",
        "#### Step 3: Map Relationships\n",
        "- What are the foreign key connections?\n",
        "- What type of joins do you need (inner, left, etc.)?\n",
        "- What's the expected result size?\n",
        "\n",
        "#### Step 4: Plan Join Sequence\n",
        "- Start with the main entity table\n",
        "- Add related tables one by one\n",
        "- Consider performance implications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a query planning template\n",
        "print(\"üìã Multi-Table Query Planning Template\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# Example business questions with their table requirements\n",
        "query_examples = [\n",
        "    {\n",
        "        'question': 'What is the average order value by customer state?',\n",
        "        'tables': ['orders', 'customers'],\n",
        "        'join_keys': ['customer_id'],\n",
        "        'metrics': ['AVG(order_total)'],\n",
        "        'group_by': ['customer_state'],\n",
        "        'sql': '''SELECT c.customer_state, AVG(o.total_amount) as avg_order_value\n",
        "FROM orders o\n",
        "JOIN customers c ON o.customer_id = c.customer_id\n",
        "GROUP BY c.customer_state''',\n",
        "        'pandas': '''orders_customers = pd.merge(orders, customers, on='customer_id')\n",
        "result = orders_customers.groupby('customer_state')['total_amount'].mean()'''\n",
        "    },\n",
        "    {\n",
        "        'question': 'Which product category has the highest customer satisfaction?',\n",
        "        'tables': ['orders', 'order_items', 'products', 'reviews', 'categories'],\n",
        "        'join_keys': ['order_id', 'product_id', 'product_category_name'],\n",
        "        'metrics': ['AVG(review_score)'],\n",
        "        'group_by': ['product_category_name_english'],\n",
        "        'sql': '''SELECT cat.product_category_name_english, AVG(r.review_score) as avg_rating\n",
        "FROM reviews r\n",
        "JOIN orders o ON r.order_id = o.order_id\n",
        "JOIN order_items oi ON o.order_id = oi.order_id\n",
        "JOIN products p ON oi.product_id = p.product_id\n",
        "JOIN categories cat ON p.product_category_name = cat.product_category_name\n",
        "GROUP BY cat.product_category_name_english''',\n",
        "        'pandas': '''# Step-by-step approach\n",
        "step1 = pd.merge(reviews, orders, on='order_id')\n",
        "step2 = pd.merge(step1, order_items, on='order_id')\n",
        "step3 = pd.merge(step2, products, on='product_id')\n",
        "result = pd.merge(step3, categories, on='product_category_name')\n",
        "final = result.groupby('product_category_name_english')['review_score'].mean()'''\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, example in enumerate(query_examples, 1):\n",
        "    print(f\"\\nüéØ Example {i}: {example['question']}\")\n",
        "    print(f\"üìä Tables needed: {', '.join(example['tables'])}\")\n",
        "    print(f\"üîó Join keys: {', '.join(example['join_keys'])}\")\n",
        "    print(f\"üìà Metrics: {', '.join(example['metrics'])}\")\n",
        "    print(f\"üìã Group by: {', '.join(example['group_by'])}\")\n",
        "    \n",
        "    print(\"\\nüíæ SQL Version:\")\n",
        "    for line in example['sql'].split('\\n'):\n",
        "        print(f\"    {line}\")\n",
        "    \n",
        "    print(\"\\nüêç Pandas Version:\")\n",
        "    for line in example['pandas'].split('\\n'):\n",
        "        print(f\"    {line}\")\n",
        "    \n",
        "    print(\"-\" * 45)\n",
        "\n",
        "print(\"\\nüí° Query Planning Best Practices:\")\n",
        "print(\"  ‚Ä¢ Start simple, add complexity gradually\")\n",
        "print(\"  ‚Ä¢ Verify join results at each step\")\n",
        "print(\"  ‚Ä¢ Consider performance for large datasets\")\n",
        "print(\"  ‚Ä¢ Document your join logic for others\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Performance Considerations\n",
        "\n",
        "### üìä Dataset Size Implications\n",
        "\n",
        "Understanding the relative sizes of tables helps plan efficient queries:\n",
        "\n",
        "| Table | Approximate Rows | Memory Impact | Join Considerations |\n",
        "|-------|------------------|---------------|--------------------|\n",
        "| **Orders** | 100K | Medium | Central table - use as starting point |\n",
        "| **Order Items** | 112K | Medium | Larger than orders (multi-item orders) |\n",
        "| **Customers** | 99K | Small | Light table - safe to join early |\n",
        "| **Sellers** | 3K | Very Small | Tiny table - negligible impact |\n",
        "| **Products** | 32K | Small | Moderate size - consider selectivity |\n",
        "| **Reviews** | 100K | Medium | Similar to orders - 1:1 relationship |\n",
        "| **Payments** | 103K | Medium | Slightly larger than orders |\n",
        "| **Geolocation** | 1M | Large | Biggest table - join carefully |\n",
        "| **Categories** | 71 | Tiny | Lookup table - no performance impact |\n",
        "\n",
        "### ‚ö° Performance Optimization Tips\n",
        "\n",
        "#### **Join Order Strategy**\n",
        "1. **Start with filtered main table** (e.g., orders for specific date range)\n",
        "2. **Add small lookup tables first** (categories, sellers)\n",
        "3. **Add larger tables incrementally** (customers, products)\n",
        "4. **Save geolocation joins for last** (if needed)\n",
        "\n",
        "#### **Memory Management**\n",
        "- **Select only needed columns** before joining\n",
        "- **Filter data early** in the process\n",
        "- **Use categorical data types** for repeated strings\n",
        "- **Consider chunking** for very large analyses\n",
        "\n",
        "#### **Query Optimization**\n",
        "- **Avoid Cartesian products** (unexpected row multiplication)\n",
        "- **Use appropriate join types** (inner vs. left)\n",
        "- **Aggregate before joining** when possible\n",
        "- **Cache intermediate results** for repeated use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance optimization examples\n",
        "print(\"‚ö° Performance Optimization Examples\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "print(\"\\n‚ùå INEFFICIENT APPROACH:\")\n",
        "print(\"# Loading everything first, then filtering\")\n",
        "inefficient_code = '''# Don't do this!\n",
        "all_data = orders.merge(customers, on='customer_id')\\\n",
        "                .merge(order_items, on='order_id')\\\n",
        "                .merge(products, on='product_id')\\\n",
        "                .merge(sellers, on='seller_id')\\\n",
        "                .merge(geolocation, left_on='customer_zip_code_prefix', \n",
        "                       right_on='geolocation_zip_code_prefix')\n",
        "\n",
        "# Then filter (too late!)\n",
        "result = all_data[all_data['order_status'] == 'delivered']\n",
        "result = result[result['customer_state'] == 'SP']'''\n",
        "\n",
        "for line in inefficient_code.split('\\n'):\n",
        "    print(f\"    {line}\")\n",
        "\n",
        "print(\"\\n‚úÖ EFFICIENT APPROACH:\")\n",
        "print(\"# Filter first, then join incrementally\")\n",
        "efficient_code = '''# Do this instead!\n",
        "# 1. Filter main table first\n",
        "delivered_orders = orders[orders['order_status'] == 'delivered']\n",
        "\n",
        "# 2. Add customer info and filter by state\n",
        "orders_customers = delivered_orders.merge(customers, on='customer_id')\n",
        "sp_orders = orders_customers[orders_customers['customer_state'] == 'SP']\n",
        "\n",
        "# 3. Add other tables incrementally\n",
        "with_items = sp_orders.merge(order_items, on='order_id')\n",
        "with_products = with_items.merge(products, on='product_id')\n",
        "\n",
        "# 4. Only add geolocation if actually needed\n",
        "if need_coordinates:\n",
        "    final = with_products.merge(geolocation, \n",
        "                               left_on='customer_zip_code_prefix',\n",
        "                               right_on='geolocation_zip_code_prefix')'''\n",
        "\n",
        "for line in efficient_code.split('\\n'):\n",
        "    print(f\"    {line}\")\n",
        "\n",
        "print(\"\\nüìä MEMORY-EFFICIENT COLUMN SELECTION:\")\n",
        "memory_code = '''# Select only needed columns\n",
        "orders_slim = orders[['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp']]\n",
        "customers_slim = customers[['customer_id', 'customer_state', 'customer_city']]\n",
        "\n",
        "# Join with reduced datasets\n",
        "result = orders_slim.merge(customers_slim, on='customer_id')'''\n",
        "\n",
        "for line in memory_code.split('\\n'):\n",
        "    print(f\"    {line}\")\n",
        "\n",
        "print(\"\\nüéØ AGGREGATION OPTIMIZATION:\")\n",
        "agg_code = '''# Aggregate before joining when possible\n",
        "# Instead of joining all order items then aggregating:\n",
        "order_totals = order_items.groupby('order_id').agg({\n",
        "    'price': 'sum',\n",
        "    'freight_value': 'sum',\n",
        "    'order_item_id': 'count'\n",
        "}).rename(columns={'order_item_id': 'item_count'})\n",
        "\n",
        "# Then join the aggregated results\n",
        "orders_with_totals = orders.merge(order_totals, on='order_id')'''\n",
        "\n",
        "for line in agg_code.split('\\n'):\n",
        "    print(f\"    {line}\")\n",
        "\n",
        "print(\"\\nüí° Performance Tips Summary:\")\n",
        "tips = [\n",
        "    \"Filter early and often\",\n",
        "    \"Select only needed columns\",\n",
        "    \"Join small tables first\",\n",
        "    \"Aggregate before joining when possible\",\n",
        "    \"Use categorical dtypes for repeated strings\",\n",
        "    \"Monitor memory usage with df.info()\",\n",
        "    \"Cache intermediate results for reuse\"\n",
        "]\n",
        "\n",
        "for tip in tips:\n",
        "    print(f\"  ‚úì {tip}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Preparation for Part 3: Loading Data\n",
        "\n",
        "### üóÇÔ∏è File Organization\n",
        "\n",
        "Before loading the actual Olist data, you should understand how the files are organized:\n",
        "\n",
        "#### **Expected File Structure**\n",
        "```\n",
        "olist_dataset/\n",
        "‚îú‚îÄ‚îÄ olist_orders_dataset.csv\n",
        "‚îú‚îÄ‚îÄ olist_order_items_dataset.csv\n",
        "‚îú‚îÄ‚îÄ olist_customers_dataset.csv\n",
        "‚îú‚îÄ‚îÄ olist_sellers_dataset.csv\n",
        "‚îú‚îÄ‚îÄ olist_products_dataset.csv\n",
        "‚îú‚îÄ‚îÄ olist_order_reviews_dataset.csv\n",
        "‚îú‚îÄ‚îÄ olist_order_payments_dataset.csv\n",
        "‚îú‚îÄ‚îÄ olist_geolocation_dataset.csv\n",
        "‚îî‚îÄ‚îÄ product_category_name_translation.csv\n",
        "```\n",
        "\n",
        "### üìã Loading Checklist\n",
        "\n",
        "When you load the data in Part 3, you'll need to:\n",
        "\n",
        "#### **1. File Validation**\n",
        "- ‚úÖ Confirm all 9 files are present\n",
        "- ‚úÖ Check file sizes are reasonable\n",
        "- ‚úÖ Verify CSV format and encoding\n",
        "\n",
        "#### **2. Schema Validation**\n",
        "- ‚úÖ Confirm column names match expected schema\n",
        "- ‚úÖ Check data types are appropriate\n",
        "- ‚úÖ Validate primary key uniqueness\n",
        "\n",
        "#### **3. Data Quality Assessment**\n",
        "- ‚úÖ Check for missing values\n",
        "- ‚úÖ Identify outliers and anomalies\n",
        "- ‚úÖ Validate referential integrity\n",
        "\n",
        "#### **4. Initial Exploration**\n",
        "- ‚úÖ Calculate basic statistics\n",
        "- ‚úÖ Test join operations\n",
        "- ‚úÖ Create sample analyses\n",
        "\n",
        "### üéØ Learning Objectives for Part 3\n",
        "\n",
        "In the next session, you will:\n",
        "- Load all 9 Olist dataset files efficiently\n",
        "- Perform comprehensive data quality assessment\n",
        "- Execute your first multi-table joins\n",
        "- Create initial business insights\n",
        "- Prepare data for ongoing analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a data loading preparation checklist\n",
        "print(\"üìã Data Loading Preparation Checklist\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "checklist_items = {\n",
        "    \"üóÇÔ∏è File Preparation\": [\n",
        "        \"Download Olist dataset from Kaggle\",\n",
        "        \"Extract all CSV files to working directory\",\n",
        "        \"Verify file names match expected schema\",\n",
        "        \"Check file sizes are reasonable (not corrupted)\"\n",
        "    ],\n",
        "    \n",
        "    \"üíª Environment Setup\": [\n",
        "        \"Import necessary libraries (pandas, numpy, matplotlib)\",\n",
        "        \"Set pandas display options for exploration\",\n",
        "        \"Configure memory usage monitoring\",\n",
        "        \"Prepare data directory paths\"\n",
        "    ],\n",
        "    \n",
        "    \"üîç Schema Validation Plan\": [\n",
        "        \"Load each file with proper data types\",\n",
        "        \"Check column names against schema documentation\",\n",
        "        \"Validate primary key uniqueness\",\n",
        "        \"Test foreign key relationships\"\n",
        "    ],\n",
        "    \n",
        "    \"üìä Initial Analysis Plan\": [\n",
        "        \"Calculate basic statistics for each table\",\n",
        "        \"Identify missing value patterns\",\n",
        "        \"Test simple join operations\",\n",
        "        \"Create first business insights\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for category, items in checklist_items.items():\n",
        "    print(f\"\\n{category}:\")\n",
        "    for item in items:\n",
        "        print(f\"  ‚òê {item}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 45)\n",
        "print(\"üéØ Ready for Part 3: Loading Data from Multiple Tables\")\n",
        "print(\"\\nNext session preview:\")\n",
        "print(\"  ‚Üí Load all 9 Olist dataset files\")\n",
        "print(\"  ‚Üí Perform data quality assessment\")\n",
        "print(\"  ‚Üí Execute multi-table joins\")\n",
        "print(\"  ‚Üí Create first business analysis\")\n",
        "print(\"  ‚Üí Prepare for Major Group Assignment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Key Takeaways\n",
        "\n",
        "### üéØ **Schema Understanding is Foundation**\n",
        "- **Table Structure**: Each table has a specific purpose and relationship pattern\n",
        "- **Data Types**: Understanding constraints helps predict data quality issues\n",
        "- **Business Logic**: Schema reflects real e-commerce operations and constraints\n",
        "\n",
        "### üîó **Relationships Enable Complex Analysis**\n",
        "- **Primary Keys**: Ensure unique identification of records\n",
        "- **Foreign Keys**: Enable joining related data across tables\n",
        "- **Cardinality**: Understanding 1:1, 1:M, M:1 relationships prevents join errors\n",
        "\n",
        "### üóÇÔ∏è **Multi-Table Strategy**\n",
        "- **Central Entity**: Orders table serves as the hub for most analyses\n",
        "- **Join Patterns**: Common patterns emerge for different business questions\n",
        "- **Performance**: Table sizes and join order affect query performance\n",
        "\n",
        "### üîç **Data Quality Awareness**\n",
        "- **Missing Values**: Expected in certain fields, problematic in others\n",
        "- **Referential Integrity**: Foreign key relationships must be validated\n",
        "- **Business Rules**: Schema constraints reflect real-world business logic\n",
        "\n",
        "### üìä **SQL Knowledge Transfers**\n",
        "- **JOIN Operations**: Directly translate to pandas merge operations\n",
        "- **Query Planning**: Same logical approach applies to both SQL and Pandas\n",
        "- **Performance Concepts**: Filtering, indexing, and optimization principles apply\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "You now have the technical foundation to work with the Olist dataset effectively. In **Part 3**, you'll put this knowledge into practice by loading the actual data files and performing your first multi-table analyses.\n",
        "\n",
        "**Coming up in Part 3: Loading Data from Multiple Tables**\n",
        "- Efficient data loading techniques\n",
        "- Data quality assessment in practice\n",
        "- Your first multi-table business analysis\n",
        "- Preparation for the Major Group Assignment\n",
        "\n",
        "### üöÄ **You're Ready!**\n",
        "With this deep understanding of the database schema and relationships, you're prepared to unlock the full analytical potential of the Olist dataset. The complexity you see here is what makes real-world data analysis both challenging and rewarding!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}