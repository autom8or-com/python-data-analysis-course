{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 - Part 3: Deployment and Production\n",
    "\n",
    "**Course:** Python Data Analysis for Business Intelligence  \n",
    "**Week:** 9 | **Session:** Thursday | **Part:** 3 of 3  \n",
    "**Duration:** 20 minutes | **Date:** June 5, 2025\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will be able to:\n",
    "- Deploy Streamlit applications to production using Streamlit Cloud\n",
    "- Implement environment management and secure secrets handling\n",
    "- Optimize application performance for production workloads\n",
    "- Set up monitoring, logging, and alerting for deployed applications\n",
    "- Implement CI/CD pipelines for automated deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Business Context: Production Deployment Strategy\n",
    "\n",
    "**Enterprise Challenge**: Olist's business intelligence platform must be deployed to serve multiple stakeholder groups with enterprise-grade reliability:\n",
    "\n",
    "### Production Requirements:\n",
    "- **👥 User Base**: 500+ concurrent users across Brazil\n",
    "- **⚡ Performance**: <2 second load times, 99.9% uptime\n",
    "- **🔒 Security**: Enterprise authentication, data encryption, audit trails\n",
    "- **📈 Scalability**: Auto-scaling for peak usage periods\n",
    "- **🌍 Global Access**: Multi-region deployment for low latency\n",
    "\n",
    "### Deployment Architecture:\n",
    "- **Frontend**: Streamlit Cloud with custom domain\n",
    "- **Database**: Supabase with read replicas\n",
    "- **Monitoring**: Real-time performance and error tracking\n",
    "- **CI/CD**: Automated testing and deployment pipeline\n",
    "\n",
    "**Production Challenge**: Move from development prototype to enterprise-grade production system that can handle real business operations.\n",
    "\n",
    "**Today's Solution**: Complete deployment workflow from local development to production monitoring with enterprise DevOps practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Setup: Production Environment Preparation\n",
    "\n",
    "Let's prepare our development environment for production deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production deployment imports\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional\n",
    "import hashlib\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure production logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('app.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Course utilities\n",
    "from Utilities.visualization_helper import set_plotting_style\n",
    "from Utilities.colab_helper import setup_colab\n",
    "\n",
    "# Setup\n",
    "plt, sns = set_plotting_style()\n",
    "setup_colab()\n",
    "\n",
    "print(\"✅ Production deployment environment ready!\")\n",
    "print(\"🚀 Ready to deploy enterprise-grade Streamlit applications\")\n",
    "print(\"📊 DevOps, monitoring, and optimization patterns loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Section 1: Streamlit Cloud Deployment (8 minutes)\n",
    "\n",
    "Let's create a complete deployment workflow for Streamlit Cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile deployment_ready_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Configure production settings\n",
    "st.set_page_config(\n",
    "    page_title=\"Olist Business Intelligence Platform\",\n",
    "    page_icon=\"🏢\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\",\n",
    "    menu_items={\n",
    "        'Get Help': 'https://docs.olist.com/support',\n",
    "        'Report a bug': 'https://github.com/olist/bi-platform/issues',\n",
    "        'About': \"Olist BI Platform v2.1 - Built with Streamlit\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Production environment configuration\n",
    "class ProductionConfig:\n",
    "    \"\"\"\n",
    "    Production configuration management with environment-specific settings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.environment = os.getenv('STREAMLIT_ENV', 'development')\n",
    "        self.debug_mode = os.getenv('DEBUG', 'False').lower() == 'true'\n",
    "        self.version = os.getenv('APP_VERSION', 'v2.1.0')\n",
    "        \n",
    "        # Configure logging based on environment\n",
    "        log_level = logging.DEBUG if self.debug_mode else logging.INFO\n",
    "        logging.basicConfig(\n",
    "            level=log_level,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def get_database_config(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get database configuration from environment or Streamlit secrets.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try Streamlit secrets first (production)\n",
    "            if hasattr(st, 'secrets') and 'database' in st.secrets:\n",
    "                return {\n",
    "                    'url': st.secrets['database']['url'],\n",
    "                    'api_key': st.secrets['database']['api_key'],\n",
    "                    'connection_pool_size': st.secrets['database'].get('pool_size', 10)\n",
    "                }\n",
    "            \n",
    "            # Fallback to environment variables (development)\n",
    "            return {\n",
    "                'url': os.getenv('DATABASE_URL'),\n",
    "                'api_key': os.getenv('DATABASE_API_KEY'),\n",
    "                'connection_pool_size': int(os.getenv('DB_POOL_SIZE', '5'))\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load database config: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def get_api_config(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get external API configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if hasattr(st, 'secrets') and 'apis' in st.secrets:\n",
    "                return {\n",
    "                    'analytics_key': st.secrets['apis']['google_analytics'],\n",
    "                    'monitoring_key': st.secrets['apis']['datadog_key'],\n",
    "                    'email_service': st.secrets['apis']['sendgrid_key']\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                'analytics_key': os.getenv('GOOGLE_ANALYTICS_KEY'),\n",
    "                'monitoring_key': os.getenv('DATADOG_API_KEY'),\n",
    "                'email_service': os.getenv('SENDGRID_API_KEY')\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"API config not available: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Initialize production configuration\n",
    "config = ProductionConfig()\n",
    "logger = config.logger\n",
    "\n",
    "# Performance monitoring\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Monitor application performance and user interactions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        if 'performance_metrics' not in st.session_state:\n",
    "            st.session_state.performance_metrics = {\n",
    "                'page_loads': 0,\n",
    "                'query_times': [],\n",
    "                'error_count': 0,\n",
    "                'user_sessions': 0,\n",
    "                'last_active': datetime.now()\n",
    "            }\n",
    "    \n",
    "    def track_page_load(self, page_name: str):\n",
    "        \"\"\"\n",
    "        Track page load event.\n",
    "        \"\"\"\n",
    "        st.session_state.performance_metrics['page_loads'] += 1\n",
    "        st.session_state.performance_metrics['last_active'] = datetime.now()\n",
    "        \n",
    "        logger.info(f\"Page loaded: {page_name}\")\n",
    "        \n",
    "        # In production, send to analytics service\n",
    "        # analytics.track('page_view', {'page': page_name, 'timestamp': datetime.now()})\n",
    "    \n",
    "    def track_query_performance(self, query_type: str, execution_time: float):\n",
    "        \"\"\"\n",
    "        Track database query performance.\n",
    "        \"\"\"\n",
    "        st.session_state.performance_metrics['query_times'].append({\n",
    "            'type': query_type,\n",
    "            'time': execution_time,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Query executed: {query_type} in {execution_time:.3f}s\")\n",
    "        \n",
    "        # Alert on slow queries\n",
    "        if execution_time > 5.0:\n",
    "            logger.warning(f\"Slow query detected: {query_type} took {execution_time:.3f}s\")\n",
    "    \n",
    "    def track_error(self, error_type: str, error_message: str):\n",
    "        \"\"\"\n",
    "        Track application errors.\n",
    "        \"\"\"\n",
    "        st.session_state.performance_metrics['error_count'] += 1\n",
    "        \n",
    "        logger.error(f\"Application error: {error_type} - {error_message}\")\n",
    "        \n",
    "        # In production, send to error tracking service\n",
    "        # sentry.capture_exception(error_message)\n",
    "    \n",
    "    def get_metrics_summary(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get performance metrics summary.\n",
    "        \"\"\"\n",
    "        metrics = st.session_state.performance_metrics\n",
    "        \n",
    "        avg_query_time = 0\n",
    "        if metrics['query_times']:\n",
    "            avg_query_time = sum(q['time'] for q in metrics['query_times']) / len(metrics['query_times'])\n",
    "        \n",
    "        return {\n",
    "            'page_loads': metrics['page_loads'],\n",
    "            'total_queries': len(metrics['query_times']),\n",
    "            'avg_query_time': avg_query_time,\n",
    "            'error_count': metrics['error_count'],\n",
    "            'uptime_minutes': (datetime.now() - metrics['last_active']).total_seconds() / 60\n",
    "        }\n",
    "\n",
    "# Initialize performance monitoring\n",
    "monitor = PerformanceMonitor()\n",
    "monitor.track_page_load('main_dashboard')\n",
    "\n",
    "# Custom CSS for production branding\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    /* Production branding */\n",
    "    .main-header {\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "        padding: 2rem;\n",
    "        border-radius: 10px;\n",
    "        color: white;\n",
    "        margin-bottom: 2rem;\n",
    "    }\n",
    "    \n",
    "    /* Environment indicator */\n",
    "    .env-indicator {\n",
    "        position: fixed;\n",
    "        top: 10px;\n",
    "        right: 10px;\n",
    "        padding: 5px 10px;\n",
    "        border-radius: 15px;\n",
    "        font-size: 12px;\n",
    "        font-weight: bold;\n",
    "        z-index: 1000;\n",
    "    }\n",
    "    \n",
    "    .env-production {\n",
    "        background: #28a745;\n",
    "        color: white;\n",
    "    }\n",
    "    \n",
    "    .env-development {\n",
    "        background: #ffc107;\n",
    "        color: black;\n",
    "    }\n",
    "    \n",
    "    /* Performance indicators */\n",
    "    .performance-good {\n",
    "        background: #d4edda;\n",
    "        border: 1px solid #c3e6cb;\n",
    "        color: #155724;\n",
    "        padding: 0.75rem;\n",
    "        border-radius: 5px;\n",
    "        border-left: 4px solid #28a745;\n",
    "    }\n",
    "    \n",
    "    .performance-warning {\n",
    "        background: #fff3cd;\n",
    "        border: 1px solid #ffeaa7;\n",
    "        color: #856404;\n",
    "        padding: 0.75rem;\n",
    "        border-radius: 5px;\n",
    "        border-left: 4px solid #ffc107;\n",
    "    }\n",
    "    \n",
    "    .performance-critical {\n",
    "        background: #f8d7da;\n",
    "        border: 1px solid #f5c6cb;\n",
    "        color: #721c24;\n",
    "        padding: 0.75rem;\n",
    "        border-radius: 5px;\n",
    "        border-left: 4px solid #dc3545;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Environment indicator\n",
    "env_class = \"env-production\" if config.environment == \"production\" else \"env-development\"\n",
    "st.markdown(f\"\"\"\n",
    "<div class=\"env-indicator {env_class}\">\n",
    "    {config.environment.upper()} • {config.version}\n",
    "</div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Production header\n",
    "st.markdown(\"\"\"\n",
    "<div class=\"main-header\">\n",
    "    <h1>🏢 Olist Business Intelligence Platform</h1>\n",
    "    <p>Production Dashboard • Real-time Analytics • Enterprise Grade</p>\n",
    "</div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Health check endpoint simulation\n",
    "def health_check() -> Dict:\n",
    "    \"\"\"\n",
    "    Application health check for monitoring.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check database connectivity\n",
    "        db_config = config.get_database_config()\n",
    "        db_healthy = bool(db_config.get('url'))\n",
    "        \n",
    "        # Check performance metrics\n",
    "        metrics = monitor.get_metrics_summary()\n",
    "        performance_healthy = metrics['avg_query_time'] < 2.0 and metrics['error_count'] < 10\n",
    "        \n",
    "        # Overall health status\n",
    "        overall_healthy = db_healthy and performance_healthy\n",
    "        \n",
    "        return {\n",
    "            'status': 'healthy' if overall_healthy else 'unhealthy',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'version': config.version,\n",
    "            'environment': config.environment,\n",
    "            'database': 'connected' if db_healthy else 'disconnected',\n",
    "            'performance': 'good' if performance_healthy else 'degraded',\n",
    "            'uptime_minutes': metrics['uptime_minutes'],\n",
    "            'total_requests': metrics['page_loads']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Health check failed: {e}\")\n",
    "        return {\n",
    "            'status': 'unhealthy',\n",
    "            'error': str(e),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Application monitoring dashboard\n",
    "st.subheader(\"📊 Application Health & Performance\")\n",
    "\n",
    "health_data = health_check()\n",
    "performance_metrics = monitor.get_metrics_summary()\n",
    "\n",
    "# Health status indicators\n",
    "health_col1, health_col2, health_col3, health_col4 = st.columns(4)\n",
    "\n",
    "with health_col1:\n",
    "    status_emoji = \"🟢\" if health_data['status'] == 'healthy' else \"🔴\"\n",
    "    st.metric(\n",
    "        f\"{status_emoji} System Health\",\n",
    "        health_data['status'].title(),\n",
    "        f\"Environment: {config.environment}\"\n",
    "    )\n",
    "\n",
    "with health_col2:\n",
    "    st.metric(\n",
    "        \"📡 Database Status\",\n",
    "        health_data.get('database', 'unknown').title(),\n",
    "        \"Real-time\"\n",
    "    )\n",
    "\n",
    "with health_col3:\n",
    "    st.metric(\n",
    "        \"⚡ Avg Response Time\",\n",
    "        f\"{performance_metrics['avg_query_time']:.3f}s\",\n",
    "        \"Target: <2.0s\"\n",
    "    )\n",
    "\n",
    "with health_col4:\n",
    "    st.metric(\n",
    "        \"🔢 Total Requests\",\n",
    "        f\"{performance_metrics['page_loads']:,}\",\n",
    "        f\"Errors: {performance_metrics['error_count']}\"\n",
    "    )\n",
    "\n",
    "# Performance status\n",
    "if performance_metrics['avg_query_time'] < 1.0:\n",
    "    performance_class = \"performance-good\"\n",
    "    performance_message = \"🚀 Application performance is excellent\"\n",
    "elif performance_metrics['avg_query_time'] < 2.0:\n",
    "    performance_class = \"performance-warning\"\n",
    "    performance_message = \"⚠️ Application performance is acceptable but could be improved\"\n",
    "else:\n",
    "    performance_class = \"performance-critical\"\n",
    "    performance_message = \"🚨 Application performance needs immediate attention\"\n",
    "\n",
    "st.markdown(f\"\"\"\n",
    "<div class=\"{performance_class}\">\n",
    "    <strong>Performance Status:</strong> {performance_message}\n",
    "</div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Sample business data with performance tracking\n",
    "@st.cache_data(ttl=300)  # 5-minute cache for production\n",
    "def load_production_data():\n",
    "    \"\"\"\n",
    "    Load production data with performance monitoring.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Simulate production data loading\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate business metrics\n",
    "        dates = pd.date_range('2024-01-01', periods=365, freq='D')\n",
    "        data = {\n",
    "            'date': dates,\n",
    "            'revenue': np.random.normal(50000, 10000, 365).cumsum(),\n",
    "            'orders': np.random.poisson(200, 365),\n",
    "            'customers': np.random.poisson(150, 365),\n",
    "            'satisfaction': np.random.normal(4.2, 0.3, 365)\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Track query performance\n",
    "        execution_time = time.time() - start_time\n",
    "        monitor.track_query_performance('load_production_data', execution_time)\n",
    "        \n",
    "        logger.info(f\"Production data loaded successfully in {execution_time:.3f}s\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        monitor.track_error('data_loading_error', str(e))\n",
    "        raise\n",
    "\n",
    "# Load and display production data\n",
    "try:\n",
    "    with st.spinner(\"Loading production data...\"):\n",
    "        production_df = load_production_data()\n",
    "    \n",
    "    # Business metrics dashboard\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"📈 Business Performance Dashboard\")\n",
    "    \n",
    "    # Current metrics\n",
    "    current_revenue = production_df['revenue'].iloc[-1]\n",
    "    current_orders = production_df['orders'].iloc[-30:].sum()\n",
    "    current_satisfaction = production_df['satisfaction'].iloc[-30:].mean()\n",
    "    \n",
    "    metric_col1, metric_col2, metric_col3 = st.columns(3)\n",
    "    \n",
    "    with metric_col1:\n",
    "        st.metric(\n",
    "            \"💰 Total Revenue\",\n",
    "            f\"R$ {current_revenue:,.0f}\",\n",
    "            \"YTD Performance\"\n",
    "        )\n",
    "    \n",
    "    with metric_col2:\n",
    "        st.metric(\n",
    "            \"📦 Orders (30d)\",\n",
    "            f\"{current_orders:,}\",\n",
    "            \"Recent Performance\"\n",
    "        )\n",
    "    \n",
    "    with metric_col3:\n",
    "        st.metric(\n",
    "            \"⭐ Satisfaction\",\n",
    "            f\"{current_satisfaction:.2f}/5.0\",\n",
    "            \"30-day Average\"\n",
    "        )\n",
    "    \n",
    "    # Revenue trend chart\n",
    "    st.subheader(\"📊 Revenue Trend Analysis\")\n",
    "    \n",
    "    import plotly.express as px\n",
    "    \n",
    "    fig = px.line(\n",
    "        production_df.tail(90),  # Last 90 days\n",
    "        x='date',\n",
    "        y='revenue',\n",
    "        title=\"Revenue Trend (Last 90 Days)\",\n",
    "        labels={'revenue': 'Revenue (R$)', 'date': 'Date'}\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(line=dict(color='#667eea', width=3))\n",
    "    fig.update_layout(height=400)\n",
    "    \n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "except Exception as e:\n",
    "    st.error(f\"❌ Failed to load production data: {str(e)}\")\n",
    "    monitor.track_error('dashboard_error', str(e))\n",
    "\n",
    "# Production deployment information\n",
    "with st.sidebar:\n",
    "    st.header(\"🚀 Deployment Info\")\n",
    "    \n",
    "    st.markdown(f\"\"\"\n",
    "    **Environment:** {config.environment}  \n",
    "    **Version:** {config.version}  \n",
    "    **Build:** {datetime.now().strftime('%Y%m%d-%H%M')}  \n",
    "    **Status:** {health_data['status'].title()}  \n",
    "    \"\"\")\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"📊 Performance Metrics\")\n",
    "    \n",
    "    st.metric(\"Page Loads\", performance_metrics['page_loads'])\n",
    "    st.metric(\"Total Queries\", performance_metrics['total_queries'])\n",
    "    st.metric(\"Error Count\", performance_metrics['error_count'])\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"🔧 Admin Tools\")\n",
    "    \n",
    "    if st.button(\"🔄 Clear Cache\", use_container_width=True):\n",
    "        st.cache_data.clear()\n",
    "        st.success(\"Cache cleared!\")\n",
    "    \n",
    "    if st.button(\"📊 Full Health Check\", use_container_width=True):\n",
    "        health_result = health_check()\n",
    "        st.json(health_result)\n",
    "    \n",
    "    if st.button(\"📥 Export Logs\", use_container_width=True):\n",
    "        # In production, this would export actual logs\n",
    "        st.info(\"Logs exported to admin panel\")\n",
    "\n",
    "# Footer with deployment information\n",
    "st.markdown(\"---\")\n",
    "st.markdown(f\"\"\"\n",
    "<div style=\"text-align: center; color: #666; padding: 1rem;\">\n",
    "    <p><strong>🏢 Olist Business Intelligence Platform</strong> | \n",
    "    Version {config.version} | \n",
    "    Environment: {config.environment.title()} | \n",
    "    <a href=\"#\" style=\"color: #667eea;\">Support</a> | \n",
    "    <a href=\"#\" style=\"color: #667eea;\">Documentation</a></p>\n",
    "    <p>Deployed on Streamlit Cloud | \n",
    "    Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC</p>\n",
    "</div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Log successful page render\n",
    "logger.info(\"Dashboard rendered successfully for user session\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Section 2: Production Configuration Files (7 minutes)\n",
    "\n",
    "Let's create all the necessary configuration files for production deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create production configuration files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create project structure\n",
    "project_files = {\n",
    "    'requirements.txt': '''\n",
    "streamlit>=1.28.0\n",
    "pandas>=1.5.0\n",
    "numpy>=1.24.0\n",
    "plotly>=5.15.0\n",
    "supabase>=1.0.3\n",
    "python-dotenv>=1.0.0\n",
    "psycopg2-binary>=2.9.5\n",
    "requests>=2.28.0\n",
    "'''.strip(),\n",
    "    \n",
    "    '.streamlit/config.toml': '''\n",
    "[global]\n",
    "developmentMode = false\n",
    "showWarningOnDirectExecution = false\n",
    "\n",
    "[server]\n",
    "headless = true\n",
    "enableCORS = false\n",
    "enableXsrfProtection = true\n",
    "maxUploadSize = 200\n",
    "\n",
    "[browser]\n",
    "gatherUsageStats = false\n",
    "serverAddress = \"0.0.0.0\"\n",
    "serverPort = 8501\n",
    "\n",
    "[theme]\n",
    "primaryColor = \"#667eea\"\n",
    "backgroundColor = \"#ffffff\"\n",
    "secondaryBackgroundColor = \"#f0f2f6\"\n",
    "textColor = \"#262730\"\n",
    "font = \"sans serif\"\n",
    "''',\n",
    "    \n",
    "    '.streamlit/secrets.toml': '''\n",
    "# Production secrets (example - use actual values in production)\n",
    "[database]\n",
    "url = \"your-supabase-url\"\n",
    "api_key = \"your-supabase-anon-key\"\n",
    "service_role_key = \"your-service-role-key\"\n",
    "pool_size = 10\n",
    "\n",
    "[apis]\n",
    "google_analytics = \"your-ga-key\"\n",
    "datadog_key = \"your-datadog-key\"\n",
    "sendgrid_key = \"your-sendgrid-key\"\n",
    "\n",
    "[auth]\n",
    "jwt_secret = \"your-jwt-secret\"\n",
    "session_timeout = 3600\n",
    "\n",
    "[monitoring]\n",
    "sentry_dsn = \"your-sentry-dsn\"\n",
    "log_level = \"INFO\"\n",
    "''',\n",
    "    \n",
    "    '.env.example': '''\n",
    "# Environment variables for local development\n",
    "STREAMLIT_ENV=development\n",
    "DEBUG=true\n",
    "APP_VERSION=v2.1.0\n",
    "\n",
    "# Database\n",
    "DATABASE_URL=your-supabase-url\n",
    "DATABASE_API_KEY=your-api-key\n",
    "DB_POOL_SIZE=5\n",
    "\n",
    "# External APIs\n",
    "GOOGLE_ANALYTICS_KEY=your-ga-key\n",
    "DATADOG_API_KEY=your-datadog-key\n",
    "SENDGRID_API_KEY=your-sendgrid-key\n",
    "\n",
    "# Monitoring\n",
    "SENTRY_DSN=your-sentry-dsn\n",
    "LOG_LEVEL=DEBUG\n",
    "''',\n",
    "    \n",
    "    'Dockerfile': '''\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    build-essential \\\\\n",
    "    curl \\\\\n",
    "    software-properties-common \\\\\n",
    "    git \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements first for better caching\n",
    "COPY requirements.txt .\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd -m -u 1000 streamlit && chown -R streamlit:streamlit /app\n",
    "USER streamlit\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8501\n",
    "\n",
    "# Run application\n",
    "ENTRYPOINT [\"streamlit\", \"run\", \"deployment_ready_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n",
    "''',\n",
    "    \n",
    "    'docker-compose.yml': '''\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  streamlit-app:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8501:8501\"\n",
    "    environment:\n",
    "      - STREAMLIT_ENV=production\n",
    "      - DEBUG=false\n",
    "    volumes:\n",
    "      - ./logs:/app/logs\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8501/_stcore/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 40s\n",
    "    \n",
    "  nginx:\n",
    "    image: nginx:alpine\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "      - \"443:443\"\n",
    "    volumes:\n",
    "      - ./nginx.conf:/etc/nginx/nginx.conf\n",
    "      - ./ssl:/etc/nginx/ssl\n",
    "    depends_on:\n",
    "      - streamlit-app\n",
    "    restart: unless-stopped\n",
    "''',\n",
    "    \n",
    "    '.github/workflows/deploy.yml': '''\n",
    "name: Deploy to Production\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    \n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.9'\n",
    "    \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install -r requirements.txt\n",
    "        pip install pytest flake8 black\n",
    "    \n",
    "    - name: Lint with flake8\n",
    "      run: |\n",
    "        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n",
    "    \n",
    "    - name: Format check with black\n",
    "      run: |\n",
    "        black --check .\n",
    "    \n",
    "    - name: Test with pytest\n",
    "      run: |\n",
    "        pytest tests/ -v\n",
    "  \n",
    "  deploy:\n",
    "    needs: test\n",
    "    runs-on: ubuntu-latest\n",
    "    if: github.ref == 'refs/heads/main'\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    \n",
    "    - name: Deploy to Streamlit Cloud\n",
    "      run: |\n",
    "        echo \"Deploying to Streamlit Cloud...\"\n",
    "        # Streamlit Cloud automatically deploys on push to main\n",
    "        echo \"Deployment triggered successfully\"\n",
    "    \n",
    "    - name: Notify deployment status\n",
    "      run: |\n",
    "        echo \"Sending deployment notification...\"\n",
    "        # Add notification logic here\n",
    "'''\n",
    "}\n",
    "\n",
    "print(\"📁 Production Configuration Files:\")\n",
    "print()\n",
    "for filename, content in project_files.items():\n",
    "    print(f\"📄 {filename}\")\n",
    "    print(f\"   {len(content.splitlines())} lines\")\n",
    "    print()\n",
    "\n",
    "# Display key configuration explanations\n",
    "st.subheader(\"📋 Production Configuration Guide\")\n",
    "\n",
    "config_tabs = st.tabs([\"Requirements\", \"Streamlit Config\", \"Secrets\", \"Docker\", \"CI/CD\"])\n",
    "\n",
    "with config_tabs[0]:\n",
    "    st.markdown(\"\"\"\n",
    "    ### 📦 requirements.txt\n",
    "    \n",
    "    **Purpose**: Define all Python dependencies for consistent deployments\n",
    "    \n",
    "    **Key Dependencies**:\n",
    "    - `streamlit>=1.28.0` - Core framework with latest features\n",
    "    - `supabase>=1.0.3` - Database integration\n",
    "    - `plotly>=5.15.0` - Interactive visualizations\n",
    "    - `psycopg2-binary>=2.9.5` - PostgreSQL adapter\n",
    "    \n",
    "    **Best Practices**:\n",
    "    - Pin major versions to avoid breaking changes\n",
    "    - Use `pip freeze > requirements.txt` after testing\n",
    "    - Separate dev dependencies if needed\n",
    "    \"\"\")\n",
    "\n",
    "with config_tabs[1]:\n",
    "    st.markdown(\"\"\"\n",
    "    ### ⚙️ .streamlit/config.toml\n",
    "    \n",
    "    **Purpose**: Configure Streamlit behavior for production\n",
    "    \n",
    "    **Key Settings**:\n",
    "    - `developmentMode = false` - Disable development features\n",
    "    - `enableXsrfProtection = true` - Enable security protection\n",
    "    - `maxUploadSize = 200` - Limit file uploads (MB)\n",
    "    - Custom theme colors for branding\n",
    "    \n",
    "    **Security Note**: Never disable XSRF protection in production\n",
    "    \"\"\")\n",
    "\n",
    "with config_tabs[2]:\n",
    "    st.markdown(\"\"\"\n",
    "    ### 🔐 .streamlit/secrets.toml\n",
    "    \n",
    "    **Purpose**: Store sensitive configuration securely\n",
    "    \n",
    "    **Never Include**:\n",
    "    - Database passwords in plain text\n",
    "    - API keys in source code\n",
    "    - JWT secrets or encryption keys\n",
    "    \n",
    "    **Production Setup**:\n",
    "    1. Add secrets in Streamlit Cloud dashboard\n",
    "    2. Use environment variables for local development\n",
    "    3. Never commit secrets.toml to version control\n",
    "    \n",
    "    **Access in Code**:\n",
    "    ```python\n",
    "    db_url = st.secrets[\"database\"][\"url\"]\n",
    "    api_key = st.secrets[\"apis\"][\"sendgrid_key\"]\n",
    "    ```\n",
    "    \"\"\")\n",
    "\n",
    "with config_tabs[3]:\n",
    "    st.markdown(\"\"\"\n",
    "    ### 🐳 Docker Configuration\n",
    "    \n",
    "    **Dockerfile Benefits**:\n",
    "    - Consistent deployment environment\n",
    "    - Easy local testing of production setup\n",
    "    - Container orchestration support\n",
    "    \n",
    "    **Security Features**:\n",
    "    - Non-root user execution\n",
    "    - Health check endpoint\n",
    "    - Minimal base image\n",
    "    \n",
    "    **Usage**:\n",
    "    ```bash\n",
    "    # Build image\n",
    "    docker build -t olist-bi .\n",
    "    \n",
    "    # Run container\n",
    "    docker run -p 8501:8501 olist-bi\n",
    "    \n",
    "    # Use docker-compose for full stack\n",
    "    docker-compose up -d\n",
    "    ```\n",
    "    \"\"\")\n",
    "\n",
    "with config_tabs[4]:\n",
    "    st.markdown(\"\"\"\n",
    "    ### 🔄 CI/CD Pipeline\n",
    "    \n",
    "    **GitHub Actions Workflow**:\n",
    "    1. **Test Stage**: Run linting, formatting, and unit tests\n",
    "    2. **Deploy Stage**: Automatic deployment on main branch\n",
    "    3. **Notification**: Alert team of deployment status\n",
    "    \n",
    "    **Quality Gates**:\n",
    "    - Code formatting with Black\n",
    "    - Linting with Flake8\n",
    "    - Unit tests with pytest\n",
    "    - Security scanning (optional)\n",
    "    \n",
    "    **Deployment Triggers**:\n",
    "    - Push to main branch\n",
    "    - Manual workflow dispatch\n",
    "    - Scheduled deployments\n",
    "    \"\"\")\n",
    "\n",
    "print(\"✅ Production configuration files created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Section 3: Monitoring and Optimization (5 minutes)\n",
    "\n",
    "Let's implement comprehensive monitoring and performance optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile monitoring_optimization_guide.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "st.title(\"🔍 Production Monitoring & Optimization Guide\")\n",
    "st.markdown(\"**Enterprise-grade monitoring and performance optimization strategies**\")\n",
    "\n",
    "# Performance monitoring class\n",
    "class ProductionMonitor:\n",
    "    \"\"\"\n",
    "    Comprehensive production monitoring system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        if 'monitoring_data' not in st.session_state:\n",
    "            st.session_state.monitoring_data = {\n",
    "                'response_times': [],\n",
    "                'memory_usage': [],\n",
    "                'cpu_usage': [],\n",
    "                'user_sessions': [],\n",
    "                'error_logs': [],\n",
    "                'cache_hits': 0,\n",
    "                'cache_misses': 0\n",
    "            }\n",
    "    \n",
    "    def log_performance_metric(self, metric_type: str, value: float, metadata: Dict = None):\n",
    "        \"\"\"\n",
    "        Log performance metrics with timestamp.\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        metric_entry = {\n",
    "            'timestamp': timestamp,\n",
    "            'value': value,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        if metric_type in st.session_state.monitoring_data:\n",
    "            st.session_state.monitoring_data[metric_type].append(metric_entry)\n",
    "            \n",
    "            # Keep only last 1000 entries to prevent memory bloat\n",
    "            if len(st.session_state.monitoring_data[metric_type]) > 1000:\n",
    "                st.session_state.monitoring_data[metric_type] = \\\n",
    "                    st.session_state.monitoring_data[metric_type][-1000:]\n",
    "    \n",
    "    def get_system_metrics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get current system performance metrics.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get system metrics\n",
    "            cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "            memory = psutil.virtual_memory()\n",
    "            disk = psutil.disk_usage('/')\n",
    "            \n",
    "            return {\n",
    "                'cpu_percent': cpu_percent,\n",
    "                'memory_percent': memory.percent,\n",
    "                'memory_available_gb': memory.available / (1024**3),\n",
    "                'disk_percent': disk.percent,\n",
    "                'disk_free_gb': disk.free / (1024**3),\n",
    "                'timestamp': datetime.now()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Fallback for environments where psutil might not work\n",
    "            return {\n",
    "                'cpu_percent': 45.2,  # Simulated values\n",
    "                'memory_percent': 62.8,\n",
    "                'memory_available_gb': 2.1,\n",
    "                'disk_percent': 78.5,\n",
    "                'disk_free_gb': 15.7,\n",
    "                'timestamp': datetime.now()\n",
    "            }\n",
    "    \n",
    "    def track_cache_performance(self, hit: bool = True):\n",
    "        \"\"\"\n",
    "        Track cache hit/miss rates.\n",
    "        \"\"\"\n",
    "        if hit:\n",
    "            st.session_state.monitoring_data['cache_hits'] += 1\n",
    "        else:\n",
    "            st.session_state.monitoring_data['cache_misses'] += 1\n",
    "    \n",
    "    def get_cache_metrics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get cache performance metrics.\n",
    "        \"\"\"\n",
    "        hits = st.session_state.monitoring_data['cache_hits']\n",
    "        misses = st.session_state.monitoring_data['cache_misses']\n",
    "        total = hits + misses\n",
    "        \n",
    "        hit_rate = (hits / total * 100) if total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'hit_rate': hit_rate,\n",
    "            'total_requests': total,\n",
    "            'cache_hits': hits,\n",
    "            'cache_misses': misses\n",
    "        }\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = ProductionMonitor()\n",
    "\n",
    "# Real-time monitoring dashboard\n",
    "st.subheader(\"📊 Real-time System Monitoring\")\n",
    "\n",
    "# Get current system metrics\n",
    "system_metrics = monitor.get_system_metrics()\n",
    "cache_metrics = monitor.get_cache_metrics()\n",
    "\n",
    "# System metrics display\n",
    "sys_col1, sys_col2, sys_col3, sys_col4 = st.columns(4)\n",
    "\n",
    "with sys_col1:\n",
    "    cpu_color = \"🟢\" if system_metrics['cpu_percent'] < 70 else \"🟡\" if system_metrics['cpu_percent'] < 90 else \"🔴\"\n",
    "    st.metric(\n",
    "        f\"{cpu_color} CPU Usage\",\n",
    "        f\"{system_metrics['cpu_percent']:.1f}%\",\n",
    "        \"Current\"\n",
    "    )\n",
    "\n",
    "with sys_col2:\n",
    "    mem_color = \"🟢\" if system_metrics['memory_percent'] < 70 else \"🟡\" if system_metrics['memory_percent'] < 85 else \"🔴\"\n",
    "    st.metric(\n",
    "        f\"{mem_color} Memory Usage\",\n",
    "        f\"{system_metrics['memory_percent']:.1f}%\",\n",
    "        f\"{system_metrics['memory_available_gb']:.1f}GB free\"\n",
    "    )\n",
    "\n",
    "with sys_col3:\n",
    "    disk_color = \"🟢\" if system_metrics['disk_percent'] < 80 else \"🟡\" if system_metrics['disk_percent'] < 90 else \"🔴\"\n",
    "    st.metric(\n",
    "        f\"{disk_color} Disk Usage\",\n",
    "        f\"{system_metrics['disk_percent']:.1f}%\",\n",
    "        f\"{system_metrics['disk_free_gb']:.1f}GB free\"\n",
    "    )\n",
    "\n",
    "with sys_col4:\n",
    "    cache_color = \"🟢\" if cache_metrics['hit_rate'] > 80 else \"🟡\" if cache_metrics['hit_rate'] > 60 else \"🔴\"\n",
    "    st.metric(\n",
    "        f\"{cache_color} Cache Hit Rate\",\n",
    "        f\"{cache_metrics['hit_rate']:.1f}%\",\n",
    "        f\"{cache_metrics['total_requests']} requests\"\n",
    "    )\n",
    "\n",
    "# Performance optimization guides\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"⚡ Performance Optimization Strategies\")\n",
    "\n",
    "opt_tabs = st.tabs([\"🚀 Caching\", \"💾 Memory\", \"📊 Database\", \"🔍 Monitoring\", \"🛡️ Security\"])\n",
    "\n",
    "with opt_tabs[0]:\n",
    "    st.markdown(\"\"\"\n",
    "    ### 🚀 Advanced Caching Strategies\n",
    "    \n",
    "    **1. Multi-level Caching**\n",
    "    ```python\n",
    "    # Application-level cache\n",
    "    @st.cache_data(ttl=300)  # 5 minutes\n",
    "    def load_dashboard_data():\n",
    "        return expensive_query()\n",
    "    \n",
    "    # Resource-level cache\n",
    "    @st.cache_resource\n",
    "    def get_database_connection():\n",
    "        return create_connection_pool()\n",
    "    \n",
    "    # Custom cache with invalidation\n",
    "    @st.cache_data(ttl=600)\n",
    "    def load_user_specific_data(user_id: str):\n",
    "        return query_user_data(user_id)\n",
    "    ```\n",
    "    \n",
    "    **2. Cache Warming**\n",
    "    ```python\n",
    "    # Pre-load critical data\n",
    "    def warm_cache():\n",
    "        load_dashboard_data()\n",
    "        load_user_specific_data('default')\n",
    "        logger.info(\"Cache warmed successfully\")\n",
    "    \n",
    "    # Run on app startup\n",
    "    if 'cache_warmed' not in st.session_state:\n",
    "        warm_cache()\n",
    "        st.session_state.cache_warmed = True\n",
    "    ```\n",
    "    \n",
    "    **3. Smart Cache Invalidation**\n",
    "    ```python\n",
    "    def invalidate_user_cache(user_id: str):\n",
    "        # Clear specific user cache\n",
    "        st.cache_data.clear()\n",
    "        logger.info(f\"Cache invalidated for user: {user_id}\")\n",
    "    ```\n",
    "    \"\"\")\n",
    "\n",
    "with opt_tabs[1]:\n",
    "    st.markdown(\"\"\"\n",
    "    ### 💾 Memory Management\n",
    "    \n",
    "    **1. DataFrame Optimization**\n",
    "    ```python\n",
    "    # Use appropriate data types\n",
    "    def optimize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Convert object columns to category for repeated strings\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            if df[col].nunique() / len(df) < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "        \n",
    "        # Downcast numeric types\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        \n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        \n",
    "        return df\n",
    "    ```\n",
    "    \n",
    "    **2. Memory Monitoring**\n",
    "    ```python\n",
    "    import tracemalloc\n",
    "    \n",
    "    def monitor_memory_usage():\n",
    "        tracemalloc.start()\n",
    "        \n",
    "        # Your code here\n",
    "        \n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        logger.info(f\"Memory usage: {current / 1024 / 1024:.1f} MB\")\n",
    "        logger.info(f\"Peak memory: {peak / 1024 / 1024:.1f} MB\")\n",
    "        \n",
    "        tracemalloc.stop()\n",
    "    ```\n",
    "    \n",
    "    **3. Garbage Collection**\n",
    "    ```python\n",
    "    import gc\n",
    "    \n",
    "    def cleanup_memory():\n",
    "        # Force garbage collection\n",
    "        collected = gc.collect()\n",
    "        logger.info(f\"Garbage collected: {collected} objects\")\n",
    "    \n",
    "    # Run cleanup periodically\n",
    "    if st.session_state.get('request_count', 0) % 100 == 0:\n",
    "        cleanup_memory()\n",
    "    ```\n",
    "    \"\"\")\n",
    "\n",
    "with opt_tabs[2]:\n",
    "    st.markdown(\"\"\"\n",
    "    ### 📊 Database Optimization\n",
    "    \n",
    "    **1. Connection Pooling**\n",
    "    ```python\n",
    "    from sqlalchemy import create_engine\n",
    "    from sqlalchemy.pool import QueuePool\n",
    "    \n",
    "    @st.cache_resource\n",
    "    def get_database_engine():\n",
    "        return create_engine(\n",
    "            DATABASE_URL,\n",
    "            poolclass=QueuePool,\n",
    "            pool_size=10,\n",
    "            max_overflow=20,\n",
    "            pool_pre_ping=True,\n",
    "            pool_recycle=3600\n",
    "        )\n",
    "    ```\n",
    "    \n",
    "    **2. Query Optimization**\n",
    "    ```python\n",
    "    # Use pagination for large datasets\n",
    "    def load_paginated_data(page: int = 1, page_size: int = 1000):\n",
    "        offset = (page - 1) * page_size\n",
    "        query = f\"\"\"\n",
    "        SELECT * FROM orders \n",
    "        ORDER BY order_date DESC \n",
    "        LIMIT {page_size} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, get_database_engine())\n",
    "    \n",
    "    # Use database-level aggregation\n",
    "    def get_daily_metrics(start_date: str, end_date: str):\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            DATE(order_date) as date,\n",
    "            COUNT(*) as orders,\n",
    "            SUM(total_amount) as revenue,\n",
    "            AVG(customer_satisfaction) as satisfaction\n",
    "        FROM orders \n",
    "        WHERE order_date BETWEEN '{start_date}' AND '{end_date}'\n",
    "        GROUP BY DATE(order_date)\n",
    "        ORDER BY date\n",
    "        \"\"\"\n",
    "        return pd.read_sql(query, get_database_engine())\n",
    "    ```\n",
    "    \n",
    "    **3. Index Optimization**\n",
    "    ```sql\n",
    "    -- Create indexes for common queries\n",
    "    CREATE INDEX idx_orders_date ON orders(order_date);\n",
    "    CREATE INDEX idx_orders_customer ON orders(customer_id);\n",
    "    CREATE INDEX idx_orders_status ON orders(status);\n",
    "    \n",
    "    -- Composite index for common filter combinations\n",
    "    CREATE INDEX idx_orders_date_status ON orders(order_date, status);\n",
    "    ```\n",
    "    \"\"\")\n",
    "\n",
    "with opt_tabs[3]:\n",
    "    st.markdown(\"\"\"\n",
    "    ### 🔍 Production Monitoring Setup\n",
    "    \n",
    "    **1. Application Performance Monitoring (APM)**\n",
    "    ```python\n",
    "    import sentry_sdk\n",
    "    from sentry_sdk.integrations.logging import LoggingIntegration\n",
    "    \n",
    "    # Initialize Sentry for error tracking\n",
    "    sentry_sdk.init(\n",
    "        dsn=\"your-sentry-dsn\",\n",
    "        integrations=[LoggingIntegration(level=logging.INFO)],\n",
    "        traces_sample_rate=0.1,\n",
    "        environment=\"production\"\n",
    "    )\n",
    "    ```\n",
    "    \n",
    "    **2. Custom Metrics**\n",
    "    ```python\n",
    "    # Track business metrics\n",
    "    def track_business_metric(metric_name: str, value: float, tags: Dict = None):\n",
    "        # Send to monitoring service (Datadog, New Relic, etc.)\n",
    "        logger.info(f\"Metric: {metric_name} = {value}\", extra={\n",
    "            'metric_name': metric_name,\n",
    "            'metric_value': value,\n",
    "            'tags': tags or {}\n",
    "        })\n",
    "    \n",
    "    # Usage\n",
    "    track_business_metric('dashboard.load_time', 1.23, {'page': 'revenue'})\n",
    "    track_business_metric('user.session_duration', 456.7, {'user_type': 'admin'})\n",
    "    ```\n",
    "    \n",
    "    **3. Health Checks**\n",
    "    ```python\n",
    "    def health_check_endpoint():\n",
    "        checks = {\n",
    "            'database': test_database_connection(),\n",
    "            'cache': test_cache_connection(),\n",
    "            'memory': get_memory_usage() < 0.9,\n",
    "            'disk': get_disk_usage() < 0.9\n",
    "        }\n",
    "        \n",
    "        all_healthy = all(checks.values())\n",
    "        \n",
    "        return {\n",
    "            'status': 'healthy' if all_healthy else 'unhealthy',\n",
    "            'checks': checks,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    ```\n",
    "    \"\"\")\n",
    "\n",
    "with opt_tabs[4]:\n",
    "    st.markdown(\"\"\"\n",
    "    ### 🛡️ Security and Compliance\n",
    "    \n",
    "    **1. Security Headers**\n",
    "    ```python\n",
    "    # Add security headers (if using custom server)\n",
    "    def add_security_headers():\n",
    "        headers = {\n",
    "            'X-Content-Type-Options': 'nosniff',\n",
    "            'X-Frame-Options': 'DENY',\n",
    "            'X-XSS-Protection': '1; mode=block',\n",
    "            'Strict-Transport-Security': 'max-age=31536000; includeSubDomains',\n",
    "            'Content-Security-Policy': \"default-src 'self'\"\n",
    "        }\n",
    "        return headers\n",
    "    ```\n",
    "    \n",
    "    **2. Audit Logging**\n",
    "    ```python\n",
    "    def audit_log(action: str, user_id: str, resource: str, details: Dict = None):\n",
    "        audit_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'action': action,\n",
    "            'user_id': user_id,\n",
    "            'resource': resource,\n",
    "            'details': details or {},\n",
    "            'ip_address': get_client_ip(),\n",
    "            'user_agent': get_user_agent()\n",
    "        }\n",
    "        \n",
    "        # Send to secure logging service\n",
    "        logger.info(\"AUDIT\", extra=audit_entry)\n",
    "    \n",
    "    # Usage\n",
    "    audit_log('data_export', user_id, 'customer_data', {\n",
    "        'export_format': 'csv',\n",
    "        'record_count': 1000\n",
    "    })\n",
    "    ```\n",
    "    \n",
    "    **3. Data Privacy**\n",
    "    ```python\n",
    "    def anonymize_sensitive_data(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Remove or anonymize sensitive data for display.\"\"\"\n",
    "        anonymized_df = df.copy()\n",
    "        \n",
    "        for col in columns:\n",
    "            if col in anonymized_df.columns:\n",
    "                # Hash sensitive data\n",
    "                anonymized_df[col] = anonymized_df[col].apply(\n",
    "                    lambda x: hashlib.sha256(str(x).encode()).hexdigest()[:8]\n",
    "                )\n",
    "        \n",
    "        return anonymized_df\n",
    "    ```\n",
    "    \"\"\")\n",
    "\n",
    "# Performance testing demo\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"🧪 Performance Testing Demo\")\n",
    "\n",
    "test_col1, test_col2 = st.columns(2)\n",
    "\n",
    "with test_col1:\n",
    "    st.markdown(\"**Load Test Simulation**\")\n",
    "    \n",
    "    if st.button(\"🚀 Run Load Test\"):\n",
    "        with st.spinner(\"Running load test...\"):\n",
    "            # Simulate load test\n",
    "            for i in range(5):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Simulate query\n",
    "                time.sleep(np.random.uniform(0.1, 0.5))\n",
    "                \n",
    "                response_time = time.time() - start_time\n",
    "                monitor.log_performance_metric('response_times', response_time)\n",
    "                \n",
    "                # Simulate cache hit/miss\n",
    "                cache_hit = np.random.choice([True, False], p=[0.8, 0.2])\n",
    "                monitor.track_cache_performance(cache_hit)\n",
    "        \n",
    "        st.success(\"✅ Load test completed!\")\n",
    "\n",
    "with test_col2:\n",
    "    st.markdown(\"**Memory Usage Test**\")\n",
    "    \n",
    "    if st.button(\"💾 Test Memory Usage\"):\n",
    "        with st.spinner(\"Testing memory usage...\"):\n",
    "            # Create large DataFrame to test memory\n",
    "            large_df = pd.DataFrame({\n",
    "                'col1': np.random.randn(100000),\n",
    "                'col2': np.random.randn(100000),\n",
    "                'col3': np.random.choice(['A', 'B', 'C'], 100000)\n",
    "            })\n",
    "            \n",
    "            # Optimize DataFrame\n",
    "            large_df['col3'] = large_df['col3'].astype('category')\n",
    "            \n",
    "            memory_usage = large_df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "            monitor.log_performance_metric('memory_usage', memory_usage)\n",
    "            \n",
    "            # Clean up\n",
    "            del large_df\n",
    "            gc.collect()\n",
    "        \n",
    "        st.success(f\"✅ Memory test completed! Peak usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "# Display monitoring results\n",
    "if st.session_state.monitoring_data['response_times']:\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"📈 Performance Metrics\")\n",
    "    \n",
    "    # Response time chart\n",
    "    response_times = [m['value'] for m in st.session_state.monitoring_data['response_times']]\n",
    "    timestamps = [m['timestamp'] for m in st.session_state.monitoring_data['response_times']]\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'response_time': response_times\n",
    "    })\n",
    "    \n",
    "    import plotly.express as px\n",
    "    \n",
    "    fig = px.line(\n",
    "        metrics_df,\n",
    "        x='timestamp',\n",
    "        y='response_time',\n",
    "        title=\"Response Time Trend\",\n",
    "        labels={'response_time': 'Response Time (seconds)'}\n",
    "    )\n",
    "    \n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "print(\"✅ Monitoring and optimization guide complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways\n",
    "\n",
    "✅ **Production Deployment**: Complete Streamlit Cloud deployment workflow with configuration  \n",
    "✅ **Environment Management**: Secure secrets handling and environment-specific configurations  \n",
    "✅ **Performance Optimization**: Caching strategies, memory management, and database optimization  \n",
    "✅ **Monitoring & Alerting**: Comprehensive application and system monitoring setup  \n",
    "✅ **CI/CD Pipeline**: Automated testing and deployment with GitHub Actions  \n",
    "\n",
    "## 🚀 Deployment Checklist\n",
    "\n",
    "Before deploying to production, ensure you have:\n",
    "\n",
    "### Pre-Deployment\n",
    "- [ ] Created `requirements.txt` with pinned versions\n",
    "- [ ] Configured `.streamlit/config.toml` for production\n",
    "- [ ] Set up secrets management in Streamlit Cloud\n",
    "- [ ] Implemented error handling and logging\n",
    "- [ ] Added performance monitoring\n",
    "- [ ] Created health check endpoints\n",
    "\n",
    "### Security\n",
    "- [ ] Enabled XSRF protection\n",
    "- [ ] Implemented input validation\n",
    "- [ ] Set up audit logging\n",
    "- [ ] Configured secure database connections\n",
    "- [ ] Added rate limiting (if needed)\n",
    "\n",
    "### Performance\n",
    "- [ ] Implemented caching strategies\n",
    "- [ ] Optimized database queries\n",
    "- [ ] Added memory management\n",
    "- [ ] Set up connection pooling\n",
    "- [ ] Configured auto-scaling (if applicable)\n",
    "\n",
    "### Monitoring\n",
    "- [ ] Set up error tracking (Sentry)\n",
    "- [ ] Configured performance monitoring\n",
    "- [ ] Added business metrics tracking\n",
    "- [ ] Set up alerting for critical issues\n",
    "- [ ] Created monitoring dashboards\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 Week 9 Complete!\n",
    "\n",
    "Congratulations! You've now mastered:\n",
    "\n",
    "**Wednesday - Fundamentals:**\n",
    "- Streamlit setup and architecture\n",
    "- Interactive widgets and user interfaces\n",
    "- Data visualization with Plotly integration\n",
    "\n",
    "**Thursday - Advanced:**\n",
    "- Multi-page business dashboard architecture\n",
    "- Advanced database integration with Supabase\n",
    "- Production deployment and monitoring\n",
    "\n",
    "## 🔜 Next Steps: Month 4 Capstone Project\n",
    "\n",
    "You're now ready to build professional business intelligence applications for your capstone project:\n",
    "\n",
    "- **Project Scope**: 6-week comprehensive BI application\n",
    "- **Technology Stack**: Streamlit + Supabase + Plotly\n",
    "- **Deployment**: Production-ready application on Streamlit Cloud\n",
    "- **Business Focus**: Real-world e-commerce analytics with Olist data\n",
    "\n",
    "---\n",
    "\n",
    "## 💼 Final Assignment: Major Group Project\n",
    "\n",
    "**Create a production-ready business intelligence application:**\n",
    "\n",
    "### Requirements:\n",
    "1. **Multi-page Application**: 4-5 pages serving different stakeholder needs\n",
    "2. **Live Data Integration**: Real-time Supabase connectivity\n",
    "3. **Professional Design**: Enterprise-grade UI/UX\n",
    "4. **Full Deployment**: Live application on Streamlit Cloud\n",
    "5. **Team Presentation**: 15-minute demo to class\n",
    "\n",
    "### Deliverables:\n",
    "- Complete Streamlit application source code\n",
    "- Production deployment with live URL\n",
    "- Technical documentation\n",
    "- Business presentation highlighting key insights\n",
    "\n",
    "### Due: End of Week 10\n",
    "\n",
    "**This project will serve as your portfolio piece for Month 4 capstone preparation!**\n",
    "\n",
    "---\n",
    "\n",
    "*Congratulations on completing Week 9! You're now ready to build enterprise-grade business intelligence applications.* 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}