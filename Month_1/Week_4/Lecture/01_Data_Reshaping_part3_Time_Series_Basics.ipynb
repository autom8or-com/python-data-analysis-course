{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Reshaping - Part 3: Time Series Manipulation Basics\n",
        "\n",
        "## Week 4, Day 1 (Wednesday) - April 30th, 2025\n",
        "\n",
        "### Overview\n",
        "This is the final part of our Data Reshaping session, focusing on basic time series manipulation. Understanding how to work with dates and times is crucial for e-commerce analysis, where you often need to analyze trends, seasonality, and time-based patterns in sales, customer behavior, and business metrics.\n",
        "\n",
        "### Learning Objectives\n",
        "- Understand pandas datetime data types and capabilities\n",
        "- Master date parsing and datetime index creation\n",
        "- Learn basic time series operations (filtering, resampling, shifting)\n",
        "- Apply time series techniques to e-commerce scenarios\n",
        "- Handle common time zone and date format issues\n",
        "- Prepare data for time-based analysis and visualization\n",
        "\n",
        "### Prerequisites\n",
        "- Completed Part 1: Merge, Join, and Concatenate\n",
        "- Completed Part 2: Melt and Pivot Operations\n",
        "- Understanding of Pandas DataFrames and indexing\n",
        "- Basic knowledge of date/time concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Time Series Data\n",
        "\n",
        "### What is Time Series Data?\n",
        "Time series data consists of observations recorded at different points in time. In e-commerce, examples include:\n",
        "- Daily sales figures\n",
        "- Hourly website traffic\n",
        "- Monthly customer acquisition\n",
        "- Quarterly revenue reports\n",
        "\n",
        "### Why Time Series Manipulation Matters\n",
        "- **Trend Analysis**: Identify growth or decline patterns\n",
        "- **Seasonality**: Understand cyclical patterns (holidays, weekends)\n",
        "- **Forecasting**: Predict future performance\n",
        "- **Reporting**: Create time-based dashboards and reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta, date\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Pandas Datetime Data Types\n",
        "\n",
        "### Key Datetime Objects in Pandas\n",
        "- **Timestamp**: A single point in time\n",
        "- **DatetimeIndex**: An array of timestamps (used as DataFrame index)\n",
        "- **Period**: A fixed time span (e.g., \"January 2025\")\n",
        "- **Timedelta**: Duration between two points in time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating different datetime objects\n",
        "\n",
        "# 1. Timestamp - single point in time\n",
        "timestamp = pd.Timestamp('2025-01-15 14:30:00')\n",
        "print(f\"Timestamp: {timestamp}\")\n",
        "print(f\"Type: {type(timestamp)}\")\n",
        "\n",
        "# 2. DatetimeIndex - array of timestamps\n",
        "date_index = pd.date_range('2025-01-01', periods=5, freq='D')\n",
        "print(f\"\\nDatetimeIndex: {date_index}\")\n",
        "print(f\"Type: {type(date_index)}\")\n",
        "\n",
        "# 3. Period - fixed time span\n",
        "period = pd.Period('2025-01', freq='M')\n",
        "print(f\"\\nPeriod: {period}\")\n",
        "print(f\"Type: {type(period)}\")\n",
        "\n",
        "# 4. Timedelta - duration\n",
        "timedelta = pd.Timedelta(days=30, hours=5, minutes=30)\n",
        "print(f\"\\nTimedelta: {timedelta}\")\n",
        "print(f\"Type: {type(timedelta)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creating and Parsing Dates\n",
        "\n",
        "### Converting Strings to Datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Different ways to create datetime objects\n",
        "\n",
        "# From string (automatic parsing)\n",
        "date_str = '2025-01-15'\n",
        "parsed_date = pd.to_datetime(date_str)\n",
        "print(f\"Parsed date: {parsed_date}\")\n",
        "\n",
        "# From multiple string formats\n",
        "date_strings = ['2025-01-15', '01/16/2025', '2025-01-17 14:30:00', 'Jan 18, 2025']\n",
        "parsed_dates = pd.to_datetime(date_strings)\n",
        "print(f\"\\nMultiple formats: {parsed_dates}\")\n",
        "\n",
        "# From components\n",
        "component_dates = pd.to_datetime({\n",
        "    'year': [2025, 2025, 2025],\n",
        "    'month': [1, 2, 3],\n",
        "    'day': [15, 16, 17]\n",
        "})\n",
        "print(f\"\\nFrom components: {component_dates}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling Different Date Formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# E-commerce data often comes in various formats\n",
        "messy_dates = [\n",
        "    '15-Jan-2025',\n",
        "    '2025/01/16',\n",
        "    '17.01.2025',\n",
        "    '18-1-25'\n",
        "]\n",
        "\n",
        "# Pandas can handle most formats automatically\n",
        "clean_dates = pd.to_datetime(messy_dates)\n",
        "print(\"Cleaned dates:\")\n",
        "for original, cleaned in zip(messy_dates, clean_dates):\n",
        "    print(f\"{original:12} -> {cleaned}\")\n",
        "\n",
        "# For problematic formats, specify the format\n",
        "specific_format = pd.to_datetime('18-1-25', format='%d-%m-%y')\n",
        "print(f\"\\nSpecific format: '18-1-25' -> {specific_format}\")\n",
        "\n",
        "# Handle errors gracefully\n",
        "problematic_dates = ['2025-01-15', 'not-a-date', '2025-02-30']\n",
        "safe_dates = pd.to_datetime(problematic_dates, errors='coerce')\n",
        "print(f\"\\nWith errors: {safe_dates}\")\n",
        "print(\"Note: Invalid dates become NaT (Not a Time)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Creating Sample E-commerce Time Series Data\n",
        "\n",
        "Let's create realistic e-commerce datasets with time components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create daily sales data for a month\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate date range\n",
        "date_range = pd.date_range('2025-01-01', '2025-01-31', freq='D')\n",
        "\n",
        "# Create sales data with some realistic patterns\n",
        "base_sales = 1000\n",
        "weekend_boost = 200  # Higher sales on weekends\n",
        "random_variation = np.random.normal(0, 100, len(date_range))\n",
        "\n",
        "daily_sales = []\n",
        "for i, date in enumerate(date_range):\n",
        "    # Base sales\n",
        "    sales = base_sales\n",
        "    \n",
        "    # Weekend boost (Saturday=5, Sunday=6)\n",
        "    if date.weekday() >= 5:\n",
        "        sales += weekend_boost\n",
        "    \n",
        "    # Random variation\n",
        "    sales += random_variation[i]\n",
        "    \n",
        "    # Ensure non-negative\n",
        "    sales = max(sales, 100)\n",
        "    \n",
        "    daily_sales.append(round(sales, 2))\n",
        "\n",
        "# Create DataFrame\n",
        "sales_data = pd.DataFrame({\n",
        "    'date': date_range,\n",
        "    'sales_amount': daily_sales,\n",
        "    'day_of_week': date_range.day_name(),\n",
        "    'is_weekend': date_range.weekday >= 5\n",
        "})\n",
        "\n",
        "print(\"Daily Sales Data:\")\n",
        "print(sales_data.head(10))\n",
        "print(f\"\\nData shape: {sales_data.shape}\")\n",
        "print(f\"Date range: {sales_data['date'].min()} to {sales_data['date'].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create hourly order data for a few days\n",
        "hourly_range = pd.date_range('2025-01-15', '2025-01-17 23:00:00', freq='H')\n",
        "\n",
        "# Simulate hourly order patterns (more orders during business hours)\n",
        "hourly_orders = []\n",
        "for dt in hourly_range:\n",
        "    hour = dt.hour\n",
        "    base_orders = 5\n",
        "    \n",
        "    # Business hours boost (9 AM - 6 PM)\n",
        "    if 9 <= hour <= 18:\n",
        "        base_orders += 10\n",
        "    \n",
        "    # Lunch time peak (12 PM - 2 PM)\n",
        "    if 12 <= hour <= 14:\n",
        "        base_orders += 5\n",
        "    \n",
        "    # Add some randomness\n",
        "    final_orders = base_orders + np.random.poisson(3)\n",
        "    hourly_orders.append(final_orders)\n",
        "\n",
        "hourly_data = pd.DataFrame({\n",
        "    'datetime': hourly_range,\n",
        "    'order_count': hourly_orders\n",
        "})\n",
        "\n",
        "print(\"Hourly Order Data (sample):\")\n",
        "print(hourly_data.head(12))\n",
        "print(f\"\\nTotal hours: {len(hourly_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Setting DateTime as Index\n",
        "\n",
        "### Why Use DateTime Index?\n",
        "- Enables powerful time-based operations\n",
        "- Simplifies filtering and slicing by dates\n",
        "- Required for many time series functions\n",
        "- Improves performance for time-based queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set datetime as index\n",
        "sales_indexed = sales_data.set_index('date')\n",
        "print(\"Sales data with datetime index:\")\n",
        "print(sales_indexed.head())\n",
        "print(f\"\\nIndex type: {type(sales_indexed.index)}\")\n",
        "print(f\"Index name: {sales_indexed.index.name}\")\n",
        "\n",
        "# Create directly with datetime index\n",
        "hourly_indexed = hourly_data.set_index('datetime')\n",
        "print(\"\\nHourly data with datetime index:\")\n",
        "print(hourly_indexed.head(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Time-Based Indexing and Filtering\n",
        "\n",
        "### Selecting Data by Date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select specific date\n",
        "specific_date = sales_indexed.loc['2025-01-15']\n",
        "print(\"Sales on 2025-01-15:\")\n",
        "print(specific_date)\n",
        "\n",
        "# Select date range\n",
        "date_range_data = sales_indexed.loc['2025-01-10':'2025-01-15']\n",
        "print(\"\\nSales from Jan 10-15:\")\n",
        "print(date_range_data)\n",
        "\n",
        "# Select by month\n",
        "january_data = sales_indexed.loc['2025-01']\n",
        "print(f\"\\nJanuary data shape: {january_data.shape}\")\n",
        "print(\"First few rows:\")\n",
        "print(january_data.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Boolean Indexing with Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter by date conditions\n",
        "recent_sales = sales_indexed[sales_indexed.index >= '2025-01-20']\n",
        "print(\"Sales from Jan 20 onwards:\")\n",
        "print(recent_sales.head())\n",
        "\n",
        "# Filter weekends\n",
        "weekend_sales = sales_indexed[sales_indexed['is_weekend'] == True]\n",
        "print(f\"\\nWeekend sales count: {len(weekend_sales)}\")\n",
        "print(\"Weekend sales sample:\")\n",
        "print(weekend_sales.head())\n",
        "\n",
        "# Complex date filtering\n",
        "mid_month_weekdays = sales_indexed[\n",
        "    (sales_indexed.index >= '2025-01-10') & \n",
        "    (sales_indexed.index <= '2025-01-20') & \n",
        "    (sales_indexed['is_weekend'] == False)\n",
        "]\n",
        "print(f\"\\nMid-month weekdays count: {len(mid_month_weekdays)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Time Series Operations\n",
        "\n",
        "### Extracting Date Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract various date components\n",
        "sales_components = sales_indexed.copy()\n",
        "\n",
        "# Extract date parts\n",
        "sales_components['year'] = sales_components.index.year\n",
        "sales_components['month'] = sales_components.index.month\n",
        "sales_components['day'] = sales_components.index.day\n",
        "sales_components['weekday'] = sales_components.index.weekday  # 0=Monday, 6=Sunday\n",
        "sales_components['week_of_year'] = sales_components.index.isocalendar().week\n",
        "sales_components['quarter'] = sales_components.index.quarter\n",
        "\n",
        "print(\"Sales data with extracted date components:\")\n",
        "print(sales_components[['sales_amount', 'year', 'month', 'day', 'weekday', 'quarter']].head(10))\n",
        "\n",
        "# Summary by day of week\n",
        "weekday_summary = sales_components.groupby('weekday')['sales_amount'].agg(['mean', 'count'])\n",
        "weekday_summary.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "print(\"\\nSales by day of week:\")\n",
        "print(weekday_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shifting and Lagging Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create lagged variables for comparison\n",
        "sales_with_lags = sales_indexed[['sales_amount']].copy()\n",
        "\n",
        "# Previous day sales\n",
        "sales_with_lags['prev_day_sales'] = sales_with_lags['sales_amount'].shift(1)\n",
        "\n",
        "# Sales from a week ago\n",
        "sales_with_lags['week_ago_sales'] = sales_with_lags['sales_amount'].shift(7)\n",
        "\n",
        "# Calculate day-over-day change\n",
        "sales_with_lags['daily_change'] = sales_with_lags['sales_amount'] - sales_with_lags['prev_day_sales']\n",
        "sales_with_lags['daily_change_pct'] = (sales_with_lags['daily_change'] / sales_with_lags['prev_day_sales'] * 100).round(2)\n",
        "\n",
        "# Week-over-week change\n",
        "sales_with_lags['weekly_change'] = sales_with_lags['sales_amount'] - sales_with_lags['week_ago_sales']\n",
        "\n",
        "print(\"Sales with lagged variables:\")\n",
        "print(sales_with_lags.head(10))\n",
        "\n",
        "# Show some interesting changes\n",
        "print(\"\\nLargest daily increases:\")\n",
        "print(sales_with_lags.nlargest(3, 'daily_change')[['sales_amount', 'prev_day_sales', 'daily_change', 'daily_change_pct']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Resampling Time Series Data\n",
        "\n",
        "### What is Resampling?\n",
        "Resampling changes the frequency of your time series data:\n",
        "- **Downsampling**: Higher to lower frequency (daily to weekly)\n",
        "- **Upsampling**: Lower to higher frequency (monthly to daily)\n",
        "\n",
        "### Downsampling Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resample daily data to weekly\n",
        "weekly_sales = sales_indexed['sales_amount'].resample('W').agg({\n",
        "    'total_sales': 'sum',\n",
        "    'avg_daily_sales': 'mean',\n",
        "    'max_daily_sales': 'max',\n",
        "    'days_count': 'count'\n",
        "})\n",
        "\n",
        "print(\"Weekly sales summary:\")\n",
        "print(weekly_sales)\n",
        "\n",
        "# Monthly summary\n",
        "monthly_sales = sales_indexed['sales_amount'].resample('M').agg({\n",
        "    'total_sales': 'sum',\n",
        "    'avg_daily_sales': 'mean',\n",
        "    'std_daily_sales': 'std'\n",
        "})\n",
        "\n",
        "print(\"\\nMonthly sales summary:\")\n",
        "print(monthly_sales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resample hourly data to daily\n",
        "daily_from_hourly = hourly_indexed['order_count'].resample('D').agg({\n",
        "    'total_orders': 'sum',\n",
        "    'avg_hourly_orders': 'mean',\n",
        "    'peak_hourly_orders': 'max',\n",
        "    'hours_with_data': 'count'\n",
        "})\n",
        "\n",
        "print(\"Daily summary from hourly data:\")\n",
        "print(daily_from_hourly)\n",
        "\n",
        "# Business hours analysis (9 AM - 6 PM)\n",
        "business_hours = hourly_indexed.between_time('09:00', '18:00')\n",
        "business_daily = business_hours['order_count'].resample('D').sum()\n",
        "\n",
        "print(\"\\nDaily orders during business hours:\")\n",
        "print(business_daily)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Common Resampling Frequencies\n",
        "- **'D'**: Daily\n",
        "- **'W'**: Weekly (Sunday to Saturday)\n",
        "- **'M'**: Monthly (end of month)\n",
        "- **'Q'**: Quarterly\n",
        "- **'H'**: Hourly\n",
        "- **'T' or 'min'**: Minute\n",
        "- **'S'**: Second"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Different resampling periods\n",
        "print(\"Different resampling frequencies:\")\n",
        "\n",
        "# Every 3 days\n",
        "three_day_sales = sales_indexed['sales_amount'].resample('3D').sum()\n",
        "print(f\"Every 3 days: {len(three_day_sales)} periods\")\n",
        "print(three_day_sales.head())\n",
        "\n",
        "# Bi-weekly (every 2 weeks)\n",
        "biweekly_sales = sales_indexed['sales_amount'].resample('2W').sum()\n",
        "print(f\"\\nBi-weekly: {len(biweekly_sales)} periods\")\n",
        "print(biweekly_sales)\n",
        "\n",
        "# Custom business day frequency (Monday to Friday)\n",
        "business_day_avg = sales_indexed['sales_amount'].resample('B').mean()\n",
        "print(f\"\\nBusiness days average (first 10 days):\")\n",
        "print(business_day_avg.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Working with Time Zones\n",
        "\n",
        "### Understanding Time Zones in E-commerce\n",
        "- Global e-commerce operates across time zones\n",
        "- Server time vs customer local time\n",
        "- Important for accurate reporting and analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create timezone-aware data\n",
        "utc_times = pd.date_range('2025-01-15 12:00:00', periods=5, freq='H', tz='UTC')\n",
        "print(\"UTC times:\")\n",
        "print(utc_times)\n",
        "\n",
        "# Convert to different time zones\n",
        "eastern_times = utc_times.tz_convert('US/Eastern')\n",
        "pacific_times = utc_times.tz_convert('US/Pacific')\n",
        "london_times = utc_times.tz_convert('Europe/London')\n",
        "\n",
        "print(\"\\nSame moment in different time zones:\")\n",
        "for i in range(len(utc_times)):\n",
        "    print(f\"UTC: {utc_times[i]} | Eastern: {eastern_times[i]} | Pacific: {pacific_times[i]} | London: {london_times[i]}\")\n",
        "\n",
        "# Localize naive datetime to a specific timezone\n",
        "naive_datetime = pd.Timestamp('2025-01-15 15:30:00')\n",
        "eastern_localized = naive_datetime.tz_localize('US/Eastern')\n",
        "print(f\"\\nNaive: {naive_datetime}\")\n",
        "print(f\"Eastern localized: {eastern_localized}\")\n",
        "print(f\"Converted to UTC: {eastern_localized.tz_convert('UTC')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Real-World E-commerce Time Series Examples\n",
        "\n",
        "### Example 1: Customer Activity Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create customer activity data\n",
        "activity_dates = pd.date_range('2025-01-01', '2025-01-31', freq='D')\n",
        "np.random.seed(123)\n",
        "\n",
        "customer_activity = pd.DataFrame({\n",
        "    'date': activity_dates,\n",
        "    'new_customers': np.random.poisson(15, len(activity_dates)),\n",
        "    'returning_customers': np.random.poisson(45, len(activity_dates)),\n",
        "    'page_views': np.random.normal(10000, 2000, len(activity_dates)).astype(int),\n",
        "    'orders': np.random.poisson(85, len(activity_dates))\n",
        "})\n",
        "\n",
        "# Set date as index\n",
        "customer_activity = customer_activity.set_index('date')\n",
        "\n",
        "print(\"Daily Customer Activity:\")\n",
        "print(customer_activity.head(10))\n",
        "\n",
        "# Calculate conversion rates\n",
        "customer_activity['total_customers'] = customer_activity['new_customers'] + customer_activity['returning_customers']\n",
        "customer_activity['conversion_rate'] = (customer_activity['orders'] / customer_activity['total_customers'] * 100).round(2)\n",
        "customer_activity['pages_per_customer'] = (customer_activity['page_views'] / customer_activity['total_customers']).round(1)\n",
        "\n",
        "print(\"\\nWith calculated metrics:\")\n",
        "print(customer_activity[['total_customers', 'orders', 'conversion_rate', 'pages_per_customer']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Seasonal Sales Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Weekly performance analysis\n",
        "weekly_performance = customer_activity.resample('W').agg({\n",
        "    'new_customers': 'sum',\n",
        "    'returning_customers': 'sum',\n",
        "    'orders': 'sum',\n",
        "    'page_views': 'sum',\n",
        "    'conversion_rate': 'mean'\n",
        "})\n",
        "\n",
        "# Calculate weekly totals and rates\n",
        "weekly_performance['total_customers'] = weekly_performance['new_customers'] + weekly_performance['returning_customers']\n",
        "weekly_performance['weekly_conversion_rate'] = (weekly_performance['orders'] / weekly_performance['total_customers'] * 100).round(2)\n",
        "\n",
        "print(\"Weekly Performance Summary:\")\n",
        "print(weekly_performance)\n",
        "\n",
        "# Month-over-month comparison (if we had multiple months)\n",
        "monthly_summary = customer_activity.resample('M').agg({\n",
        "    'new_customers': 'sum',\n",
        "    'returning_customers': 'sum',\n",
        "    'orders': 'sum',\n",
        "    'page_views': 'sum'\n",
        "})\n",
        "\n",
        "print(\"\\nMonthly Summary:\")\n",
        "print(monthly_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Moving Averages for Trend Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate moving averages to smooth out daily fluctuations\n",
        "trend_analysis = sales_indexed[['sales_amount']].copy()\n",
        "\n",
        "# 3-day moving average\n",
        "trend_analysis['sales_3day_ma'] = trend_analysis['sales_amount'].rolling(window=3).mean()\n",
        "\n",
        "# 7-day moving average\n",
        "trend_analysis['sales_7day_ma'] = trend_analysis['sales_amount'].rolling(window=7).mean()\n",
        "\n",
        "# Exponential moving average (gives more weight to recent values)\n",
        "trend_analysis['sales_ema'] = trend_analysis['sales_amount'].ewm(span=7).mean()\n",
        "\n",
        "print(\"Trend Analysis with Moving Averages:\")\n",
        "print(trend_analysis.head(10))\n",
        "\n",
        "# Show the smoothing effect\n",
        "print(\"\\nComparison of actual vs smoothed values (last 5 days):\")\n",
        "comparison = trend_analysis[['sales_amount', 'sales_7day_ma', 'sales_ema']].tail()\n",
        "print(comparison.round(2))\n",
        "\n",
        "# Calculate trend direction\n",
        "trend_analysis['trend_direction'] = np.where(\n",
        "    trend_analysis['sales_7day_ma'] > trend_analysis['sales_7day_ma'].shift(1), \n",
        "    'Up', 'Down'\n",
        ")\n",
        "\n",
        "print(\"\\nTrend directions (last 10 days):\")\n",
        "print(trend_analysis[['sales_amount', 'sales_7day_ma', 'trend_direction']].tail(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Common Time Series Challenges and Solutions\n",
        "\n",
        "### Challenge 1: Missing Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data with missing dates\n",
        "incomplete_dates = pd.date_range('2025-01-01', '2025-01-10', freq='D')\n",
        "# Remove some dates to simulate missing data\n",
        "incomplete_dates = incomplete_dates.delete([2, 5, 7])  # Remove 3rd, 6th, and 8th days\n",
        "\n",
        "incomplete_sales = pd.DataFrame({\n",
        "    'date': incomplete_dates,\n",
        "    'sales': [1000, 1100, 950, 1200, 1050, 1150, 1080]\n",
        "}).set_index('date')\n",
        "\n",
        "print(\"Data with missing dates:\")\n",
        "print(incomplete_sales)\n",
        "\n",
        "# Solution 1: Reindex to include all dates\n",
        "full_date_range = pd.date_range('2025-01-01', '2025-01-10', freq='D')\n",
        "complete_sales = incomplete_sales.reindex(full_date_range)\n",
        "\n",
        "print(\"\\nAfter reindexing (with NaN for missing dates):\")\n",
        "print(complete_sales)\n",
        "\n",
        "# Solution 2: Fill missing values\n",
        "complete_sales_filled = complete_sales.fillna(method='forward')  # Forward fill\n",
        "print(\"\\nWith forward fill:\")\n",
        "print(complete_sales_filled)\n",
        "\n",
        "# Solution 3: Interpolate missing values\n",
        "complete_sales_interpolated = complete_sales.interpolate()\n",
        "print(\"\\nWith interpolation:\")\n",
        "print(complete_sales_interpolated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 2: Irregular Time Intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create irregular timestamp data (like real customer orders)\n",
        "irregular_orders = pd.DataFrame({\n",
        "    'timestamp': [\n",
        "        '2025-01-15 09:15:23',\n",
        "        '2025-01-15 09:47:12',\n",
        "        '2025-01-15 11:23:45',\n",
        "        '2025-01-15 14:56:33',\n",
        "        '2025-01-15 16:12:09',\n",
        "        '2025-01-15 18:34:21'\n",
        "    ],\n",
        "    'order_value': [45.99, 123.50, 67.25, 234.00, 89.99, 156.75]\n",
        "})\n",
        "\n",
        "irregular_orders['timestamp'] = pd.to_datetime(irregular_orders['timestamp'])\n",
        "irregular_orders = irregular_orders.set_index('timestamp')\n",
        "\n",
        "print(\"Irregular order timestamps:\")\n",
        "print(irregular_orders)\n",
        "\n",
        "# Solution: Resample to regular intervals\n",
        "hourly_totals = irregular_orders['order_value'].resample('H').sum()\n",
        "print(\"\\nResampled to hourly totals:\")\n",
        "print(hourly_totals[hourly_totals > 0])  # Show only hours with orders\n",
        "\n",
        "# Count orders per hour\n",
        "hourly_counts = irregular_orders['order_value'].resample('H').count()\n",
        "print(\"\\nOrders per hour:\")\n",
        "print(hourly_counts[hourly_counts > 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Preparing Data for Visualization and Analysis\n",
        "\n",
        "### Creating Analysis-Ready Time Series Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive time series dataset for analysis\n",
        "analysis_data = sales_indexed[['sales_amount']].copy()\n",
        "\n",
        "# Add time components\n",
        "analysis_data['year'] = analysis_data.index.year\n",
        "analysis_data['month'] = analysis_data.index.month\n",
        "analysis_data['day'] = analysis_data.index.day\n",
        "analysis_data['weekday'] = analysis_data.index.weekday\n",
        "analysis_data['is_weekend'] = analysis_data.index.weekday >= 5\n",
        "analysis_data['day_name'] = analysis_data.index.day_name()\n",
        "\n",
        "# Add lagged variables\n",
        "analysis_data['sales_lag1'] = analysis_data['sales_amount'].shift(1)\n",
        "analysis_data['sales_lag7'] = analysis_data['sales_amount'].shift(7)\n",
        "\n",
        "# Add moving averages\n",
        "analysis_data['sales_ma3'] = analysis_data['sales_amount'].rolling(3).mean()\n",
        "analysis_data['sales_ma7'] = analysis_data['sales_amount'].rolling(7).mean()\n",
        "\n",
        "# Add rolling statistics\n",
        "analysis_data['sales_std7'] = analysis_data['sales_amount'].rolling(7).std()\n",
        "analysis_data['sales_min7'] = analysis_data['sales_amount'].rolling(7).min()\n",
        "analysis_data['sales_max7'] = analysis_data['sales_amount'].rolling(7).max()\n",
        "\n",
        "# Add percentage changes\n",
        "analysis_data['daily_change_pct'] = analysis_data['sales_amount'].pct_change() * 100\n",
        "analysis_data['weekly_change_pct'] = analysis_data['sales_amount'].pct_change(periods=7) * 100\n",
        "\n",
        "print(\"Comprehensive analysis dataset:\")\n",
        "print(analysis_data.head(10))\n",
        "print(f\"\\nColumns: {list(analysis_data.columns)}\")\n",
        "print(f\"Shape: {analysis_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics by different time periods\n",
        "print(\"Weekly summary statistics:\")\n",
        "weekly_stats = analysis_data.groupby('weekday')['sales_amount'].agg([\n",
        "    'count', 'mean', 'std', 'min', 'max'\n",
        "]).round(2)\n",
        "weekly_stats.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "print(weekly_stats)\n",
        "\n",
        "print(\"\\nWeekend vs Weekday comparison:\")\n",
        "weekend_comparison = analysis_data.groupby('is_weekend')['sales_amount'].agg([\n",
        "    'count', 'mean', 'std'\n",
        "]).round(2)\n",
        "weekend_comparison.index = ['Weekday', 'Weekend']\n",
        "print(weekend_comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Practice Exercises\n",
        "\n",
        "### Exercise 1: Date Parsing and Cleaning\n",
        "Clean and parse the following messy date data from an e-commerce system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise data with messy dates\n",
        "messy_order_data = pd.DataFrame({\n",
        "    'order_id': ['ORD001', 'ORD002', 'ORD003', 'ORD004', 'ORD005'],\n",
        "    'order_date': ['2025-01-15', '15/01/2025', 'Jan 16, 2025', '2025.01.17', '17-1-25'],\n",
        "    'ship_date': ['2025-01-16 14:30', '16/01/2025 10:15', 'Jan 17, 2025 16:45', '2025.01.18 09:30', '18-1-25 11:20'],\n",
        "    'order_value': [123.45, 67.89, 234.56, 89.12, 156.78]\n",
        "})\n",
        "\n",
        "print(\"Messy order data:\")\n",
        "print(messy_order_data)\n",
        "\n",
        "# Your task:\n",
        "# 1. Convert order_date and ship_date to proper datetime format\n",
        "# 2. Calculate delivery_time (difference between ship_date and order_date)\n",
        "# 3. Set order_date as the index\n",
        "# 4. Add day_of_week column\n",
        "\n",
        "# Your code here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Time Series Resampling\n",
        "Create hourly sales data and then analyze it at different time frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create hourly sales data for a week\n",
        "np.random.seed(456)\n",
        "hourly_sales_dates = pd.date_range('2025-01-13', '2025-01-19 23:00:00', freq='H')\n",
        "hourly_sales_values = []\n",
        "\n",
        "for dt in hourly_sales_dates:\n",
        "    base_sales = 50\n",
        "    hour = dt.hour\n",
        "    weekday = dt.weekday\n",
        "    \n",
        "    # Business hours boost\n",
        "    if 9 <= hour <= 17:\n",
        "        base_sales += 30\n",
        "    \n",
        "    # Weekend reduction\n",
        "    if weekday >= 5:\n",
        "        base_sales *= 0.7\n",
        "    \n",
        "    # Add randomness\n",
        "    final_sales = base_sales + np.random.normal(0, 15)\n",
        "    hourly_sales_values.append(max(final_sales, 10))  # Minimum 10\n",
        "\n",
        "hourly_sales_df = pd.DataFrame({\n",
        "    'datetime': hourly_sales_dates,\n",
        "    'sales': hourly_sales_values\n",
        "}).set_index('datetime')\n",
        "\n",
        "print(f\"Hourly sales data created: {len(hourly_sales_df)} hours\")\n",
        "print(hourly_sales_df.head(12))\n",
        "\n",
        "# Your tasks:\n",
        "# 1. Resample to daily totals\n",
        "# 2. Resample to 6-hour periods with average sales\n",
        "# 3. Create business hours summary (9 AM - 5 PM)\n",
        "# 4. Compare weekend vs weekday average sales\n",
        "\n",
        "# Your code here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Moving Averages and Trend Analysis\n",
        "Analyze trends in the provided customer data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customer registration data\n",
        "registration_dates = pd.date_range('2025-01-01', '2025-01-31', freq='D')\n",
        "np.random.seed(789)\n",
        "\n",
        "# Simulate growing customer registrations with some noise\n",
        "base_registrations = 20\n",
        "growth_trend = np.linspace(0, 10, len(registration_dates))  # Linear growth\n",
        "noise = np.random.normal(0, 5, len(registration_dates))\n",
        "registrations = base_registrations + growth_trend + noise\n",
        "registrations = [max(int(reg), 1) for reg in registrations]  # Minimum 1\n",
        "\n",
        "customer_registrations = pd.DataFrame({\n",
        "    'date': registration_dates,\n",
        "    'new_customers': registrations\n",
        "}).set_index('date')\n",
        "\n",
        "print(\"Daily customer registrations:\")\n",
        "print(customer_registrations.head(10))\n",
        "\n",
        "# Your tasks:\n",
        "# 1. Calculate 3-day, 7-day, and 14-day moving averages\n",
        "# 2. Identify days where actual registrations were >20% above the 7-day average\n",
        "# 3. Calculate the overall trend (is it increasing or decreasing?)\n",
        "# 4. Find the best and worst performing weeks\n",
        "\n",
        "# Your code here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4: Time Zone Conversion Challenge\n",
        "Convert order timestamps from different time zones to UTC for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Orders from different time zones\n",
        "global_orders = pd.DataFrame({\n",
        "    'order_id': ['US_001', 'EU_001', 'ASIA_001', 'US_002', 'EU_002'],\n",
        "    'local_timestamp': [\n",
        "        '2025-01-15 10:30:00',  # US Eastern\n",
        "        '2025-01-15 16:30:00',  # Europe/London\n",
        "        '2025-01-16 01:30:00',  # Asia/Tokyo\n",
        "        '2025-01-15 14:45:00',  # US Pacific\n",
        "        '2025-01-15 18:15:00'   # Europe/Berlin\n",
        "    ],\n",
        "    'timezone': ['US/Eastern', 'Europe/London', 'Asia/Tokyo', 'US/Pacific', 'Europe/Berlin'],\n",
        "    'order_value': [123.45, 67.89, 234.56, 89.12, 156.78]\n",
        "})\n",
        "\n",
        "print(\"Global orders with local timestamps:\")\n",
        "print(global_orders)\n",
        "\n",
        "# Your tasks:\n",
        "# 1. Convert all local timestamps to UTC\n",
        "# 2. Sort orders by UTC timestamp\n",
        "# 3. Calculate the time difference between the first and last order\n",
        "# 4. Group orders by UTC hour to see global activity patterns\n",
        "\n",
        "# Your code here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps and Summary\n",
        "\n",
        "Congratulations! You've completed the comprehensive Data Reshaping session covering:\n",
        "\n",
        "### What We've Learned Today:\n",
        "\n",
        "**Part 1: Merge, Join, and Concatenate**\n",
        "- Combining DataFrames using `pd.merge()` and `pd.concat()`\n",
        "- Different types of joins (inner, left, right, outer)\n",
        "- Handling complex multi-table relationships\n",
        "\n",
        "**Part 2: Melt and Pivot Operations**\n",
        "- Converting between wide and long formats\n",
        "- Creating pivot tables for analysis and reporting\n",
        "- Advanced reshaping techniques\n",
        "\n",
        "**Part 3: Time Series Manipulation Basics**\n",
        "- Working with datetime data types and indices\n",
        "- Time-based filtering and resampling\n",
        "- Moving averages and trend analysis\n",
        "- Time zone handling for global e-commerce\n",
        "\n",
        "### Key Skills Acquired:\n",
        "1. **Data Integration**: Combine data from multiple sources effectively\n",
        "2. **Data Reshaping**: Transform data for analysis and visualization\n",
        "3. **Time Series Preparation**: Handle temporal data for trend analysis\n",
        "4. **E-commerce Analytics**: Apply techniques to real business scenarios\n",
        "5. **Problem Solving**: Handle common data quality issues\n",
        "\n",
        "### What's Next:\n",
        "Tomorrow (Thursday, May 1st), we'll dive into the **Olist Brazilian E-commerce Dataset**:\n",
        "- Understanding the database schema and relationships\n",
        "- Loading and exploring multiple data tables\n",
        "- Applying today's techniques to real e-commerce data\n",
        "- Initial data exploration and quality assessment\n",
        "\n",
        "### Practice Recommendations:\n",
        "- Work through all the exercises in each part\n",
        "- Try combining techniques (e.g., merge data, then pivot, then analyze trends)\n",
        "- Experiment with your own datasets\n",
        "- Review SQL equivalents to reinforce your existing knowledge\n",
        "\n",
        "### Important Reminders:\n",
        "- **Always verify your data** after reshaping operations\n",
        "- **Plan your transformations** step by step\n",
        "- **Handle missing values** appropriately for your analysis\n",
        "- **Consider performance** with large datasets\n",
        "- **Document your process** for reproducibility\n",
        "\n",
        "These data transformation skills form the foundation for all advanced analytics work you'll do throughout the course!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}