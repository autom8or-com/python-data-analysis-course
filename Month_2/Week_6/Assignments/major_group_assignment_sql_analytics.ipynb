{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Major Group Assignment: Advanced SQL-Python E-commerce Analytics\n",
    "\n",
    "## üéØ Assignment Overview\n",
    "\n",
    "**Business Context**: You are a data analytics team at Olist, Brazil's largest e-commerce platform. The executive team needs comprehensive insights to drive strategic decisions for 2024. Your team must combine advanced data analysis techniques with interactive visualizations to deliver actionable business intelligence.\n",
    "\n",
    "**Group Size**: 3-4 students per group\n",
    "**Duration**: 2 weeks\n",
    "**Deliverables**: \n",
    "- This completed Jupyter notebook with analysis\n",
    "- 10-minute group presentation\n",
    "- Executive summary report (2-3 pages)\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "This assignment integrates **Week 6 topics**:\n",
    "- **Wednesday**: Advanced pandas operations, data manipulation, and feature engineering\n",
    "- **Thursday**: SQL-Python integration, database connections, and hybrid analytics\n",
    "- **Interactive Visualizations**: Plotly dashboards and storytelling with data\n",
    "\n",
    "## üè¢ Business Scenario\n",
    "\n",
    "Olist's executive team is preparing for their 2024 strategic planning meeting. They need data-driven insights to answer critical business questions:\n",
    "\n",
    "1. **Customer Strategy**: Which customer segments drive the most value?\n",
    "2. **Marketing Optimization**: Which acquisition channels provide the best ROI?\n",
    "3. **Product Portfolio**: What product categories should we prioritize?\n",
    "4. **Geographic Expansion**: Which regions offer the biggest growth opportunities?\n",
    "5. **Operational Excellence**: How can we improve delivery performance and customer satisfaction?\n",
    "\n",
    "Your team's analysis will directly influence million-dollar strategic decisions.\n",
    "\n",
    "## üóÑÔ∏è Data Sources\n",
    "\n",
    "You have access to Olist's complete dataset through **Supabase PostgreSQL**:\n",
    "- **E-commerce Data**: 100k+ orders, customers, products, reviews\n",
    "- **Marketing Funnel**: Lead generation, conversions, channel performance\n",
    "- **Geographic Data**: Brazilian state and city information\n",
    "\n",
    "**Database Connection**: Use the provided Supabase credentials from Thursday's lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Assignment Structure\n",
    "\n",
    "### Part 1: Data Foundation (20 points)\n",
    "- Establish PostgreSQL connection using SQLAlchemy\n",
    "- Explore database schema and data quality\n",
    "- Create comprehensive data quality report\n",
    "\n",
    "### Part 2: Advanced Analytics (35 points)\n",
    "- **Customer Segmentation**: Advanced pandas analysis with RFM modeling\n",
    "- **Marketing Channel Analysis**: SQL-Python integration for ROI calculation\n",
    "- **Geographic Intelligence**: Regional performance and expansion opportunities\n",
    "- **Product Performance**: Category analysis with recommendation engine logic\n",
    "\n",
    "### Part 3: Interactive Business Intelligence (25 points)\n",
    "- Create interactive Plotly dashboard with 5+ visualizations\n",
    "- Implement filters and cross-chart interactions\n",
    "- Design executive-level presentation visuals\n",
    "\n",
    "### Part 4: Strategic Recommendations (20 points)\n",
    "- Generate actionable business insights\n",
    "- Quantify impact and ROI of recommendations\n",
    "- Create implementation roadmap\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Getting Started\n",
    "\n",
    "**Step 1**: Form your team and assign roles:\n",
    "- **Data Engineer**: Database connections and data pipeline\n",
    "- **Analyst**: Advanced pandas analysis and statistical modeling\n",
    "- **Visualization Specialist**: Interactive dashboards and storytelling\n",
    "- **Business Strategist**: Insights translation and recommendations\n",
    "\n",
    "**Step 2**: Set up your environment and establish database connection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Setup and imports\nimport pandas as pd\nimport numpy as np\nimport sqlalchemy\nfrom sqlalchemy import create_engine, text, inspect\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport warnings\nimport os\nfrom dotenv import load_dotenv\n\nwarnings.filterwarnings('ignore')\n\n# Team Information\nTEAM_INFO = {\n    'team_name': 'YOUR_TEAM_NAME',\n    'members': [\n        'Student 1 Name - Role',\n        'Student 2 Name - Role', \n        'Student 3 Name - Role',\n        'Student 4 Name - Role (if applicable)'\n    ],\n    'submission_date': 'YYYY-MM-DD'\n}\n\nprint(f\"üéØ Team: {TEAM_INFO['team_name']}\")\nprint(f\"üë• Members: {len(TEAM_INFO['members'])}\")\nfor member in TEAM_INFO['members']:\n    print(f\"   ‚Ä¢ {member}\")\n    \nprint(\"\\\\nüîí Loading environment variables for secure database connection...\")\nprint(\"üìù Make sure you have created a .env file with your Supabase credentials\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Foundation (20 points)\n",
    "\n",
    "### 1.1 Database Connection and Schema Exploration\n",
    "\n",
    "**Tasks**:\n",
    "1. Establish secure PostgreSQL connection to Supabase\n",
    "2. Document all available tables and their relationships\n",
    "3. Analyze data completeness and quality issues\n",
    "4. Create a comprehensive data dictionary\n",
    "\n",
    "**Evaluation Criteria**:\n",
    "- ‚úÖ Successful database connection with proper error handling\n",
    "- ‚úÖ Complete schema documentation\n",
    "- ‚úÖ Data quality assessment with quantified issues\n",
    "- ‚úÖ Professional data dictionary with business context"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 1.1 Database Connection Setup\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Supabase PostgreSQL Configuration\nDATABASE_CONFIG = {\n    'host': os.getenv('POSTGRES_HOST'),\n    'port': int(os.getenv('POSTGRES_PORT', 6543)),\n    'database': os.getenv('POSTGRES_DATABASE', 'postgres'),\n    'user': os.getenv('POSTGRES_USER'),\n    'password': os.getenv('POSTGRES_PASSWORD')\n}\n\n# Verify that environment variables were loaded\nif not all([DATABASE_CONFIG['host'], DATABASE_CONFIG['user'], DATABASE_CONFIG['password']]):\n    print(\"‚ùå Missing required database credentials. Please check your .env file.\")\n    print(\"üìù Required variables: POSTGRES_HOST, POSTGRES_USER, POSTGRES_PASSWORD\")\n    print(\"üí° See the .env.example file for the correct format\")\n    raise ValueError(\"Database credentials not found in environment variables\")\n\ndef create_database_connection():\n    \"\"\"\n    Create and test PostgreSQL database connection.\n    \n    Returns:\n    --------\n    sqlalchemy.engine.Engine: Database engine\n    \"\"\"\n    try:\n        # Create PostgreSQL connection string\n        connection_string = f\"postgresql://{DATABASE_CONFIG['user']}:{DATABASE_CONFIG['password']}@{DATABASE_CONFIG['host']}:{DATABASE_CONFIG['port']}/{DATABASE_CONFIG['database']}\"\n        \n        # Create engine with connection pooling\n        engine = create_engine(\n            connection_string,\n            pool_size=5,\n            max_overflow=10,\n            pool_timeout=30,\n            pool_recycle=3600,\n            connect_args={\n                \"connect_timeout\": 30,\n                \"application_name\": \"Python_Data_Analysis_Course\"\n            }\n        )\n        \n        # Test connection\n        with engine.connect() as conn:\n            result = conn.execute(text(\"SELECT version()\"))\n            version = result.scalar()\n            print(\"‚úÖ PostgreSQL connection established successfully\")\n            print(f\"üêò Database version: {version[:50]}...\")\n        \n        return engine\n        \n    except Exception as e:\n        print(f\"‚ùå PostgreSQL connection failed: {e}\")\n        print(\"üîß Troubleshooting tips:\")\n        print(\"  ‚Ä¢ Check your internet connection\")\n        print(\"  ‚Ä¢ Verify database credentials in .env file\")\n        print(\"  ‚Ä¢ Ensure Supabase database is running\")\n        print(\"  ‚Ä¢ Check if you need to whitelist your IP address\")\n        raise\n\n# Test connection\nprint(\"üîó Testing database connection...\")\ndb_engine = create_database_connection()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 1.2 Schema Exploration and Documentation\n\ndef explore_database_schema(engine):\n    \"\"\"\n    Comprehensive database schema exploration.\n    \n    Explores the actual Olist e-commerce database structure:\n    - olist_sales_data_set: Core business tables\n    - olist_marketing_data_set: Marketing and leads data\n    \n    Returns:\n    --------\n    dict: Comprehensive schema information\n    \"\"\"\n    inspector = inspect(engine)\n    schema_info = {}\n    \n    # Define the schemas we're working with\n    target_schemas = ['olist_sales_data_set', 'olist_marketing_data_set']\n    \n    print(\"üìã Discovering Olist database schema...\")\n    \n    for schema in target_schemas:\n        try:\n            tables = inspector.get_table_names(schema=schema)\n            print(f\"\\\\nüìä Schema '{schema}' contains {len(tables)} tables:\")\n            \n            for table in tables:\n                schema_qualified_name = f\"{schema}.{table}\"\n                try:\n                    with engine.connect() as conn:\n                        # Get row count\n                        result = conn.execute(text(f'SELECT COUNT(*) FROM \"{schema}\".\"{table}\"'))\n                        row_count = result.scalar()\n                        \n                        # Get column information\n                        columns = inspector.get_columns(table, schema=schema)\n                        \n                        schema_info[schema_qualified_name] = {\n                            'schema': schema,\n                            'table': table,\n                            'rows': row_count,\n                            'columns': [col['name'] for col in columns],\n                            'column_types': {col['name']: str(col['type']) for col in columns}\n                        }\n                        \n                        print(f\"  üìã {table}: {row_count:,} rows, {len(columns)} columns\")\n                        \n                except Exception as e:\n                    print(f\"  ‚ö†Ô∏è Could not access {schema}.{table}: {e}\")\n                    continue\n                    \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Could not access schema '{schema}': {e}\")\n            continue\n    \n    # Display schema summary\n    total_rows = sum(info['rows'] for info in schema_info.values())\n    print(f\"\\\\nüóÉÔ∏è Total tables discovered: {len(schema_info)}\")\n    print(f\"üìè Total rows across all tables: {total_rows:,}\")\n    \n    # Show table relationships for business understanding\n    print(f\"\\\\nüîó Key Business Relationships:\")\n    print(\"  ‚Ä¢ customers ‚Üí orders ‚Üí order_items ‚Üí products\")\n    print(\"  ‚Ä¢ orders ‚Üí payments & reviews\")\n    print(\"  ‚Ä¢ marketing_leads ‚Üí closed_deals ‚Üí sellers\")\n    print(\"  ‚Ä¢ geographic coordinates for Brazilian analysis\")\n    \n    return schema_info\n\n# Execute schema exploration\nprint(\"üìä Exploring database schema...\")\nschema_info = explore_database_schema(db_engine)\n\n# Show sample of main business table\nprint(\"\\\\nüìã Sample from main orders table:\")\nsample_query = '''\nSELECT \n    order_id,\n    customer_id,\n    order_status,\n    order_purchase_timestamp,\n    order_delivered_customer_date\nFROM \"olist_sales_data_set\".\"olist_orders_dataset\" \nLIMIT 3\n'''\n\ntry:\n    sample_data = pd.read_sql(text(sample_query), db_engine)\n    print(\"‚úÖ Sample orders data:\")\n    display(sample_data)\nexcept Exception as e:\n    print(f\"‚ùå Could not fetch sample data: {e}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 1.3 Data Quality Assessment\n\ndef assess_data_quality(engine):\n    \"\"\"\n    Comprehensive data quality assessment using actual Olist data.\n    \n    Analyzes:\n    1. Missing values by table and column\n    2. Duplicate records\n    3. Data consistency across relationships\n    4. Date range validation\n    5. Business rule compliance\n    \n    Returns:\n    --------\n    dict: Data quality report with scores and recommendations\n    \"\"\"\n    \n    quality_report = {\n        'completeness': {},\n        'consistency': {},\n        'validity': {},\n        'overall_score': 0\n    }\n    \n    print(\"üîç Assessing data quality across Olist dataset...\")\n    \n    # Check completeness for key business tables\n    completeness_checks = {\n        'customers': '''\n            SELECT \n                COUNT(*) as total_records,\n                COUNT(customer_id) as customer_id_filled,\n                COUNT(customer_state) as state_filled,\n                COUNT(customer_city) as city_filled\n            FROM \"olist_sales_data_set\".\"olist_customers_dataset\"\n        ''',\n        'orders': '''\n            SELECT \n                COUNT(*) as total_records,\n                COUNT(order_id) as order_id_filled,\n                COUNT(customer_id) as customer_id_filled,\n                COUNT(order_purchase_timestamp) as purchase_date_filled,\n                COUNT(order_status) as status_filled\n            FROM \"olist_sales_data_set\".\"olist_orders_dataset\"\n        ''',\n        'reviews': '''\n            SELECT \n                COUNT(*) as total_records,\n                COUNT(review_id) as review_id_filled,\n                COUNT(order_id) as order_id_filled,\n                COUNT(review_score) as score_filled,\n                COUNT(review_creation_date) as date_filled\n            FROM \"olist_sales_data_set\".\"olist_order_reviews_dataset\"\n        '''\n    }\n    \n    print(\"\\\\nüìä Data Completeness Analysis:\")\n    for table_name, query in completeness_checks.items():\n        try:\n            result = pd.read_sql(text(query), engine)\n            total = result.iloc[0]['total_records']\n            \n            print(f\"\\\\n  üìã {table_name.title()} Table:\")\n            print(f\"    Total records: {total:,}\")\n            \n            completeness_scores = []\n            for col in result.columns:\n                if col != 'total_records':\n                    filled = result.iloc[0][col]\n                    completeness = (filled / total) * 100 if total > 0 else 0\n                    completeness_scores.append(completeness)\n                    print(f\"    {col}: {completeness:.1f}% complete\")\n            \n            quality_report['completeness'][table_name] = sum(completeness_scores) / len(completeness_scores)\n            \n        except Exception as e:\n            print(f\"    ‚ùå Could not assess {table_name}: {e}\")\n    \n    # Check data consistency\n    print(\"\\\\nüîó Data Consistency Checks:\")\n    \n    consistency_checks = {\n        'customer_order_relationship': '''\n            SELECT \n                COUNT(DISTINCT o.customer_id) as orders_customers,\n                COUNT(DISTINCT c.customer_id) as total_customers,\n                ROUND(COUNT(DISTINCT o.customer_id) * 100.0 / COUNT(DISTINCT c.customer_id), 1) as customer_engagement_rate\n            FROM \"olist_sales_data_set\".\"olist_customers_dataset\" c\n            LEFT JOIN \"olist_sales_data_set\".\"olist_orders_dataset\" o ON c.customer_id = o.customer_id\n        ''',\n        'order_items_integrity': '''\n            SELECT \n                COUNT(DISTINCT oi.order_id) as orders_with_items,\n                COUNT(DISTINCT o.order_id) as total_orders,\n                ROUND(COUNT(DISTINCT oi.order_id) * 100.0 / COUNT(DISTINCT o.order_id), 1) as order_completion_rate\n            FROM \"olist_sales_data_set\".\"olist_orders_dataset\" o\n            LEFT JOIN \"olist_sales_data_set\".\"olist_order_items_dataset\" oi ON o.order_id = oi.order_id\n        '''\n    }\n    \n    for check_name, query in consistency_checks.items():\n        try:\n            result = pd.read_sql(text(query), engine)\n            print(f\"  ‚úÖ {check_name.replace('_', ' ').title()}:\")\n            for col in result.columns:\n                value = result.iloc[0][col]\n                print(f\"    {col}: {value}\")\n        except Exception as e:\n            print(f\"  ‚ùå {check_name} failed: {e}\")\n    \n    # Check business logic validity\n    print(\"\\\\nüìà Business Logic Validation:\")\n    \n    validity_checks = {\n        'review_scores': '''\n            SELECT \n                MIN(review_score) as min_score,\n                MAX(review_score) as max_score,\n                COUNT(CASE WHEN review_score BETWEEN 1 AND 5 THEN 1 END) as valid_scores,\n                COUNT(*) as total_scores\n            FROM \"olist_sales_data_set\".\"olist_order_reviews_dataset\"\n            WHERE review_score IS NOT NULL\n        ''',\n        'delivery_logic': '''\n            SELECT \n                COUNT(CASE WHEN order_delivered_customer_date >= order_purchase_timestamp THEN 1 END) as logical_deliveries,\n                COUNT(CASE WHEN order_delivered_customer_date IS NOT NULL AND order_purchase_timestamp IS NOT NULL THEN 1 END) as total_with_dates\n            FROM \"olist_sales_data_set\".\"olist_orders_dataset\"\n        '''\n    }\n    \n    validity_scores = []\n    for check_name, query in validity_checks.items():\n        try:\n            result = pd.read_sql(text(query), engine)\n            print(f\"  ‚úÖ {check_name.replace('_', ' ').title()}:\")\n            \n            if 'valid_scores' in result.columns:\n                valid = result.iloc[0]['valid_scores']\n                total = result.iloc[0]['total_scores']\n                score = (valid / total) * 100 if total > 0 else 0\n                validity_scores.append(score)\n                print(f\"    Valid review scores: {score:.1f}% ({valid:,}/{total:,})\")\n            \n            if 'logical_deliveries' in result.columns:\n                logical = result.iloc[0]['logical_deliveries']\n                total = result.iloc[0]['total_with_dates']\n                score = (logical / total) * 100 if total > 0 else 0\n                validity_scores.append(score)\n                print(f\"    Logical delivery dates: {score:.1f}% ({logical:,}/{total:,})\")\n                \n        except Exception as e:\n            print(f\"  ‚ùå {check_name} failed: {e}\")\n    \n    # Calculate overall quality score\n    completeness_avg = sum(quality_report['completeness'].values()) / len(quality_report['completeness']) if quality_report['completeness'] else 0\n    validity_avg = sum(validity_scores) / len(validity_scores) if validity_scores else 0\n    overall_score = (completeness_avg + validity_avg) / 2\n    \n    quality_report['overall_score'] = overall_score\n    \n    print(f\"\\\\nüìä Data Quality Summary:\")\n    print(f\"  ‚Ä¢ Completeness Score: {completeness_avg:.1f}/100\")\n    print(f\"  ‚Ä¢ Validity Score: {validity_avg:.1f}/100\") \n    print(f\"  ‚Ä¢ Overall Quality Score: {overall_score:.1f}/100\")\n    \n    if overall_score >= 90:\n        print(\"  üéâ Excellent data quality - ready for analysis!\")\n    elif overall_score >= 80:\n        print(\"  ‚úÖ Good data quality - minor issues to address\")\n    elif overall_score >= 70:\n        print(\"  ‚ö†Ô∏è Acceptable data quality - some attention needed\")\n    else:\n        print(\"  ‚ùå Poor data quality - significant cleanup required\")\n    \n    return quality_report\n\n# Execute data quality assessment\nprint(\"üîç Assessing data quality...\")\nquality_report = assess_data_quality(db_engine)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Quality Report\n",
    "\n",
    "**Document your findings here**:\n",
    "\n",
    "#### Table Inventory\n",
    "| Table Name | Row Count | Key Columns | Business Purpose |\n",
    "|------------|-----------|-------------|------------------|\n",
    "| | | | |\n",
    "\n",
    "#### Data Quality Issues\n",
    "| Issue Type | Affected Tables | Severity | Recommended Action |\n",
    "|------------|----------------|----------|--------------------|\n",
    "| | | | |\n",
    "\n",
    "#### Data Quality Score: __/100\n",
    "\n",
    "**Executive Summary**: Write 2-3 sentences summarizing the overall data quality and readiness for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Analytics (35 points)\n",
    "\n",
    "### 2.1 Customer Segmentation Analysis (Advanced Pandas)\n",
    "\n",
    "**Business Question**: \"Which customer segments drive the most value, and how should we tailor our strategies to each segment?\"\n",
    "\n",
    "**Requirements**:\n",
    "1. Implement RFM (Recency, Frequency, Monetary) analysis using advanced pandas\n",
    "2. Create customer lifetime value predictions\n",
    "3. Identify high-value customer characteristics\n",
    "4. Develop customer churn risk scoring\n",
    "\n",
    "**Advanced Pandas Techniques to Demonstrate**:\n",
    "- Multi-level groupby operations\n",
    "- Window functions and rolling calculations\n",
    "- Advanced indexing and data transformation\n",
    "- Statistical analysis and correlation studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Customer Segmentation with Advanced Pandas\n",
    "\n",
    "def extract_customer_data(engine):\n",
    "    \"\"\"\n",
    "    Extract comprehensive customer data using SQL.\n",
    "    \n",
    "    TODO: Create SQL query that joins:\n",
    "    - Customer information\n",
    "    - Order history\n",
    "    - Purchase behavior\n",
    "    - Geographic data\n",
    "    - Review scores\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: Customer dataset ready for analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Write comprehensive SQL query\n",
    "    sql_query = \"\"\"\n",
    "    -- Your comprehensive customer analysis query here\n",
    "    -- Think about what data you need for RFM analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute query and return DataFrame\n",
    "    # return pd.read_sql(sql_query, engine)\n",
    "    pass\n",
    "\n",
    "def perform_rfm_analysis(customer_df):\n",
    "    \"\"\"\n",
    "    Advanced RFM analysis using pandas.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Recency calculation (days since last purchase)\n",
    "    2. Frequency calculation (number of orders)\n",
    "    3. Monetary calculation (total customer value)\n",
    "    4. RFM scoring (1-5 scale for each dimension)\n",
    "    5. Customer segmentation based on RFM scores\n",
    "    \n",
    "    Advanced pandas techniques:\n",
    "    - Use pd.qcut() for quintile-based scoring\n",
    "    - Apply custom scoring functions\n",
    "    - Create multi-level segments\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: Customer data with RFM scores and segments\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def calculate_customer_lifetime_value(customer_df):\n",
    "    \"\"\"\n",
    "    Calculate CLV using advanced pandas operations.\n",
    "    \n",
    "    TODO: Implement CLV calculation using:\n",
    "    - Historical purchase patterns\n",
    "    - Cohort analysis\n",
    "    - Retention rate modeling\n",
    "    - Predictive CLV scoring\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: Customer data with CLV predictions\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Execute customer segmentation analysis\n",
    "print(\"üë• Performing customer segmentation analysis...\")\n",
    "# customer_data = extract_customer_data(db_engine)\n",
    "# rfm_analysis = perform_rfm_analysis(customer_data)\n",
    "# clv_analysis = calculate_customer_lifetime_value(customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1.2 Advanced Customer Analytics\n",
    "\n",
    "def analyze_customer_behavior_patterns(customer_df):\n",
    "    \"\"\"\n",
    "    Advanced behavioral analysis using pandas.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Purchase seasonality analysis\n",
    "    2. Product preference mapping\n",
    "    3. Geographic behavior patterns\n",
    "    4. Customer journey analysis\n",
    "    5. Churn prediction scoring\n",
    "    \n",
    "    Advanced pandas techniques:\n",
    "    - Time series analysis with groupby\n",
    "    - Rolling window calculations\n",
    "    - Cross-tabulation and pivot operations\n",
    "    - Statistical correlation analysis\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# TODO: Create interactive customer segmentation dashboard\n",
    "def create_customer_dashboard(customer_analysis):\n",
    "    \"\"\"\n",
    "    Create interactive Plotly dashboard for customer insights.\n",
    "    \n",
    "    Include:\n",
    "    - RFM segment distribution\n",
    "    - CLV predictions by segment\n",
    "    - Geographic customer value mapping\n",
    "    - Customer journey visualization\n",
    "    \"\"\"\n",
    "    # Your dashboard implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Marketing Channel ROI Analysis (SQL-Python Integration)\n",
    "\n",
    "**Business Question**: \"Which marketing channels provide the best return on investment, and how should we optimize our marketing budget allocation?\"\n",
    "\n",
    "**Requirements**:\n",
    "1. Combine marketing funnel data with customer value using SQL joins\n",
    "2. Calculate channel-specific conversion rates and customer acquisition costs\n",
    "3. Analyze customer lifetime value by acquisition channel\n",
    "4. Build ROI optimization model\n",
    "\n",
    "**SQL-Python Integration Techniques**:\n",
    "- Complex SQL queries with CTEs and window functions\n",
    "- Parameterized queries for dynamic analysis\n",
    "- Combining SQL aggregation with pandas statistical analysis\n",
    "- Performance optimization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Marketing Channel Analysis\n",
    "\n",
    "def analyze_marketing_funnel_performance(engine):\n",
    "    \"\"\"\n",
    "    Comprehensive marketing funnel analysis using advanced SQL.\n",
    "    \n",
    "    TODO: Create SQL analysis that includes:\n",
    "    1. Lead-to-customer conversion rates by channel\n",
    "    2. Customer acquisition cost calculation\n",
    "    3. Channel performance over time\n",
    "    4. Customer quality metrics by channel\n",
    "    \n",
    "    Use advanced SQL features:\n",
    "    - CTEs for complex multi-step calculations\n",
    "    - Window functions for running totals and rankings\n",
    "    - CASE statements for business logic\n",
    "    - Date functions for cohort analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Write advanced SQL query\n",
    "    marketing_analysis_sql = \"\"\"\n",
    "    WITH marketing_funnel AS (\n",
    "        -- Your comprehensive marketing funnel query here\n",
    "        -- Include lead generation, conversion, and customer value data\n",
    "    ),\n",
    "    channel_performance AS (\n",
    "        -- Calculate channel-specific metrics\n",
    "    ),\n",
    "    roi_analysis AS (\n",
    "        -- ROI calculations by channel\n",
    "    )\n",
    "    SELECT \n",
    "        -- Your final results here\n",
    "    FROM roi_analysis\n",
    "    ORDER BY roi DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    # return pd.read_sql(marketing_analysis_sql, engine)\n",
    "    pass\n",
    "\n",
    "def calculate_customer_acquisition_metrics(engine):\n",
    "    \"\"\"\n",
    "    Calculate detailed customer acquisition metrics.\n",
    "    \n",
    "    TODO: Implement SQL-Python hybrid approach:\n",
    "    1. Use SQL for data extraction and initial aggregation\n",
    "    2. Use pandas for advanced statistical analysis\n",
    "    3. Calculate CAC, LTV, and ROI by channel\n",
    "    4. Perform cohort analysis\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Execute marketing analysis\n",
    "print(\"üìà Analyzing marketing channel performance...\")\n",
    "# marketing_performance = analyze_marketing_funnel_performance(db_engine)\n",
    "# acquisition_metrics = calculate_customer_acquisition_metrics(db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Geographic Market Analysis\n",
    "\n",
    "**Business Question**: \"Which geographic markets offer the biggest growth opportunities, and what market entry strategies should we pursue?\"\n",
    "\n",
    "**Requirements**:\n",
    "1. Analyze market penetration by state and city\n",
    "2. Identify underperforming regions with high potential\n",
    "3. Calculate market size and competition analysis\n",
    "4. Develop geographic expansion scoring model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Geographic Market Intelligence\n",
    "\n",
    "def analyze_geographic_performance(engine):\n",
    "    \"\"\"\n",
    "    Comprehensive geographic market analysis.\n",
    "    \n",
    "    TODO: Implement analysis including:\n",
    "    1. Revenue and customer density by state/city\n",
    "    2. Market penetration rates\n",
    "    3. Average order values by region\n",
    "    4. Growth rate analysis\n",
    "    5. Delivery performance by geography\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def identify_expansion_opportunities(geographic_data):\n",
    "    \"\"\"\n",
    "    Identify high-potential expansion markets.\n",
    "    \n",
    "    TODO: Create scoring model based on:\n",
    "    1. Market size vs current penetration\n",
    "    2. Customer value potential\n",
    "    3. Competitive landscape\n",
    "    4. Operational feasibility\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# TODO: Create geographic visualization dashboard\n",
    "def create_geographic_dashboard(geo_analysis):\n",
    "    \"\"\"\n",
    "    Create interactive geographic dashboard.\n",
    "    \n",
    "    Include:\n",
    "    - Choropleth maps of market performance\n",
    "    - Bubble charts for opportunity sizing\n",
    "    - Time series of regional growth\n",
    "    \"\"\"\n",
    "    # Your dashboard implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Product Portfolio Optimization\n",
    "\n",
    "**Business Question**: \"Which product categories should we prioritize for investment, and what products show the most promise for growth?\"\n",
    "\n",
    "**Requirements**:\n",
    "1. Analyze product category performance trends\n",
    "2. Identify emerging product opportunities\n",
    "3. Calculate product profitability and customer satisfaction\n",
    "4. Build product recommendation engine logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Product Portfolio Analysis\n",
    "\n",
    "def analyze_product_performance(engine):\n",
    "    \"\"\"\n",
    "    Comprehensive product and category analysis.\n",
    "    \n",
    "    TODO: Analyze:\n",
    "    1. Category revenue and growth trends\n",
    "    2. Product satisfaction scores\n",
    "    3. Cross-selling opportunities\n",
    "    4. Seasonal demand patterns\n",
    "    5. Inventory turnover analysis\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def build_product_recommendation_engine(product_data, customer_data):\n",
    "    \"\"\"\n",
    "    Build product recommendation logic.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Customer purchase pattern analysis\n",
    "    2. Product affinity scoring\n",
    "    3. Collaborative filtering basics\n",
    "    4. Business rule integration\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Execute product analysis\n",
    "print(\"üõçÔ∏è Analyzing product portfolio performance...\")\n",
    "# product_analysis = analyze_product_performance(db_engine)\n",
    "# recommendation_engine = build_product_recommendation_engine(product_analysis, customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Interactive Business Intelligence Dashboard (25 points)\n",
    "\n",
    "### 3.1 Executive Dashboard Creation\n",
    "\n",
    "**Requirements**:\n",
    "1. Create comprehensive interactive dashboard with 5+ visualizations\n",
    "2. Implement cross-chart filtering and interactions\n",
    "3. Include key performance indicators (KPIs)\n",
    "4. Design for executive-level presentation\n",
    "\n",
    "**Dashboard Components**:\n",
    "- Executive KPI summary\n",
    "- Customer segmentation insights\n",
    "- Marketing channel performance\n",
    "- Geographic market opportunities\n",
    "- Product portfolio analysis\n",
    "- Trend analysis and forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Executive Dashboard\n",
    "\n",
    "def create_executive_dashboard(customer_analysis, marketing_analysis, geographic_analysis, product_analysis):\n",
    "    \"\"\"\n",
    "    Create comprehensive executive dashboard.\n",
    "    \n",
    "    TODO: Build interactive dashboard with:\n",
    "    1. KPI summary cards\n",
    "    2. Multi-chart layout with subplots\n",
    "    3. Interactive filters and controls\n",
    "    4. Professional styling and branding\n",
    "    5. Responsive design\n",
    "    \n",
    "    Dashboard sections:\n",
    "    - Executive Summary (KPIs)\n",
    "    - Customer Intelligence\n",
    "    - Marketing Performance\n",
    "    - Geographic Insights\n",
    "    - Product Analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Create multi-section dashboard\n",
    "    \n",
    "    # Section 1: Executive KPIs\n",
    "    def create_kpi_section():\n",
    "        # Create KPI cards and summary metrics\n",
    "        pass\n",
    "    \n",
    "    # Section 2: Customer Intelligence\n",
    "    def create_customer_section():\n",
    "        # Customer segmentation and behavior visualizations\n",
    "        pass\n",
    "    \n",
    "    # Section 3: Marketing Performance\n",
    "    def create_marketing_section():\n",
    "        # Channel performance and ROI analysis\n",
    "        pass\n",
    "    \n",
    "    # Section 4: Geographic Intelligence\n",
    "    def create_geographic_section():\n",
    "        # Market opportunity and performance maps\n",
    "        pass\n",
    "    \n",
    "    # Section 5: Product Portfolio\n",
    "    def create_product_section():\n",
    "        # Product performance and recommendations\n",
    "        pass\n",
    "    \n",
    "    # Combine all sections\n",
    "    # Your dashboard implementation here\n",
    "    pass\n",
    "\n",
    "def create_interactive_filters(dashboard):\n",
    "    \"\"\"\n",
    "    Add interactive filters and controls.\n",
    "    \n",
    "    TODO: Implement:\n",
    "    1. Date range selectors\n",
    "    2. Geographic filters\n",
    "    3. Customer segment filters\n",
    "    4. Product category filters\n",
    "    5. Cross-chart interactions\n",
    "    \"\"\"\n",
    "    # Your filter implementation here\n",
    "    pass\n",
    "\n",
    "# Create and display executive dashboard\n",
    "print(\"üìä Creating executive business intelligence dashboard...\")\n",
    "# executive_dashboard = create_executive_dashboard(\n",
    "#     customer_analysis, marketing_analysis, geographic_analysis, product_analysis\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Advanced Visualization Techniques\n",
    "\n",
    "**Demonstrate advanced Plotly features**:\n",
    "- Animation for temporal analysis\n",
    "- 3D visualizations for multi-dimensional data\n",
    "- Custom hover templates and annotations\n",
    "- Advanced color schemes and styling\n",
    "- Responsive layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Advanced Visualizations\n",
    "\n",
    "def create_advanced_visualizations():\n",
    "    \"\"\"\n",
    "    Showcase advanced Plotly visualization techniques.\n",
    "    \n",
    "    TODO: Create:\n",
    "    1. Animated time series showing business evolution\n",
    "    2. 3D customer segmentation visualization\n",
    "    3. Interactive geographic choropleth with drill-down\n",
    "    4. Advanced financial charts (candlestick, waterfall)\n",
    "    5. Network graph for product relationships\n",
    "    \"\"\"\n",
    "    \n",
    "    # Animation: Business metrics over time\n",
    "    def create_animated_business_evolution():\n",
    "        # Your animated visualization here\n",
    "        pass\n",
    "    \n",
    "    # 3D: Customer segmentation space\n",
    "    def create_3d_customer_analysis():\n",
    "        # Your 3D visualization here\n",
    "        pass\n",
    "    \n",
    "    # Geographic: Interactive market analysis\n",
    "    def create_interactive_map():\n",
    "        # Your geographic visualization here\n",
    "        pass\n",
    "    \n",
    "    # Financial: Revenue waterfall analysis\n",
    "    def create_financial_analysis():\n",
    "        # Your financial visualization here\n",
    "        pass\n",
    "    \n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Create advanced visualizations\n",
    "print(\"üé® Creating advanced visualizations...\")\n",
    "# advanced_visuals = create_advanced_visualizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Strategic Recommendations (20 points)\n",
    "\n",
    "### 4.1 Business Intelligence Synthesis\n",
    "\n",
    "**Requirements**:\n",
    "1. Synthesize insights from all analyses\n",
    "2. Quantify business impact and ROI\n",
    "3. Prioritize recommendations by impact and feasibility\n",
    "4. Create implementation roadmap\n",
    "\n",
    "**Deliverables**:\n",
    "- Executive summary of key findings\n",
    "- Prioritized recommendation list\n",
    "- ROI projections and business case\n",
    "- Implementation timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Strategic Analysis and Recommendations\n",
    "\n",
    "def synthesize_business_insights():\n",
    "    \"\"\"\n",
    "    Synthesize all analyses into actionable business insights.\n",
    "    \n",
    "    TODO: Create comprehensive analysis including:\n",
    "    1. Key performance drivers identification\n",
    "    2. Growth opportunity quantification\n",
    "    3. Risk assessment and mitigation strategies\n",
    "    4. Competitive advantage analysis\n",
    "    5. Resource requirement estimation\n",
    "    \"\"\"\n",
    "    \n",
    "    business_insights = {\n",
    "        'customer_strategy': {\n",
    "            'key_segments': [],\n",
    "            'growth_opportunities': [],\n",
    "            'retention_strategies': [],\n",
    "            'revenue_impact': 0\n",
    "        },\n",
    "        'marketing_optimization': {\n",
    "            'top_channels': [],\n",
    "            'budget_reallocation': {},\n",
    "            'new_channel_opportunities': [],\n",
    "            'roi_improvement': 0\n",
    "        },\n",
    "        'geographic_expansion': {\n",
    "            'priority_markets': [],\n",
    "            'entry_strategies': {},\n",
    "            'investment_requirements': {},\n",
    "            'revenue_potential': 0\n",
    "        },\n",
    "        'product_portfolio': {\n",
    "            'priority_categories': [],\n",
    "            'discontinuation_candidates': [],\n",
    "            'new_product_opportunities': [],\n",
    "            'margin_improvement': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # TODO: Populate insights from your analyses\n",
    "    \n",
    "    return business_insights\n",
    "\n",
    "def calculate_business_impact(insights):\n",
    "    \"\"\"\n",
    "    Quantify the business impact of recommendations.\n",
    "    \n",
    "    TODO: Calculate:\n",
    "    1. Revenue impact projections\n",
    "    2. Cost savings opportunities\n",
    "    3. ROI estimates\n",
    "    4. Payback periods\n",
    "    5. Risk-adjusted returns\n",
    "    \"\"\"\n",
    "    # Your impact calculation here\n",
    "    pass\n",
    "\n",
    "# Generate strategic insights\n",
    "print(\"üéØ Synthesizing strategic business insights...\")\n",
    "business_insights = synthesize_business_insights()\n",
    "# business_impact = calculate_business_impact(business_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Executive Summary and Recommendations\n",
    "\n",
    "**Complete this section with your team's findings and recommendations**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Executive Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**1. Customer Strategy**\n",
    "- *Finding*: [Your key customer insights here]\n",
    "- *Impact*: [Quantified business impact]\n",
    "- *Recommendation*: [Specific actionable recommendation]\n",
    "\n",
    "**2. Marketing Optimization**\n",
    "- *Finding*: [Your marketing channel insights here]\n",
    "- *Impact*: [ROI improvement potential]\n",
    "- *Recommendation*: [Budget reallocation strategy]\n",
    "\n",
    "**3. Geographic Expansion**\n",
    "- *Finding*: [Your geographic opportunities here]\n",
    "- *Impact*: [Market size and revenue potential]\n",
    "- *Recommendation*: [Expansion strategy and timeline]\n",
    "\n",
    "**4. Product Portfolio**\n",
    "- *Finding*: [Your product insights here]\n",
    "- *Impact*: [Margin and growth improvement]\n",
    "- *Recommendation*: [Portfolio optimization strategy]\n",
    "\n",
    "### Strategic Recommendations\n",
    "\n",
    "#### Priority 1: High Impact, Low Effort\n",
    "1. **[Recommendation Title]**\n",
    "   - *Description*: [What to do]\n",
    "   - *Business Impact*: [Quantified benefit]\n",
    "   - *Implementation*: [How to execute]\n",
    "   - *Timeline*: [When to complete]\n",
    "   - *Investment Required*: [Resources needed]\n",
    "\n",
    "#### Priority 2: High Impact, Medium Effort\n",
    "2. **[Recommendation Title]**\n",
    "   - *Description*: [What to do]\n",
    "   - *Business Impact*: [Quantified benefit]\n",
    "   - *Implementation*: [How to execute]\n",
    "   - *Timeline*: [When to complete]\n",
    "   - *Investment Required*: [Resources needed]\n",
    "\n",
    "#### Priority 3: High Impact, High Effort\n",
    "3. **[Recommendation Title]**\n",
    "   - *Description*: [What to do]\n",
    "   - *Business Impact*: [Quantified benefit]\n",
    "   - *Implementation*: [How to execute]\n",
    "   - *Timeline*: [When to complete]\n",
    "   - *Investment Required*: [Resources needed]\n",
    "\n",
    "### Financial Projections\n",
    "\n",
    "| Recommendation | Year 1 Impact | Year 2 Impact | Total ROI | Implementation Cost |\n",
    "|----------------|---------------|---------------|-----------|--------------------|\n",
    "| [Rec 1] | $X.X M | $X.X M | X% | $X.X M |\n",
    "| [Rec 2] | $X.X M | $X.X M | X% | $X.X M |\n",
    "| [Rec 3] | $X.X M | $X.X M | X% | $X.X M |\n",
    "| **Total** | **$X.X M** | **$X.X M** | **X%** | **$X.X M** |\n",
    "\n",
    "### Implementation Roadmap\n",
    "\n",
    "#### Q1 2024\n",
    "- [Action items for Q1]\n",
    "\n",
    "#### Q2 2024\n",
    "- [Action items for Q2]\n",
    "\n",
    "#### Q3 2024\n",
    "- [Action items for Q3]\n",
    "\n",
    "#### Q4 2024\n",
    "- [Action items for Q4]\n",
    "\n",
    "### Success Metrics\n",
    "\n",
    "**Customer Strategy Success Metrics:**\n",
    "- [Specific KPIs to track]\n",
    "\n",
    "**Marketing Success Metrics:**\n",
    "- [Specific KPIs to track]\n",
    "\n",
    "**Geographic Success Metrics:**\n",
    "- [Specific KPIs to track]\n",
    "\n",
    "**Product Success Metrics:**\n",
    "- [Specific KPIs to track]\n",
    "\n",
    "### Risk Mitigation\n",
    "\n",
    "**Key Risks and Mitigation Strategies:**\n",
    "1. *Risk*: [Identified risk] | *Mitigation*: [How to address]\n",
    "2. *Risk*: [Identified risk] | *Mitigation*: [How to address]\n",
    "3. *Risk*: [Identified risk] | *Mitigation*: [How to address]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Assignment Evaluation Rubric\n",
    "\n",
    "### Part 1: Data Foundation (20 points)\n",
    "- **Excellent (18-20)**: Robust database connection, comprehensive schema documentation, thorough data quality assessment\n",
    "- **Good (15-17)**: Solid connection setup, good schema exploration, adequate quality analysis\n",
    "- **Satisfactory (12-14)**: Basic connection, limited exploration, minimal quality assessment\n",
    "- **Needs Improvement (0-11)**: Connection issues, incomplete exploration, poor quality analysis\n",
    "\n",
    "### Part 2: Advanced Analytics (35 points)\n",
    "- **Excellent (32-35)**: Sophisticated pandas operations, advanced SQL integration, insightful business analysis\n",
    "- **Good (28-31)**: Good analytical techniques, solid SQL-pandas integration, meaningful insights\n",
    "- **Satisfactory (21-27)**: Basic analysis, simple integration, limited insights\n",
    "- **Needs Improvement (0-20)**: Poor analysis, minimal integration, unclear insights\n",
    "\n",
    "### Part 3: Interactive Visualizations (25 points)\n",
    "- **Excellent (23-25)**: Professional dashboard, advanced interactivity, excellent design\n",
    "- **Good (20-22)**: Solid dashboard, good interactivity, clear visualizations\n",
    "- **Satisfactory (15-19)**: Basic dashboard, limited interactivity, acceptable design\n",
    "- **Needs Improvement (0-14)**: Poor visualizations, minimal interactivity, unclear design\n",
    "\n",
    "### Part 4: Strategic Recommendations (20 points)\n",
    "- **Excellent (18-20)**: Actionable insights, quantified impact, clear implementation plan\n",
    "- **Good (15-17)**: Good insights, estimated impact, reasonable implementation\n",
    "- **Satisfactory (12-14)**: Basic insights, limited impact analysis, vague implementation\n",
    "- **Needs Improvement (0-11)**: Poor insights, no impact analysis, unclear implementation\n",
    "\n",
    "### Additional Evaluation Criteria:\n",
    "- **Code Quality**: Clean, documented, efficient code\n",
    "- **Business Acumen**: Understanding of business context and implications\n",
    "- **Technical Innovation**: Creative use of advanced techniques\n",
    "- **Presentation**: Professional presentation and communication\n",
    "- **Teamwork**: Evidence of effective collaboration\n",
    "\n",
    "### Total Score: ___/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Submission Instructions\n",
    "\n",
    "### Deliverables Checklist:\n",
    "- [ ] **Completed Jupyter Notebook** (this file) with all code and analysis\n",
    "- [ ] **Executive Summary Report** (2-3 page PDF) with key findings and recommendations\n",
    "- [ ] **Group Presentation** (10 minutes) showcasing insights and dashboard\n",
    "- [ ] **Individual Reflection** (1 page per team member) on learning and contribution\n",
    "\n",
    "### Technical Requirements:\n",
    "- [ ] All code cells execute without errors\n",
    "- [ ] Database connections are properly managed and closed\n",
    "- [ ] Visualizations render correctly in the notebook\n",
    "- [ ] Code is well-documented with comments and docstrings\n",
    "- [ ] Data sources are properly cited\n",
    "\n",
    "### Presentation Guidelines:\n",
    "- **Duration**: 10 minutes + 5 minutes Q&A\n",
    "- **Format**: Professional business presentation\n",
    "- **Content**: Focus on insights and recommendations, not technical details\n",
    "- **Audience**: Assume C-level executives as your audience\n",
    "- **Demo**: Include live dashboard demonstration\n",
    "\n",
    "### Submission Details:\n",
    "- **Due Date**: [Insert due date]\n",
    "- **Submission Method**: [Insert submission instructions]\n",
    "- **File Naming**: `Week6_Major_Assignment_[TeamName].ipynb`\n",
    "- **Late Policy**: [Insert late policy]\n",
    "\n",
    "### Support Resources:\n",
    "- **Office Hours**: [Insert schedule]\n",
    "- **Discussion Forum**: [Insert link]\n",
    "- **Technical Support**: [Insert contact]\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Good Luck!\n",
    "\n",
    "This assignment represents the culmination of your SQL-Python integration learning. Focus on:\n",
    "- **Business Impact**: Always connect technical findings to business value\n",
    "- **Collaboration**: Leverage each team member's strengths\n",
    "- **Innovation**: Don't be afraid to try advanced techniques\n",
    "- **Communication**: Tell compelling stories with your data\n",
    "\n",
    "Remember: The goal is not just to analyze data, but to generate insights that drive real business decisions. Think like consultants delivering recommendations to Olist's executive team.\n",
    "\n",
    "**Happy analyzing! üöÄüìä**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}