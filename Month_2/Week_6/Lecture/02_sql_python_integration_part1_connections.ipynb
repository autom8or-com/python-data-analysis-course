{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - SQL and Python Integration Part 1: Database Connections\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Establish database connections from Python using SQLAlchemy\n",
    "2. Load Olist e-commerce data into SQLite for SQL analysis\n",
    "3. Execute basic SQL queries from Python notebooks\n",
    "4. Understand the relationship between SQL databases and Python DataFrames\n",
    "5. Implement proper connection management and error handling\n",
    "\n",
    "## Business Context: Bridging SQL and Python\n",
    "\n",
    "In modern business environments, data often lives in **SQL databases** while analysis happens in **Python**. The ability to seamlessly bridge these two worlds is essential for:\n",
    "\n",
    "- **Data Extraction** - Pull specific datasets from enterprise databases\n",
    "- **Performance** - Leverage database engines for heavy computation\n",
    "- **Real-time Analysis** - Connect to live business systems\n",
    "- **Scalability** - Handle datasets too large for memory\n",
    "- **Integration** - Combine SQL's querying power with Python's analytical capabilities\n",
    "\n",
    "Today we'll master the fundamental skills of connecting Python to databases and working with real business data using both SQL and pandas approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for database connectivity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "import zipfile\n",
    "import io\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Database configuration\n",
    "DATABASE_CONFIG = {\n",
    "    'sqlite_path': 'olist_ecommerce.db',\n",
    "    'connection_timeout': 30,\n",
    "    'echo': False  # Set to True to see SQL queries\n",
    "}\n",
    "\n",
    "print(\"üóÑÔ∏è SQL-Python Integration Environment Ready!\")\n",
    "print(f\"SQLAlchemy version: {sqlalchemy.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(\"Ready to bridge SQL and Python for business analytics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Business Data into Database\n",
    "\n",
    "First, let's load our Olist e-commerce data into a SQLite database. This simulates the common scenario where business data resides in a database system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_olist_data_to_database():\n",
    "    \"\"\"\n",
    "    Load Olist e-commerce data from zip file and store in SQLite database.\n",
    "    This simulates real-world scenario where business data resides in databases.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Download the Olist dataset\n",
    "        print(\"üì• Downloading Olist e-commerce dataset...\")\n",
    "        zip_url = \"https://github.com/autom8or-com/python-data-analysis-course/raw/main/Resources/data/sales.zip\"\n",
    "        response = requests.get(zip_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Create database connection\n",
    "        print(\"üóÑÔ∏è Creating SQLite database connection...\")\n",
    "        engine = create_engine(f\"sqlite:///{DATABASE_CONFIG['sqlite_path']}\", echo=DATABASE_CONFIG['echo'])\n",
    "        \n",
    "        datasets = {}\n",
    "        table_info = {}\n",
    "        \n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:\n",
    "            print(\"üìä Extracting and loading tables into database...\")\n",
    "            \n",
    "            # Define table mapping with business-friendly names\n",
    "            table_mapping = {\n",
    "                'customers': 'olist_customers_dataset.csv',\n",
    "                'orders': 'olist_orders_dataset.csv',\n",
    "                'order_items': 'olist_order_items_dataset.csv',\n",
    "                'products': 'olist_products_dataset.csv',\n",
    "                'sellers': 'olist_sellers_dataset.csv',\n",
    "                'payments': 'olist_order_payments_dataset.csv',\n",
    "                'reviews': 'olist_order_reviews_dataset.csv',\n",
    "                'geolocation': 'olist_geolocation_dataset.csv'\n",
    "            }\n",
    "            \n",
    "            for table_name, filename in table_mapping.items():\n",
    "                try:\n",
    "                    # Load CSV data\n",
    "                    df = pd.read_csv(zip_file.open(filename))\n",
    "                    \n",
    "                    # Data cleaning and optimization\n",
    "                    df = clean_data_for_database(df, table_name)\n",
    "                    \n",
    "                    # Store in database\n",
    "                    df.to_sql(table_name, engine, if_exists='replace', index=False, method='multi')\n",
    "                    \n",
    "                    # Track table information\n",
    "                    table_info[table_name] = {\n",
    "                        'rows': len(df),\n",
    "                        'columns': list(df.columns),\n",
    "                        'memory_usage': df.memory_usage().sum()\n",
    "                    }\n",
    "                    \n",
    "                    datasets[table_name] = df\n",
    "                    print(f\"  ‚úì {table_name}: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "                    \n",
    "                except KeyError:\n",
    "                    print(f\"  ‚ö† {filename} not found in zip file\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå Error loading {table_name}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Create indexes for better query performance\n",
    "        create_database_indexes(engine)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Database setup complete!\")\n",
    "        print(f\"üìÅ Database file: {DATABASE_CONFIG['sqlite_path']}\")\n",
    "        print(f\"üìä Tables loaded: {len(table_info)}\")\n",
    "        \n",
    "        return engine, datasets, table_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error setting up database: {e}\")\n",
    "        return create_sample_database()\n",
    "\n",
    "def clean_data_for_database(df, table_name):\n",
    "    \"\"\"\n",
    "    Clean and optimize data for database storage.\n",
    "    \"\"\"\n",
    "    # Convert date columns\n",
    "    date_columns = [col for col in df.columns if 'date' in col.lower() or 'timestamp' in col.lower()]\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "        else:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Optimize data types for storage\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            if df[col].max() < 3.4e38 and df[col].min() > -3.4e38:  # Float32 range\n",
    "                df[col] = df[col].astype('float32')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            if df[col].max() < 2147483647 and df[col].min() > -2147483648:  # Int32 range\n",
    "                df[col] = df[col].astype('int32')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_database_indexes(engine):\n",
    "    \"\"\"\n",
    "    Create indexes for better query performance on key business fields.\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        # Primary business entity indexes\n",
    "        indexes = [\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_orders_customer_id ON orders(customer_id)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_orders_purchase_date ON orders(order_purchase_timestamp)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_orders_status ON orders(order_status)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_order_items_order_id ON order_items(order_id)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_order_items_product_id ON order_items(product_id)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_customers_state ON customers(customer_state)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_products_category ON products(product_category_name)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_reviews_order_id ON reviews(order_id)\",\n",
    "            \"CREATE INDEX IF NOT EXISTS idx_reviews_score ON reviews(review_score)\"\n",
    "        ]\n",
    "        \n",
    "        for index_sql in indexes:\n",
    "            try:\n",
    "                conn.execute(text(index_sql))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not create index: {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"üóÉÔ∏è Database indexes created for optimal query performance\")\n",
    "\n",
    "def create_sample_database():\n",
    "    \"\"\"\n",
    "    Create sample database if real data loading fails.\n",
    "    \"\"\"\n",
    "    print(\"üìù Creating sample database for demonstration...\")\n",
    "    \n",
    "    engine = create_engine(f\"sqlite:///{DATABASE_CONFIG['sqlite_path']}\", echo=DATABASE_CONFIG['echo'])\n",
    "    \n",
    "    # Generate realistic sample data\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Sample customers\n",
    "    n_customers = 5000\n",
    "    customers = pd.DataFrame({\n",
    "        'customer_id': [f'cust_{i:06d}' for i in range(n_customers)],\n",
    "        'customer_unique_id': [f'uniq_{i:06d}' for i in range(n_customers)],\n",
    "        'customer_zip_code_prefix': np.random.randint(10000, 99999, n_customers),\n",
    "        'customer_city': np.random.choice(['S√£o Paulo', 'Rio de Janeiro', 'Belo Horizonte', \n",
    "                                         'Porto Alegre', 'Curitiba', 'Salvador'], n_customers),\n",
    "        'customer_state': np.random.choice(['SP', 'RJ', 'MG', 'RS', 'PR', 'BA'], n_customers)\n",
    "    })\n",
    "    \n",
    "    # Sample orders with business patterns\n",
    "    n_orders = 8000\n",
    "    start_date = datetime(2017, 1, 1)\n",
    "    end_date = datetime(2018, 8, 31)\n",
    "    \n",
    "    orders = pd.DataFrame({\n",
    "        'order_id': [f'ord_{i:08d}' for i in range(n_orders)],\n",
    "        'customer_id': np.random.choice(customers['customer_id'], n_orders),\n",
    "        'order_status': np.random.choice(['delivered', 'shipped', 'processing', 'canceled'], \n",
    "                                       n_orders, p=[0.85, 0.08, 0.05, 0.02]),\n",
    "        'order_purchase_timestamp': pd.date_range(start_date, end_date, periods=n_orders),\n",
    "        'order_approved_at': pd.date_range(start_date, end_date, periods=n_orders),\n",
    "        'order_delivered_carrier_date': pd.date_range(start_date + timedelta(days=2), \n",
    "                                                    end_date + timedelta(days=5), periods=n_orders),\n",
    "        'order_delivered_customer_date': pd.date_range(start_date + timedelta(days=5), \n",
    "                                                     end_date + timedelta(days=10), periods=n_orders),\n",
    "        'order_estimated_delivery_date': pd.date_range(start_date + timedelta(days=10), \n",
    "                                                      end_date + timedelta(days=15), periods=n_orders)\n",
    "    })\n",
    "    \n",
    "    # Sample products\n",
    "    categories = ['health_beauty', 'computers_accessories', 'furniture_decor', 'sports_leisure',\n",
    "                 'housewares', 'watches_gifts', 'telephony', 'auto', 'toys', 'cool_stuff']\n",
    "    \n",
    "    n_products = 3000\n",
    "    products = pd.DataFrame({\n",
    "        'product_id': [f'prod_{i:06d}' for i in range(n_products)],\n",
    "        'product_category_name': np.random.choice(categories, n_products),\n",
    "        'product_name_lenght': np.random.randint(10, 100, n_products),\n",
    "        'product_description_lenght': np.random.randint(50, 500, n_products),\n",
    "        'product_photos_qty': np.random.randint(1, 10, n_products),\n",
    "        'product_weight_g': np.random.lognormal(6, 1, n_products),\n",
    "        'product_length_cm': np.random.lognormal(3, 0.5, n_products),\n",
    "        'product_height_cm': np.random.lognormal(2.5, 0.5, n_products),\n",
    "        'product_width_cm': np.random.lognormal(3, 0.5, n_products)\n",
    "    })\n",
    "    \n",
    "    # Sample order items\n",
    "    n_items = 12000\n",
    "    order_items = pd.DataFrame({\n",
    "        'order_id': np.random.choice(orders['order_id'], n_items),\n",
    "        'order_item_id': np.random.randint(1, 5, n_items),\n",
    "        'product_id': np.random.choice(products['product_id'], n_items),\n",
    "        'seller_id': [f'sell_{i:04d}' for i in np.random.randint(0, 500, n_items)],\n",
    "        'shipping_limit_date': pd.date_range(start_date, end_date, periods=n_items),\n",
    "        'price': np.random.lognormal(3.5, 0.8, n_items),\n",
    "        'freight_value': np.random.exponential(15, n_items)\n",
    "    })\n",
    "    \n",
    "    # Sample reviews\n",
    "    n_reviews = int(n_orders * 0.8)\n",
    "    reviews = pd.DataFrame({\n",
    "        'review_id': [f'rev_{i:08d}' for i in range(n_reviews)],\n",
    "        'order_id': np.random.choice(orders['order_id'], n_reviews),\n",
    "        'review_score': np.random.choice([1, 2, 3, 4, 5], n_reviews, p=[0.05, 0.08, 0.15, 0.32, 0.40]),\n",
    "        'review_comment_title': ['Sample Title'] * n_reviews,\n",
    "        'review_comment_message': ['Sample Comment'] * n_reviews,\n",
    "        'review_creation_date': pd.date_range(start_date, end_date, periods=n_reviews),\n",
    "        'review_answer_timestamp': pd.date_range(start_date, end_date, periods=n_reviews)\n",
    "    })\n",
    "    \n",
    "    # Store all tables in database\n",
    "    datasets = {\n",
    "        'customers': customers,\n",
    "        'orders': orders,\n",
    "        'products': products,\n",
    "        'order_items': order_items,\n",
    "        'reviews': reviews\n",
    "    }\n",
    "    \n",
    "    table_info = {}\n",
    "    \n",
    "    for table_name, df in datasets.items():\n",
    "        df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "        table_info[table_name] = {\n",
    "            'rows': len(df),\n",
    "            'columns': list(df.columns),\n",
    "            'memory_usage': df.memory_usage().sum()\n",
    "        }\n",
    "        print(f\"  ‚úì {table_name}: {len(df):,} rows\")\n",
    "    \n",
    "    create_database_indexes(engine)\n",
    "    \n",
    "    return engine, datasets, table_info\n",
    "\n",
    "# Load data into database\n",
    "print(\"üöÄ Setting up Olist E-commerce Database...\")\n",
    "db_engine, raw_datasets, table_info = load_olist_data_to_database()\n",
    "\n",
    "print(f\"\\nüìã Database Summary:\")\n",
    "for table, info in table_info.items():\n",
    "    print(f\"  üìä {table}: {info['rows']:,} rows, {len(info['columns'])} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Connection Management\n",
    "\n",
    "Proper connection management is crucial for production applications. Let's explore different ways to connect to and interact with our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    \"\"\"\n",
    "    Professional database connection manager with proper error handling,\n",
    "    connection pooling, and resource management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, database_url=None):\n",
    "        self.database_url = database_url or f\"sqlite:///{DATABASE_CONFIG['sqlite_path']}\"\n",
    "        self.engine = None\n",
    "        self._connect()\n",
    "    \n",
    "    def _connect(self):\n",
    "        \"\"\"\n",
    "        Establish database connection with proper configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.engine = create_engine(\n",
    "                self.database_url,\n",
    "                echo=DATABASE_CONFIG['echo'],\n",
    "                pool_timeout=DATABASE_CONFIG['connection_timeout'],\n",
    "                pool_recycle=3600  # Recycle connections every hour\n",
    "            )\n",
    "            \n",
    "            # Test connection\n",
    "            with self.engine.connect() as conn:\n",
    "                conn.execute(text(\"SELECT 1\"))\n",
    "            \n",
    "            print(\"‚úÖ Database connection established successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Database connection failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_table_info(self):\n",
    "        \"\"\"\n",
    "        Get comprehensive information about all tables in the database.\n",
    "        \"\"\"\n",
    "        inspector = inspect(self.engine)\n",
    "        tables = inspector.get_table_names()\n",
    "        \n",
    "        table_info = {}\n",
    "        \n",
    "        for table in tables:\n",
    "            with self.engine.connect() as conn:\n",
    "                # Get row count\n",
    "                result = conn.execute(text(f\"SELECT COUNT(*) FROM {table}\"))\n",
    "                row_count = result.scalar()\n",
    "                \n",
    "                # Get column information\n",
    "                columns = inspector.get_columns(table)\n",
    "                \n",
    "                table_info[table] = {\n",
    "                    'rows': row_count,\n",
    "                    'columns': [col['name'] for col in columns],\n",
    "                    'column_types': {col['name']: str(col['type']) for col in columns}\n",
    "                }\n",
    "        \n",
    "        return table_info\n",
    "    \n",
    "    def execute_query(self, query, params=None):\n",
    "        \"\"\"\n",
    "        Execute a SQL query with proper error handling and return results as DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.engine.connect() as conn:\n",
    "                if params:\n",
    "                    result = pd.read_sql(text(query), conn, params=params)\n",
    "                else:\n",
    "                    result = pd.read_sql(text(query), conn)\n",
    "                \n",
    "                return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Query execution failed: {e}\")\n",
    "            print(f\"üìù Query: {query[:100]}...\")\n",
    "            raise\n",
    "    \n",
    "    def get_sample_data(self, table_name, limit=5):\n",
    "        \"\"\"\n",
    "        Get sample data from a table for exploration.\n",
    "        \"\"\"\n",
    "        query = f\"SELECT * FROM {table_name} LIMIT {limit}\"\n",
    "        return self.execute_query(query)\n",
    "    \n",
    "    def get_table_schema(self, table_name):\n",
    "        \"\"\"\n",
    "        Get detailed schema information for a specific table.\n",
    "        \"\"\"\n",
    "        inspector = inspect(self.engine)\n",
    "        columns = inspector.get_columns(table_name)\n",
    "        \n",
    "        schema_df = pd.DataFrame([\n",
    "            {\n",
    "                'column_name': col['name'],\n",
    "                'data_type': str(col['type']),\n",
    "                'nullable': col['nullable'],\n",
    "                'default': col.get('default'),\n",
    "                'primary_key': col.get('primary_key', False)\n",
    "            }\n",
    "            for col in columns\n",
    "        ])\n",
    "        \n",
    "        return schema_df\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Properly close database connections.\n",
    "        \"\"\"\n",
    "        if self.engine:\n",
    "            self.engine.dispose()\n",
    "            print(\"üîí Database connections closed\")\n",
    "\n",
    "# Create database manager instance\n",
    "db = DatabaseManager()\n",
    "\n",
    "# Display database information\n",
    "print(\"\\nüìä Database Overview:\")\n",
    "db_info = db.get_table_info()\n",
    "\n",
    "for table_name, info in db_info.items():\n",
    "    print(f\"\\nüóÉÔ∏è Table: {table_name}\")\n",
    "    print(f\"   üìè Rows: {info['rows']:,}\")\n",
    "    print(f\"   üìã Columns: {len(info['columns'])}\")\n",
    "    print(f\"   üè∑Ô∏è Key columns: {', '.join(info['columns'][:5])}{'...' if len(info['columns']) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic SQL Queries from Python\n",
    "\n",
    "Now let's execute SQL queries directly from Python and see how they compare to pandas operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with basic data exploration using SQL\n",
    "print(\"üîç Data Exploration: SQL vs Pandas Comparison\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Example 1: Basic SELECT statement\n",
    "print(\"\\nüìã Example 1: Basic Data Selection\")\n",
    "print(\"SQL Approach:\")\n",
    "\n",
    "# SQL query\n",
    "sql_customers = db.execute_query(\"\"\"\n",
    "    SELECT customer_id, customer_city, customer_state \n",
    "    FROM customers \n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL Result:\")\n",
    "display(sql_customers)\n",
    "\n",
    "print(\"\\nPandas Equivalent:\")\n",
    "pandas_customers = raw_datasets['customers'][['customer_id', 'customer_city', 'customer_state']].head(5)\n",
    "display(pandas_customers)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Filtering and counting\n",
    "print(\"\\nüìä Example 2: Filtering and Aggregation\")\n",
    "print(\"Business Question: How many orders by status?\")\n",
    "\n",
    "print(\"\\nSQL Approach:\")\n",
    "sql_order_status = db.execute_query(\"\"\"\n",
    "    SELECT \n",
    "        order_status,\n",
    "        COUNT(*) as order_count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "    FROM orders\n",
    "    GROUP BY order_status\n",
    "    ORDER BY order_count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL Result:\")\n",
    "display(sql_order_status)\n",
    "\n",
    "print(\"\\nPandas Equivalent:\")\n",
    "pandas_order_status = raw_datasets['orders']['order_status'].value_counts().reset_index()\n",
    "pandas_order_status.columns = ['order_status', 'order_count']\n",
    "pandas_order_status['percentage'] = (pandas_order_status['order_count'] / pandas_order_status['order_count'].sum() * 100).round(2)\n",
    "display(pandas_order_status)\n",
    "\n",
    "print(\"\\nüí° Observation: SQL provides more concise syntax for window functions!\")\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Complex business analysis with joins\n",
    "print(\"\\nüíº Example 3: Business Analysis with Multiple Tables\")\n",
    "print(\"Business Question: Revenue by product category\")\n",
    "\n",
    "print(\"\\nSQL Approach (with JOINs):\")\n",
    "sql_category_revenue = db.execute_query(\"\"\"\n",
    "    SELECT \n",
    "        p.product_category_name,\n",
    "        COUNT(DISTINCT oi.order_id) as total_orders,\n",
    "        SUM(oi.price) as total_revenue,\n",
    "        ROUND(AVG(oi.price), 2) as avg_item_price,\n",
    "        SUM(oi.freight_value) as total_freight\n",
    "    FROM order_items oi\n",
    "    JOIN products p ON oi.product_id = p.product_id\n",
    "    GROUP BY p.product_category_name\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL Result:\")\n",
    "display(sql_category_revenue)\n",
    "\n",
    "print(\"\\nPandas Equivalent (with merge):\")\n",
    "pandas_category_revenue = (\n",
    "    raw_datasets['order_items']\n",
    "    .merge(raw_datasets['products'], on='product_id')\n",
    "    .groupby('product_category_name')\n",
    "    .agg({\n",
    "        'order_id': 'nunique',\n",
    "        'price': ['sum', 'mean'],\n",
    "        'freight_value': 'sum'\n",
    "    })\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "pandas_category_revenue.columns = ['total_orders', 'total_revenue', 'avg_item_price', 'total_freight']\n",
    "pandas_category_revenue = pandas_category_revenue.sort_values('total_revenue', ascending=False).head(10).reset_index()\n",
    "\n",
    "display(pandas_category_revenue)\n",
    "\n",
    "print(\"\\nüí° Observation: SQL JOINs are often more readable than pandas merges!\")\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced SQL Queries for Business Intelligence\n",
    "\n",
    "Let's explore more sophisticated SQL queries that are common in business intelligence scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Intelligence Query 1: Customer Lifetime Value Analysis\n",
    "print(\"üí∞ Business Intelligence Analysis 1: Customer Lifetime Value\")\n",
    "print(\"\\nSQL Query: Customer spending patterns with statistical analysis\")\n",
    "\n",
    "customer_ltv_sql = \"\"\"\n",
    "WITH customer_metrics AS (\n",
    "    SELECT \n",
    "        o.customer_id,\n",
    "        c.customer_state,\n",
    "        COUNT(DISTINCT o.order_id) as total_orders,\n",
    "        SUM(oi.price + oi.freight_value) as total_spent,\n",
    "        AVG(oi.price + oi.freight_value) as avg_order_value,\n",
    "        MIN(o.order_purchase_timestamp) as first_order_date,\n",
    "        MAX(o.order_purchase_timestamp) as last_order_date,\n",
    "        JULIANDAY(MAX(o.order_purchase_timestamp)) - JULIANDAY(MIN(o.order_purchase_timestamp)) as customer_lifetime_days\n",
    "    FROM orders o\n",
    "    JOIN customers c ON o.customer_id = c.customer_id\n",
    "    JOIN order_items oi ON o.order_id = oi.order_id\n",
    "    WHERE o.order_status = 'delivered'\n",
    "    GROUP BY o.customer_id, c.customer_state\n",
    "),\n",
    "customer_segments AS (\n",
    "    SELECT *,\n",
    "        CASE \n",
    "            WHEN total_spent >= 500 THEN 'High Value'\n",
    "            WHEN total_spent >= 200 THEN 'Medium Value'\n",
    "            ELSE 'Low Value'\n",
    "        END as value_segment,\n",
    "        CASE \n",
    "            WHEN total_orders >= 3 THEN 'Frequent'\n",
    "            WHEN total_orders >= 2 THEN 'Regular'\n",
    "            ELSE 'One-time'\n",
    "        END as frequency_segment\n",
    "    FROM customer_metrics\n",
    ")\n",
    "SELECT \n",
    "    customer_state,\n",
    "    value_segment,\n",
    "    frequency_segment,\n",
    "    COUNT(*) as customer_count,\n",
    "    ROUND(AVG(total_spent), 2) as avg_customer_value,\n",
    "    ROUND(AVG(total_orders), 2) as avg_orders_per_customer,\n",
    "    ROUND(AVG(customer_lifetime_days), 1) as avg_lifetime_days\n",
    "FROM customer_segments\n",
    "GROUP BY customer_state, value_segment, frequency_segment\n",
    "ORDER BY customer_state, avg_customer_value DESC\n",
    "\"\"\"\n",
    "\n",
    "customer_ltv_results = db.execute_query(customer_ltv_sql)\n",
    "print(\"\\nüìä Customer Lifetime Value Analysis Results:\")\n",
    "display(customer_ltv_results.head(15))\n",
    "\n",
    "print(f\"\\nüìà Key Insights:\")\n",
    "total_customers = customer_ltv_results['customer_count'].sum()\n",
    "high_value_customers = customer_ltv_results[customer_ltv_results['value_segment'] == 'High Value']['customer_count'].sum()\n",
    "print(f\"  ‚Ä¢ Total customer segments analyzed: {total_customers:,}\")\n",
    "print(f\"  ‚Ä¢ High-value customers: {high_value_customers:,} ({high_value_customers/total_customers*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Average customer value across all segments: ${customer_ltv_results['avg_customer_value'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Intelligence Query 2: Sales Performance Trends\n",
    "print(\"\\nüìà Business Intelligence Analysis 2: Sales Performance Trends\")\n",
    "print(\"\\nSQL Query: Monthly sales trends with growth calculations\")\n",
    "\n",
    "sales_trends_sql = \"\"\"\n",
    "WITH monthly_sales AS (\n",
    "    SELECT \n",
    "        strftime('%Y', o.order_purchase_timestamp) as year,\n",
    "        strftime('%m', o.order_purchase_timestamp) as month,\n",
    "        COUNT(DISTINCT o.order_id) as total_orders,\n",
    "        COUNT(DISTINCT o.customer_id) as unique_customers,\n",
    "        SUM(oi.price) as total_revenue,\n",
    "        SUM(oi.freight_value) as total_freight,\n",
    "        ROUND(AVG(oi.price), 2) as avg_order_value\n",
    "    FROM orders o\n",
    "    JOIN order_items oi ON o.order_id = oi.order_id\n",
    "    WHERE o.order_status = 'delivered'\n",
    "        AND o.order_purchase_timestamp IS NOT NULL\n",
    "    GROUP BY strftime('%Y', o.order_purchase_timestamp), strftime('%m', o.order_purchase_timestamp)\n",
    "),\n",
    "sales_with_growth AS (\n",
    "    SELECT *,\n",
    "        total_revenue + total_freight as total_value,\n",
    "        LAG(total_revenue, 1) OVER (ORDER BY year, month) as prev_month_revenue,\n",
    "        LAG(total_orders, 1) OVER (ORDER BY year, month) as prev_month_orders\n",
    "    FROM monthly_sales\n",
    ")\n",
    "SELECT \n",
    "    year || '-' || printf('%02d', month) as period,\n",
    "    total_orders,\n",
    "    unique_customers,\n",
    "    ROUND(total_revenue, 2) as revenue,\n",
    "    ROUND(total_value, 2) as total_value,\n",
    "    avg_order_value,\n",
    "    CASE \n",
    "        WHEN prev_month_revenue IS NOT NULL THEN \n",
    "            ROUND((total_revenue - prev_month_revenue) / prev_month_revenue * 100, 2)\n",
    "        ELSE NULL \n",
    "    END as revenue_growth_pct,\n",
    "    CASE \n",
    "        WHEN prev_month_orders IS NOT NULL THEN \n",
    "            ROUND((total_orders - prev_month_orders) / CAST(prev_month_orders AS FLOAT) * 100, 2)\n",
    "        ELSE NULL \n",
    "    END as order_growth_pct\n",
    "FROM sales_with_growth\n",
    "ORDER BY year, month\n",
    "\"\"\"\n",
    "\n",
    "sales_trends_results = db.execute_query(sales_trends_sql)\n",
    "print(\"\\nüìä Monthly Sales Performance Trends:\")\n",
    "display(sales_trends_results)\n",
    "\n",
    "# Calculate key performance metrics\n",
    "avg_growth = sales_trends_results['revenue_growth_pct'].dropna().mean()\n",
    "best_month = sales_trends_results.loc[sales_trends_results['revenue'].idxmax()]\n",
    "total_revenue = sales_trends_results['revenue'].sum()\n",
    "\n",
    "print(f\"\\nüìà Performance Summary:\")\n",
    "print(f\"  ‚Ä¢ Total revenue across all periods: ${total_revenue:,.2f}\")\n",
    "print(f\"  ‚Ä¢ Average monthly revenue growth: {avg_growth:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Best performing month: {best_month['period']} (${best_month['revenue']:,.2f})\")\n",
    "print(f\"  ‚Ä¢ Total unique customers served: {sales_trends_results['unique_customers'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Intelligence Query 3: Product Performance Analysis\n",
    "print(\"\\nüõçÔ∏è Business Intelligence Analysis 3: Product Performance Matrix\")\n",
    "print(\"\\nSQL Query: Product category performance with customer satisfaction metrics\")\n",
    "\n",
    "product_performance_sql = \"\"\"\n",
    "WITH product_sales AS (\n",
    "    SELECT \n",
    "        p.product_category_name,\n",
    "        COUNT(DISTINCT oi.order_id) as total_orders,\n",
    "        COUNT(DISTINCT o.customer_id) as unique_customers,\n",
    "        SUM(oi.price) as total_revenue,\n",
    "        AVG(oi.price) as avg_item_price,\n",
    "        SUM(oi.freight_value) as total_freight,\n",
    "        AVG(oi.freight_value) as avg_freight_cost\n",
    "    FROM order_items oi\n",
    "    JOIN products p ON oi.product_id = p.product_id\n",
    "    JOIN orders o ON oi.order_id = o.order_id\n",
    "    WHERE o.order_status = 'delivered'\n",
    "    GROUP BY p.product_category_name\n",
    "),\n",
    "product_reviews AS (\n",
    "    SELECT \n",
    "        p.product_category_name,\n",
    "        AVG(CAST(r.review_score AS FLOAT)) as avg_rating,\n",
    "        COUNT(r.review_id) as total_reviews,\n",
    "        SUM(CASE WHEN r.review_score >= 4 THEN 1 ELSE 0 END) * 100.0 / COUNT(r.review_id) as satisfaction_rate\n",
    "    FROM reviews r\n",
    "    JOIN orders o ON r.order_id = o.order_id\n",
    "    JOIN order_items oi ON o.order_id = oi.order_id\n",
    "    JOIN products p ON oi.product_id = p.product_id\n",
    "    GROUP BY p.product_category_name\n",
    "),\n",
    "category_performance AS (\n",
    "    SELECT \n",
    "        ps.*,\n",
    "        COALESCE(pr.avg_rating, 0) as avg_rating,\n",
    "        COALESCE(pr.total_reviews, 0) as total_reviews,\n",
    "        COALESCE(pr.satisfaction_rate, 0) as satisfaction_rate,\n",
    "        (ps.total_revenue / ps.total_orders) as revenue_per_order,\n",
    "        (ps.total_revenue / ps.unique_customers) as revenue_per_customer\n",
    "    FROM product_sales ps\n",
    "    LEFT JOIN product_reviews pr ON ps.product_category_name = pr.product_category_name\n",
    ")\n",
    "SELECT \n",
    "    product_category_name,\n",
    "    total_orders,\n",
    "    unique_customers,\n",
    "    ROUND(total_revenue, 2) as total_revenue,\n",
    "    ROUND(avg_item_price, 2) as avg_item_price,\n",
    "    ROUND(avg_freight_cost, 2) as avg_freight_cost,\n",
    "    ROUND(avg_rating, 2) as avg_rating,\n",
    "    total_reviews,\n",
    "    ROUND(satisfaction_rate, 1) as satisfaction_rate,\n",
    "    ROUND(revenue_per_order, 2) as revenue_per_order,\n",
    "    ROUND(revenue_per_customer, 2) as revenue_per_customer,\n",
    "    CASE \n",
    "        WHEN total_revenue >= 50000 AND avg_rating >= 4.0 THEN 'Star Performer'\n",
    "        WHEN total_revenue >= 50000 AND avg_rating < 4.0 THEN 'High Volume, Low Satisfaction'\n",
    "        WHEN total_revenue < 50000 AND avg_rating >= 4.0 THEN 'High Satisfaction, Low Volume'\n",
    "        ELSE 'Needs Attention'\n",
    "    END as performance_category\n",
    "FROM category_performance\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "product_performance_results = db.execute_query(product_performance_sql)\n",
    "print(\"\\nüìä Product Category Performance Matrix:\")\n",
    "display(product_performance_results)\n",
    "\n",
    "# Analyze performance categories\n",
    "performance_summary = product_performance_results['performance_category'].value_counts()\n",
    "print(f\"\\nüéØ Performance Category Distribution:\")\n",
    "for category, count in performance_summary.items():\n",
    "    print(f\"  ‚Ä¢ {category}: {count} categories\")\n",
    "\n",
    "# Top performers\n",
    "top_revenue = product_performance_results.nlargest(3, 'total_revenue')\n",
    "top_satisfaction = product_performance_results.nlargest(3, 'avg_rating')\n",
    "\n",
    "print(f\"\\nüèÜ Top 3 Categories by Revenue:\")\n",
    "for _, row in top_revenue.iterrows():\n",
    "    print(f\"  ‚Ä¢ {row['product_category_name']}: ${row['total_revenue']:,.2f} (Rating: {row['avg_rating']:.2f})\")\n",
    "\n",
    "print(f\"\\n‚≠ê Top 3 Categories by Customer Satisfaction:\")\n",
    "for _, row in top_satisfaction.iterrows():\n",
    "    print(f\"  ‚Ä¢ {row['product_category_name']}: {row['avg_rating']:.2f} rating (Revenue: ${row['total_revenue']:,.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQL vs Pandas: Performance and Use Case Comparison\n",
    "\n",
    "Let's compare the performance and use cases of SQL versus pandas for different types of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def compare_sql_vs_pandas():\n",
    "    \"\"\"\n",
    "    Compare performance and readability of SQL vs pandas operations.\n",
    "    \"\"\"\n",
    "    print(\"‚ö° SQL vs Pandas Performance Comparison\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Test 1: Simple aggregation\n",
    "    print(\"\\nüìä Test 1: Simple Aggregation (Order counts by state)\")\n",
    "    \n",
    "    # SQL approach\n",
    "    start_time = time.time()\n",
    "    sql_result = db.execute_query(\"\"\"\n",
    "        SELECT c.customer_state, COUNT(o.order_id) as order_count\n",
    "        FROM orders o\n",
    "        JOIN customers c ON o.customer_id = c.customer_id\n",
    "        GROUP BY c.customer_state\n",
    "        ORDER BY order_count DESC\n",
    "    \"\"\")\n",
    "    sql_time = time.time() - start_time\n",
    "    \n",
    "    # Pandas approach\n",
    "    start_time = time.time()\n",
    "    pandas_result = (\n",
    "        raw_datasets['orders']\n",
    "        .merge(raw_datasets['customers'], on='customer_id')\n",
    "        .groupby('customer_state')['order_id']\n",
    "        .count()\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    pandas_result.columns = ['customer_state', 'order_count']\n",
    "    pandas_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  SQL Time: {sql_time:.4f} seconds\")\n",
    "    print(f\"  Pandas Time: {pandas_time:.4f} seconds\")\n",
    "    print(f\"  Winner: {'SQL' if sql_time < pandas_time else 'Pandas'} ({min(sql_time, pandas_time)/max(sql_time, pandas_time)*100:.1f}% faster)\")\n",
    "    \n",
    "    # Test 2: Complex multi-table join with calculations\n",
    "    print(\"\\nüíº Test 2: Complex Business Query (Revenue by category with metrics)\")\n",
    "    \n",
    "    # SQL approach\n",
    "    start_time = time.time()\n",
    "    sql_complex = db.execute_query(\"\"\"\n",
    "        SELECT \n",
    "            p.product_category_name,\n",
    "            COUNT(DISTINCT o.order_id) as orders,\n",
    "            SUM(oi.price + oi.freight_value) as total_value,\n",
    "            AVG(oi.price + oi.freight_value) as avg_order_value,\n",
    "            COUNT(DISTINCT o.customer_id) as unique_customers\n",
    "        FROM orders o\n",
    "        JOIN order_items oi ON o.order_id = oi.order_id\n",
    "        JOIN products p ON oi.product_id = p.product_id\n",
    "        WHERE o.order_status = 'delivered'\n",
    "        GROUP BY p.product_category_name\n",
    "        ORDER BY total_value DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    sql_complex_time = time.time() - start_time\n",
    "    \n",
    "    # Pandas approach\n",
    "    start_time = time.time()\n",
    "    pandas_complex = (\n",
    "        raw_datasets['orders']\n",
    "        [raw_datasets['orders']['order_status'] == 'delivered']\n",
    "        .merge(raw_datasets['order_items'], on='order_id')\n",
    "        .merge(raw_datasets['products'], on='product_id')\n",
    "    )\n",
    "    pandas_complex['total_item_value'] = pandas_complex['price'] + pandas_complex['freight_value']\n",
    "    \n",
    "    pandas_complex_result = (\n",
    "        pandas_complex\n",
    "        .groupby('product_category_name')\n",
    "        .agg({\n",
    "            'order_id': ['nunique', 'count'],\n",
    "            'total_item_value': ['sum', 'mean'],\n",
    "            'customer_id': 'nunique'\n",
    "        })\n",
    "    )\n",
    "    pandas_complex_result.columns = ['orders', 'items', 'total_value', 'avg_order_value', 'unique_customers']\n",
    "    pandas_complex_result = pandas_complex_result.sort_values('total_value', ascending=False).head(10)\n",
    "    pandas_complex_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  SQL Time: {sql_complex_time:.4f} seconds\")\n",
    "    print(f\"  Pandas Time: {pandas_complex_time:.4f} seconds\")\n",
    "    print(f\"  Winner: {'SQL' if sql_complex_time < pandas_complex_time else 'Pandas'} ({min(sql_complex_time, pandas_complex_time)/max(sql_complex_time, pandas_complex_time)*100:.1f}% faster)\")\n",
    "    \n",
    "    # Test 3: Window functions and advanced analytics\n",
    "    print(\"\\nüìà Test 3: Advanced Analytics (Running totals and rankings)\")\n",
    "    \n",
    "    # SQL approach (much more concise for window functions)\n",
    "    start_time = time.time()\n",
    "    sql_window = db.execute_query(\"\"\"\n",
    "        WITH monthly_revenue AS (\n",
    "            SELECT \n",
    "                strftime('%Y-%m', o.order_purchase_timestamp) as month,\n",
    "                SUM(oi.price) as monthly_revenue\n",
    "            FROM orders o\n",
    "            JOIN order_items oi ON o.order_id = oi.order_id\n",
    "            WHERE o.order_status = 'delivered'\n",
    "            GROUP BY strftime('%Y-%m', o.order_purchase_timestamp)\n",
    "        )\n",
    "        SELECT \n",
    "            month,\n",
    "            monthly_revenue,\n",
    "            SUM(monthly_revenue) OVER (ORDER BY month) as running_total,\n",
    "            LAG(monthly_revenue, 1) OVER (ORDER BY month) as prev_month,\n",
    "            RANK() OVER (ORDER BY monthly_revenue DESC) as revenue_rank\n",
    "        FROM monthly_revenue\n",
    "        ORDER BY month\n",
    "    \"\"\")\n",
    "    sql_window_time = time.time() - start_time\n",
    "    \n",
    "    # Pandas approach (more verbose for window functions)\n",
    "    start_time = time.time()\n",
    "    pandas_window_data = (\n",
    "        raw_datasets['orders']\n",
    "        [raw_datasets['orders']['order_status'] == 'delivered']\n",
    "        .merge(raw_datasets['order_items'], on='order_id')\n",
    "    )\n",
    "    pandas_window_data['order_purchase_timestamp'] = pd.to_datetime(pandas_window_data['order_purchase_timestamp'])\n",
    "    pandas_window_data['month'] = pandas_window_data['order_purchase_timestamp'].dt.to_period('M')\n",
    "    \n",
    "    monthly_pandas = pandas_window_data.groupby('month')['price'].sum().reset_index()\n",
    "    monthly_pandas.columns = ['month', 'monthly_revenue']\n",
    "    monthly_pandas['month'] = monthly_pandas['month'].astype(str)\n",
    "    monthly_pandas = monthly_pandas.sort_values('month')\n",
    "    \n",
    "    monthly_pandas['running_total'] = monthly_pandas['monthly_revenue'].cumsum()\n",
    "    monthly_pandas['prev_month'] = monthly_pandas['monthly_revenue'].shift(1)\n",
    "    monthly_pandas['revenue_rank'] = monthly_pandas['monthly_revenue'].rank(ascending=False, method='min')\n",
    "    pandas_window_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  SQL Time: {sql_window_time:.4f} seconds\")\n",
    "    print(f\"  Pandas Time: {pandas_window_time:.4f} seconds\")\n",
    "    print(f\"  Winner: {'SQL' if sql_window_time < pandas_window_time else 'Pandas'} ({min(sql_window_time, pandas_window_time)/max(sql_window_time, pandas_window_time)*100:.1f}% faster)\")\n",
    "    \n",
    "    print(\"\\nüìã Sample Results from Advanced Analytics:\")\n",
    "    display(sql_window.head(8))\n",
    "    \n",
    "    return {\n",
    "        'simple_agg': {'sql': sql_time, 'pandas': pandas_time},\n",
    "        'complex_join': {'sql': sql_complex_time, 'pandas': pandas_complex_time},\n",
    "        'window_functions': {'sql': sql_window_time, 'pandas': pandas_window_time}\n",
    "    }\n",
    "\n",
    "# Run performance comparison\n",
    "performance_results = compare_sql_vs_pandas()\n",
    "\n",
    "print(\"\\nüéØ Performance Summary:\")\n",
    "for test_name, times in performance_results.items():\n",
    "    faster = 'SQL' if times['sql'] < times['pandas'] else 'Pandas'\n",
    "    ratio = min(times['sql'], times['pandas']) / max(times['sql'], times['pandas'])\n",
    "    print(f\"  ‚Ä¢ {test_name.replace('_', ' ').title()}: {faster} wins ({ratio*100:.1f}% faster)\")\n",
    "\n",
    "print(\"\\nüí° Key Takeaways:\")\n",
    "print(\"  ‚Ä¢ SQL excels at: Complex joins, window functions, set operations\")\n",
    "print(\"  ‚Ä¢ Pandas excels at: Data cleaning, statistical analysis, visualization prep\")\n",
    "print(\"  ‚Ä¢ Best practice: Use SQL for data extraction, pandas for analysis\")\n",
    "print(\"  ‚Ä¢ Performance varies by dataset size and query complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Handling and Best Practices\n",
    "\n",
    "Production database applications require robust error handling and connection management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_error_handling():\n",
    "    \"\"\"\n",
    "    Demonstrate proper error handling techniques for database operations.\n",
    "    \"\"\"\n",
    "    print(\"üõ°Ô∏è Database Error Handling and Best Practices\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Example 1: Handling SQL syntax errors\n",
    "    print(\"\\n‚ùå Example 1: SQL Syntax Error Handling\")\n",
    "    try:\n",
    "        # Intentional syntax error\n",
    "        result = db.execute_query(\"\"\"\n",
    "            SELCT * FROM orders  -- Missing 'E' in SELECT\n",
    "            WHERE order_status = 'delivered'\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úÖ Caught SQL syntax error: {type(e).__name__}\")\n",
    "        print(f\"   Error message: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Example 2: Handling table/column not found\n",
    "    print(\"\\nüîç Example 2: Table/Column Not Found Error\")\n",
    "    try:\n",
    "        result = db.execute_query(\"\"\"\n",
    "            SELECT customer_id, nonexistent_column \n",
    "            FROM customers\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úÖ Caught column error: {type(e).__name__}\")\n",
    "        print(f\"   Error message: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Example 3: Parameterized queries (SQL injection prevention)\n",
    "    print(\"\\nüîí Example 3: Safe Parameterized Queries\")\n",
    "    \n",
    "    def safe_customer_lookup(customer_state):\n",
    "        \"\"\"\n",
    "        Safely query customers by state using parameterized queries.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "                SELECT customer_id, customer_city, customer_state\n",
    "                FROM customers \n",
    "                WHERE customer_state = :state\n",
    "                LIMIT 5\n",
    "            \"\"\"\n",
    "            result = db.execute_query(query, params={'state': customer_state})\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Query failed: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # Test safe query\n",
    "    safe_result = safe_customer_lookup('SP')\n",
    "    print(f\"‚úÖ Safe query returned {len(safe_result)} customers from SP state\")\n",
    "    \n",
    "    # Example 4: Connection management with context managers\n",
    "    print(\"\\nüîå Example 4: Proper Connection Management\")\n",
    "    \n",
    "    class SafeDatabaseQuery:\n",
    "        \"\"\"\n",
    "        Context manager for safe database operations.\n",
    "        \"\"\"\n",
    "        def __init__(self, engine):\n",
    "            self.engine = engine\n",
    "            self.connection = None\n",
    "        \n",
    "        def __enter__(self):\n",
    "            self.connection = self.engine.connect()\n",
    "            return self.connection\n",
    "        \n",
    "        def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "            if self.connection:\n",
    "                self.connection.close()\n",
    "            if exc_type:\n",
    "                print(f\"‚ùå Database error occurred: {exc_type.__name__}: {exc_val}\")\n",
    "            return False  # Don't suppress exceptions\n",
    "    \n",
    "    # Use context manager for safe operations\n",
    "    try:\n",
    "        with SafeDatabaseQuery(db.engine) as conn:\n",
    "            result = pd.read_sql(\n",
    "                text(\"SELECT COUNT(*) as total_orders FROM orders\"), \n",
    "                conn\n",
    "            )\n",
    "            print(f\"‚úÖ Context manager query successful: {result.iloc[0, 0]:,} total orders\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Context manager caught error: {e}\")\n",
    "    \n",
    "    # Example 5: Data validation\n",
    "    print(\"\\n‚úÖ Example 5: Data Validation Best Practices\")\n",
    "    \n",
    "    def validate_query_result(df, expected_columns=None, min_rows=0):\n",
    "        \"\"\"\n",
    "        Validate query results meet business requirements.\n",
    "        \"\"\"\n",
    "        validations = []\n",
    "        \n",
    "        # Check if DataFrame is empty\n",
    "        if df.empty:\n",
    "            validations.append(\"‚ùå Query returned no data\")\n",
    "        else:\n",
    "            validations.append(f\"‚úÖ Query returned {len(df):,} rows\")\n",
    "        \n",
    "        # Check minimum row count\n",
    "        if len(df) < min_rows:\n",
    "            validations.append(f\"‚ö†Ô∏è Row count ({len(df)}) below minimum ({min_rows})\")\n",
    "        \n",
    "        # Check expected columns\n",
    "        if expected_columns:\n",
    "            missing_cols = set(expected_columns) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                validations.append(f\"‚ùå Missing columns: {missing_cols}\")\n",
    "            else:\n",
    "                validations.append(\"‚úÖ All expected columns present\")\n",
    "        \n",
    "        # Check for null values in key columns\n",
    "        null_counts = df.isnull().sum()\n",
    "        if null_counts.any():\n",
    "            validations.append(f\"‚ö†Ô∏è Null values found: {null_counts[null_counts > 0].to_dict()}\")\n",
    "        else:\n",
    "            validations.append(\"‚úÖ No null values detected\")\n",
    "        \n",
    "        return validations\n",
    "    \n",
    "    # Test validation\n",
    "    test_query = db.execute_query(\"\"\"\n",
    "        SELECT customer_id, customer_state, customer_city\n",
    "        FROM customers\n",
    "        LIMIT 100\n",
    "    \"\"\")\n",
    "    \n",
    "    validations = validate_query_result(\n",
    "        test_query, \n",
    "        expected_columns=['customer_id', 'customer_state', 'customer_city'],\n",
    "        min_rows=50\n",
    "    )\n",
    "    \n",
    "    print(\"Query validation results:\")\n",
    "    for validation in validations:\n",
    "        print(f\"  {validation}\")\n",
    "    \n",
    "    return validations\n",
    "\n",
    "# Run error handling demonstration\n",
    "error_handling_results = demonstrate_error_handling()\n",
    "\n",
    "print(\"\\nüìö Database Best Practices Summary:\")\n",
    "print(\"  üîí Always use parameterized queries to prevent SQL injection\")\n",
    "print(\"  üõ°Ô∏è Implement comprehensive error handling for all database operations\")\n",
    "print(\"  üîå Use connection context managers to ensure proper resource cleanup\")\n",
    "print(\"  ‚úÖ Validate query results before processing in business logic\")\n",
    "print(\"  üìä Log query performance for optimization opportunities\")\n",
    "print(\"  üîÑ Implement retry logic for transient connection issues\")\n",
    "print(\"  üìù Document query patterns and business logic for team maintenance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways and Next Steps\n",
    "\n",
    "### What We've Accomplished:\n",
    "\n",
    "1. **Database Setup and Connection Management**\n",
    "   - Loaded real business data into SQLite database\n",
    "   - Established professional connection patterns\n",
    "   - Implemented proper resource management\n",
    "\n",
    "2. **SQL vs Pandas Comparison**\n",
    "   - Basic queries and aggregations\n",
    "   - Complex business intelligence analysis\n",
    "   - Performance trade-offs and use cases\n",
    "\n",
    "3. **Advanced SQL Techniques**\n",
    "   - Window functions for business analytics\n",
    "   - Common Table Expressions (CTEs)\n",
    "   - Complex joins and subqueries\n",
    "\n",
    "4. **Production-Ready Practices**\n",
    "   - Error handling and validation\n",
    "   - Parameterized queries for security\n",
    "   - Connection pooling and resource management\n",
    "\n",
    "### Business Value:\n",
    "\n",
    "- **Performance**: Leverage database engines for heavy computation\n",
    "- **Scalability**: Handle datasets larger than memory\n",
    "- **Integration**: Connect to existing business systems\n",
    "- **Security**: Proper authentication and query sanitization\n",
    "- **Maintenance**: Centralized business logic in database layer\n",
    "\n",
    "### When to Use SQL vs Pandas:\n",
    "\n",
    "**Use SQL for:**\n",
    "- Data extraction and filtering\n",
    "- Complex joins across multiple tables\n",
    "- Window functions and analytical queries\n",
    "- Set operations (UNION, INTERSECT, EXCEPT)\n",
    "- Large dataset aggregations\n",
    "\n",
    "**Use Pandas for:**\n",
    "- Data cleaning and transformation\n",
    "- Statistical analysis and modeling\n",
    "- Visualization preparation\n",
    "- Iterative data exploration\n",
    "- Machine learning feature engineering\n",
    "\n",
    "### Next Session Preview:\n",
    "In Part 2, we'll explore:\n",
    "- Advanced SQL query patterns for business intelligence\n",
    "- Real-time data integration techniques\n",
    "- Combining SQL results with pandas analysis\n",
    "- Building automated reporting pipelines\n",
    "\n",
    "**üéâ You now have the fundamental skills to bridge SQL databases with Python analytics for powerful business intelligence solutions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practice Exercise\n",
    "\n",
    "**Your Challenge! üöÄ**\n",
    "\n",
    "**Business Scenario**: The Olist marketing team wants to understand customer behavior patterns to improve their email marketing campaigns.\n",
    "\n",
    "**Your Task**: Create a SQL query that identifies:\n",
    "1. Customers who have made multiple purchases\n",
    "2. Their average time between orders\n",
    "3. Their preferred product categories\n",
    "4. Their average order value trend over time\n",
    "\n",
    "**Requirements**:\n",
    "- Use proper JOINs to combine multiple tables\n",
    "- Include error handling for your query\n",
    "- Validate your results\n",
    "- Compare with an equivalent pandas approach\n",
    "\n",
    "**Deliverable**: A customer segmentation analysis that the marketing team can use to personalize their campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your practice exercise solution here\n",
    "\n",
    "def analyze_customer_behavior_patterns():\n",
    "    \"\"\"\n",
    "    Your challenge: Create a comprehensive customer behavior analysis using SQL.\n",
    "    \n",
    "    Business Goal: Help marketing team understand customer patterns for better campaigns.\n",
    "    \n",
    "    Requirements:\n",
    "    1. Multi-purchase customers analysis\n",
    "    2. Average time between orders\n",
    "    3. Preferred product categories\n",
    "    4. Order value trends\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Write your SQL query\n",
    "    customer_behavior_sql = \"\"\"\n",
    "    -- Your SQL query here\n",
    "    -- Think about:\n",
    "    -- - Which tables need to be joined?\n",
    "    -- - How to calculate time between orders?\n",
    "    -- - How to identify preferred categories?\n",
    "    -- - How to track value trends?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 2: Execute with error handling\n",
    "    try:\n",
    "        # Your query execution here\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Query failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Validate results\n",
    "    # Your validation logic here\n",
    "    \n",
    "    # Step 4: Compare with pandas approach\n",
    "    # Your pandas equivalent here\n",
    "    \n",
    "    # Step 5: Generate marketing insights\n",
    "    # Your business insights here\n",
    "    \n",
    "    return None  # Return your results\n",
    "\n",
    "print(\"üéØ Customer Behavior Analysis Challenge\")\n",
    "print(\"\\nüìß Marketing Use Case:\")\n",
    "print(\"Help the marketing team understand customer patterns for email campaigns\")\n",
    "print(\"\\nüîç Analysis Requirements:\")\n",
    "print(\"  1. Multi-purchase customer identification\")\n",
    "print(\"  2. Purchase frequency patterns\")\n",
    "print(\"  3. Category preferences\")\n",
    "print(\"  4. Value trend analysis\")\n",
    "print(\"\\nüí° Think about:\")\n",
    "print(\"  ‚Ä¢ Which customers are most valuable for retention campaigns?\")\n",
    "print(\"  ‚Ä¢ What product recommendations would be most effective?\")\n",
    "print(\"  ‚Ä¢ How can we identify customers at risk of churning?\")\n",
    "\n",
    "# Uncomment to run your solution:\n",
    "# customer_analysis_results = analyze_customer_behavior_patterns()\n",
    "\n",
    "# Clean up database connection\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3\",\n",
   "language": "python\",\n",
   "name": "python3\"\n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython\",\n",
    "version": 3
   },
   "file_extension": ".py\",\n",
   "mimetype": "text/x-python\",\n",
   "name": "python\",\n",
   "nbconvert_exporter": "python\",\n",
   "pygments_lexer": "ipython3\",\n",
   "version": "3.8.5\"\n"
  },
 "nbformat": 4,
 "nbformat_minor": 4
}}