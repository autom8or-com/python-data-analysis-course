{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pandas Fundamentals II - Part 3: Handling Missing Values\n",
        "\n",
        "## Week 3, Day 1 (Wednesday) - April 23rd, 2025\n",
        "\n",
        "### Overview\n",
        "This is the third part of our Pandas Fundamentals II session, focusing on handling missing values. Missing data is a common challenge in real-world datasets, and properly identifying and addressing these gaps is crucial for accurate analysis.\n",
        "\n",
        "### Learning Objectives\n",
        "- Understand how Pandas represents missing values\n",
        "- Learn various methods to detect missing data\n",
        "- Master different strategies for handling missing values\n",
        "- Apply appropriate techniques based on the nature of the data\n",
        "- Understand the implications of different missing data handling approaches\n",
        "\n",
        "### Prerequisites\n",
        "- Python fundamentals (Week 1)\n",
        "- Pandas Fundamentals I (Week 2, Day 2)\n",
        "- Indexing and Selection (Week 3, Part 1)\n",
        "- Filtering Data (Week 3, Part 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Missing Data in Pandas\n",
        "\n",
        "Missing data is represented in Pandas primarily in two ways:\n",
        "1. `NaN` (Not a Number) - Used for numeric data and is part of the NumPy library\n",
        "2. `None` - Python's built-in object for representing missing data\n",
        "\n",
        "Let's create a dataset with missing values to explore different handling techniques:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a sample DataFrame with missing values\n",
        "data = {\n",
        "    'customer_id': ['C001', 'C002', 'C003', 'C004', 'C005', 'C006', 'C007', 'C008'],\n",
        "    'name': ['John Smith', 'Emily Davis', 'Michael Johnson', None, 'Sarah Wilson', 'David Brown', None, 'Lisa Anderson'],\n",
        "    'age': [28, 34, np.nan, 45, 31, np.nan, 39, 42],\n",
        "    'email': ['john@example.com', None, 'michael@example.com', 'robert@example.com', None, 'david@example.com', 'james@example.com', 'lisa@example.com'],\n",
        "    'purchases': [5, 12, 8, np.nan, 15, 7, 3, np.nan],\n",
        "    'last_purchase_date': [pd.Timestamp('2025-01-15'), pd.Timestamp('2025-02-20'), None, pd.Timestamp('2025-01-30'), pd.Timestamp('2025-03-05'), None, pd.Timestamp('2025-02-10'), pd.Timestamp('2025-03-15')],\n",
        "    'loyalty_score': [4.5, 3.8, 4.2, None, 4.9, 3.2, None, 4.7]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "customers_df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Sample Customer DataFrame with Missing Values:\")\n",
        "print(customers_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Detecting Missing Values\n",
        "\n",
        "Before handling missing values, we need to detect them. Pandas provides several methods to identify missing data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values with .isna() or .isnull() (they're identical)\n",
        "missing_values = customers_df.isna()\n",
        "print(\"Boolean mask of missing values:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Count missing values in each column\n",
        "missing_count = customers_df.isna().sum()\n",
        "print(\"\\nCount of missing values per column:\")\n",
        "print(missing_count)\n",
        "\n",
        "# Check for rows with any missing values\n",
        "rows_with_missing = customers_df[customers_df.isna().any(axis=1)]\n",
        "print(\"\\nRows with at least one missing value:\")\n",
        "print(rows_with_missing)\n",
        "\n",
        "# Calculate percentage of missing values per column\n",
        "percent_missing = customers_df.isna().mean() * 100\n",
        "print(\"\\nPercentage of missing values per column:\")\n",
        "print(percent_missing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Missing Data\n",
        "\n",
        "It can be helpful to visualize missing data patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a heatmap of missing values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(customers_df.isna(), cmap='viridis', aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.xticks(range(len(customers_df.columns)), customers_df.columns, rotation=45)\n",
        "plt.yticks(range(len(customers_df)), customers_df.index)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot percentage of missing values per column\n",
        "plt.figure(figsize=(10, 6))\n",
        "percent_missing.plot(kind='bar')\n",
        "plt.title('Percentage of Missing Values by Column')\n",
        "plt.ylabel('Percentage')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Understanding Types of Missing Data\n",
        "\n",
        "Missing data can be categorized into three types:\n",
        "\n",
        "1. **Missing Completely at Random (MCAR):** The probability of missing is the same for all observations\n",
        "2. **Missing at Random (MAR):** The probability of missing depends on observed data\n",
        "3. **Missing Not at Random (MNAR):** The probability of missing depends on unobserved data\n",
        "\n",
        "Understanding the type of missing data helps determine the appropriate handling strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Handling Missing Values: Deletion Methods\n",
        "\n",
        "One approach to handling missing data is to delete rows or columns with missing values. This is appropriate when missing data is minimal or when the missing data is MCAR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy of the DataFrame to avoid modifying the original\n",
        "df_copy = customers_df.copy()\n",
        "\n",
        "# Drop rows with any missing values (listwise deletion)\n",
        "df_dropna_rows = df_copy.dropna()\n",
        "print(\"DataFrame after dropping rows with missing values:\")\n",
        "print(df_dropna_rows)\n",
        "print(f\"Original shape: {df_copy.shape}, New shape: {df_dropna_rows.shape}\")\n",
        "\n",
        "# Drop rows only if all columns are missing (rarely used)\n",
        "df_dropna_all = df_copy.dropna(how='all')\n",
        "print(\"\\nDataFrame after dropping rows where all values are missing:\")\n",
        "print(f\"Original shape: {df_copy.shape}, New shape: {df_dropna_all.shape}\")\n",
        "\n",
        "# Drop rows with missing values in specific columns\n",
        "df_dropna_subset = df_copy.dropna(subset=['name', 'email'])\n",
        "print(\"\\nDataFrame after dropping rows with missing 'name' or 'email':\")\n",
        "print(df_dropna_subset)\n",
        "\n",
        "# Drop columns with missing values\n",
        "df_dropna_cols = df_copy.dropna(axis=1)\n",
        "print(\"\\nDataFrame after dropping columns with any missing values:\")\n",
        "print(df_dropna_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### When to use deletion methods:\n",
        "\n",
        "- **Pros:**\n",
        "  - Simple and quick to implement\n",
        "  - No risk of introducing bias if data is MCAR\n",
        "  - Ensures complete cases for analyses that require them\n",
        "  \n",
        "- **Cons:**\n",
        "  - Can significantly reduce sample size\n",
        "  - May introduce bias if data is MAR or MNAR\n",
        "  - Wasteful of potentially useful data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Handling Missing Values: Imputation Methods\n",
        "\n",
        "Imputation involves replacing missing values with estimated values. There are several methods for imputation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Fill with a specific value\n",
        "df_fill_specific = customers_df.copy()\n",
        "df_fill_specific['age'] = df_fill_specific['age'].fillna(0)\n",
        "df_fill_specific['name'] = df_fill_specific['name'].fillna('Unknown')\n",
        "print(\"DataFrame after filling specific values:\")\n",
        "print(df_fill_specific[['name', 'age']])\n",
        "\n",
        "# 2. Fill with statistical measures\n",
        "df_fill_stats = customers_df.copy()\n",
        "df_fill_stats['age'] = df_fill_stats['age'].fillna(df_fill_stats['age'].mean())  # Mean\n",
        "df_fill_stats['purchases'] = df_fill_stats['purchases'].fillna(df_fill_stats['purchases'].median())  # Median\n",
        "df_fill_stats['loyalty_score'] = df_fill_stats['loyalty_score'].fillna(df_fill_stats['loyalty_score'].mode()[0])  # Mode\n",
        "print(\"\\nDataFrame after filling with statistical measures:\")\n",
        "print(df_fill_stats[['age', 'purchases', 'loyalty_score']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Forward fill (uses previous valid value)\n",
        "df_ffill = customers_df.copy().sort_values('customer_id').reset_index(drop=True)\n",
        "df_ffill_result = df_ffill.fillna(method='ffill')\n",
        "print(\"DataFrame after forward fill:\")\n",
        "print(df_ffill_result)\n",
        "\n",
        "# 4. Backward fill (uses next valid value)\n",
        "df_bfill_result = df_ffill.fillna(method='bfill')\n",
        "print(\"\\nDataFrame after backward fill:\")\n",
        "print(df_bfill_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Interpolation (linear, polynomial, etc.)\n",
        "# First create a DataFrame with time series data that has missing values\n",
        "dates = pd.date_range('2025-01-01', periods=10, freq='D')\n",
        "values = [10, 11, np.nan, np.nan, 15, 16, np.nan, 19, np.nan, 22]\n",
        "time_series = pd.Series(values, index=dates)\n",
        "\n",
        "print(\"Original time series with missing values:\")\n",
        "print(time_series)\n",
        "\n",
        "# Linear interpolation\n",
        "linear_interp = time_series.interpolate(method='linear')\n",
        "print(\"\\nTime series after linear interpolation:\")\n",
        "print(linear_interp)\n",
        "\n",
        "# Polynomial interpolation\n",
        "poly_interp = time_series.interpolate(method='polynomial', order=2)\n",
        "print(\"\\nTime series after polynomial interpolation:\")\n",
        "print(poly_interp)\n",
        "\n",
        "# Plot the different interpolation methods\n",
        "plt.figure(figsize=(12, 6))\n",
        "time_series.plot(marker='o', linestyle='none', label='Original Data', ax=plt.gca())\n",
        "linear_interp.plot(marker='.', label='Linear Interpolation', ax=plt.gca())\n",
        "poly_interp.plot(marker='.', label='Polynomial Interpolation', ax=plt.gca())\n",
        "plt.title('Comparison of Interpolation Methods')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Fill missing values based on groups\n",
        "# Create a DataFrame with customer segments\n",
        "customer_data = {\n",
        "    'customer_id': ['C001', 'C002', 'C003', 'C004', 'C005', 'C006', 'C007', 'C008'],\n",
        "    'segment': ['Premium', 'Standard', 'Premium', 'Standard', 'Premium', 'Standard', 'Premium', 'Standard'],\n",
        "    'spending': [1500, 600, np.nan, 450, 1800, np.nan, 1200, 500]\n",
        "}\n",
        "segment_df = pd.DataFrame(customer_data)\n",
        "\n",
        "print(\"Customer segment data with missing values:\")\n",
        "print(segment_df)\n",
        "\n",
        "# Group by segment and fill missing values with group mean\n",
        "segment_means = segment_df.groupby('segment')['spending'].transform('mean')\n",
        "segment_df['spending_filled'] = segment_df['spending'].fillna(segment_means)\n",
        "\n",
        "print(\"\\nAfter filling with segment means:\")\n",
        "print(segment_df)\n",
        "\n",
        "# Show the group means\n",
        "print(\"\\nSegment means:\")\n",
        "print(segment_df.groupby('segment')['spending'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### When to use imputation methods:\n",
        "\n",
        "- **Pros:**\n",
        "  - Preserves sample size\n",
        "  - Can handle both MCAR and MAR data\n",
        "  - Multiple techniques available for different data types\n",
        "  \n",
        "- **Cons:**\n",
        "  - Can introduce bias if not done carefully\n",
        "  - May distort distribution and relationships between variables\n",
        "  - Some methods can be computationally intensive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Handling Missing Values in Different Data Types\n",
        "\n",
        "Different data types require different approaches to handle missing values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame with different data types\n",
        "mixed_data = {\n",
        "    'id': [1, 2, 3, 4, 5],\n",
        "    'numeric': [10.5, np.nan, 15.7, 8.2, np.nan],\n",
        "    'categorical': ['A', 'B', None, 'A', None],\n",
        "    'date': [pd.Timestamp('2025-01-15'), None, pd.Timestamp('2025-02-20'), None, pd.Timestamp('2025-03-10')],\n",
        "    'boolean': [True, False, None, True, np.nan]\n",
        "}\n",
        "mixed_df = pd.DataFrame(mixed_data)\n",
        "\n",
        "print(\"Mixed data types with missing values:\")\n",
        "print(mixed_df)\n",
        "print(\"\\nData types:\")\n",
        "print(mixed_df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Handling missing numeric values\n",
        "mixed_df['numeric_filled'] = mixed_df['numeric'].fillna(mixed_df['numeric'].mean())\n",
        "\n",
        "# 2. Handling missing categorical values\n",
        "mixed_df['categorical_filled'] = mixed_df['categorical'].fillna('Unknown')\n",
        "\n",
        "# 3. Handling missing date values\n",
        "mixed_df['date_filled'] = mixed_df['date'].fillna(pd.Timestamp('2025-01-01'))\n",
        "\n",
        "# 4. Handling missing boolean values\n",
        "mixed_df['boolean_filled'] = mixed_df['boolean'].fillna(False)  # Filling with False\n",
        "\n",
        "print(\"DataFrame after type-specific handling:\")\n",
        "print(mixed_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Advanced Techniques: Indicator Variables\n",
        "\n",
        "Sometimes, the fact that a value is missing can be informative. In such cases, we can create indicator variables to track which values were originally missing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create indicator variables\n",
        "df_indicator = customers_df.copy()\n",
        "\n",
        "# Create an indicator for missing ages\n",
        "df_indicator['age_missing'] = df_indicator['age'].isna()\n",
        "\n",
        "# Fill missing age values with mean\n",
        "df_indicator['age'] = df_indicator['age'].fillna(df_indicator['age'].mean())\n",
        "\n",
        "print(\"DataFrame with indicator variable:\")\n",
        "print(df_indicator[['customer_id', 'age', 'age_missing']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Advanced Techniques: Multiple Imputation\n",
        "\n",
        "Multiple imputation involves creating multiple complete datasets with different imputed values, analyzing each dataset, and then combining the results. This accounts for the uncertainty in imputation.\n",
        "\n",
        "While a full implementation is beyond the scope of this lesson, here's a conceptual example using `scikit-learn`'s `IterativeImputer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conceptual example of multiple imputation\n",
        "'''\n",
        "from sklearn.experimental import enable_iterative_imputer  # Needed for IterativeImputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "import numpy as np\n",
        "\n",
        "# Extract numeric columns for imputation\n",
        "numeric_cols = ['age', 'purchases', 'loyalty_score']\n",
        "numeric_data = customers_df[numeric_cols].values\n",
        "\n",
        "# Create imputer\n",
        "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
        "imputed_data = imputer.fit_transform(numeric_data)\n",
        "\n",
        "# Create DataFrame with imputed values\n",
        "imputed_df = customers_df.copy()\n",
        "imputed_df[numeric_cols] = imputed_data\n",
        "\n",
        "print(\"Original DataFrame (numeric columns only):\")\n",
        "print(customers_df[numeric_cols])\n",
        "\n",
        "print(\"\\nImputed DataFrame (numeric columns only):\")\n",
        "print(imputed_df[numeric_cols])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Best Practices for Handling Missing Data\n",
        "\n",
        "1. **Understand the nature of your missing data**\n",
        "   - Investigate why data is missing (MCAR, MAR, or MNAR)\n",
        "   - Document missing data patterns\n",
        "\n",
        "2. **Choose appropriate methods based on data type and analysis goals**\n",
        "   - For simple exploratory analysis, simple imputation may suffice\n",
        "   - For more complex analysis, consider multiple imputation or more sophisticated techniques\n",
        "\n",
        "3. **Use multiple approaches and compare results**\n",
        "   - Sensitivity analysis: compare different imputation methods\n",
        "   - Validate results by creating artificial missing data in complete datasets\n",
        "\n",
        "4. **Document your missing data handling approach**\n",
        "   - Transparency about how missing data was handled is important for reproducibility\n",
        "   - Acknowledge limitations of your approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. SQL Comparison to Pandas Missing Data Handling\n",
        "\n",
        "In SQL, handling missing values is typically done using `NULL` values and functions like `COALESCE`. Here's a comparison between SQL and Pandas techniques:\n",
        "\n",
        "| SQL Operation | Pandas Equivalent |\n",
        "|--------------|-------------------|\n",
        "| `SELECT * FROM table WHERE column IS NULL` | `df[df['column'].isna()]` |\n",
        "| `SELECT * FROM table WHERE column IS NOT NULL` | `df[df['column'].notna()]` |\n",
        "| `COALESCE(column, 0)` | `df['column'].fillna(0)` |\n",
        "| `COALESCE(column, AVG(column) OVER())` | `df['column'].fillna(df['column'].mean())` |\n",
        "| `DELETE FROM table WHERE column IS NULL` | `df.dropna(subset=['column'])` |\n",
        "| `COUNT(column)` (counts non-NULL values) | `df['column'].count()` |\n",
        "| `COUNT(*)` (counts all rows) | `len(df)` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Practice Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Detecting Missing Values\n",
        "Calculate the percentage of missing values for each row in the `customers_df` DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Filling Missing Values\n",
        "Fill the missing values in the `customers_df` DataFrame with appropriate values for each column:\n",
        "- 'name': 'Unknown Customer'\n",
        "- 'age': The median age\n",
        "- 'email': '{customer_id}@example.com'\n",
        "- 'purchases': 0\n",
        "- 'last_purchase_date': The current date\n",
        "- 'loyalty_score': The mean loyalty score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Missing Data Analysis\n",
        "Find if there's a relationship between missing age values and missing loyalty scores in the `customers_df` DataFrame (i.e., do records that have missing ages also tend to have missing loyalty scores?)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4: SQL to Pandas Translation\n",
        "Translate the following SQL query to Pandas code:\n",
        "```sql\n",
        "SELECT \n",
        "    customer_id, \n",
        "    COALESCE(name, 'Unknown') as name,\n",
        "    COALESCE(age, (SELECT AVG(age) FROM customers)) as age,\n",
        "    email\n",
        "FROM customers\n",
        "WHERE email IS NOT NULL\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5: Advanced Missing Data Handling\n",
        "Create a new DataFrame that:\n",
        "1. Removes customers who have more than 3 missing values\n",
        "2. For the remaining customers, fills missing values using forward fill where possible\n",
        "3. For any values still missing, uses the mean for numeric columns and 'Unknown' for string columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "With indexing, filtering, and handling missing values covered, you now have a solid foundation in Pandas data manipulation. In the next sessions, we'll explore advanced Pandas operations including GroupBy, merging, and pivoting data.\n",
        "\n",
        "Next week, we'll move on to Data Transformation techniques which will further enhance your data preprocessing capabilities."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
