{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "business_modeling_header"
   },
   "source": [
    "# Linear Regression Part 2: Business Modeling Applications\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will be able to:\n",
    "- Apply regression models to specific business problems\n",
    "- Build customer lifetime value prediction models\n",
    "- Implement pricing optimization using regression analysis\n",
    "- Develop demand forecasting models for inventory management\n",
    "- Create marketing ROI prediction frameworks\n",
    "- Handle categorical variables and interaction effects in regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "environment_setup"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "Building on our regression fundamentals with additional business modeling libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "imports_setup"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete for business modeling!\n"
     ]
    }
   ],
   "source": [
    "# Standard data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Statistical and machine learning libraries\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Database connectivity\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Visualization styling\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Environment setup complete for business modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "database_connection"
   },
   "source": [
    "## Database Connection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "db_config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection successful!\n",
      "Test result: 1\n",
      "üîí Security Note: Database credentials loaded from .env file\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Database configuration - reading from environment variables\n",
    "DATABASE_CONFIG = {\n",
    "    'host': os.getenv('POSTGRES_HOST'),\n",
    "    'port': os.getenv('POSTGRES_PORT', '6543'),\n",
    "    'database': os.getenv('POSTGRES_DATABASE', 'postgres'),\n",
    "    'user': os.getenv('POSTGRES_USER'),\n",
    "    'password': os.getenv('POSTGRES_PASSWORD')\n",
    "}\n",
    "\n",
    "def create_database_connection():\n",
    "    \"\"\"\n",
    "    Create a SQLAlchemy engine for database connections.\n",
    "    \n",
    "    Returns:\n",
    "        sqlalchemy.engine.Engine: Database engine for executing queries\n",
    "    \"\"\"\n",
    "    # Check if all required credentials are available\n",
    "    required_fields = ['host', 'user', 'password']\n",
    "    missing_fields = [field for field in required_fields if not DATABASE_CONFIG[field]]\n",
    "    \n",
    "    if missing_fields:\n",
    "        raise ValueError(f\"Missing database credentials: {missing_fields}\")\n",
    "    \n",
    "    connection_string = f\"postgresql://{DATABASE_CONFIG['user']}:{DATABASE_CONFIG['password']}@{DATABASE_CONFIG['host']}:{DATABASE_CONFIG['port']}/{DATABASE_CONFIG['database']}\"\n",
    "    engine = create_engine(connection_string, pool_size=5, max_overflow=10)\n",
    "    return engine\n",
    "\n",
    "# Test database connection\n",
    "try:\n",
    "    engine = create_database_connection()\n",
    "    \n",
    "    # Use proper SQLAlchemy syntax for newer versions\n",
    "    from sqlalchemy import text\n",
    "    test_query = text(\"SELECT 1 as test\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        test_result = conn.execute(test_query)\n",
    "        result_value = test_result.scalar()\n",
    "        print(\"‚úÖ Database connection successful!\")\n",
    "        print(f\"Test result: {result_value}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Database connection failed: {str(e)}\")\n",
    "    print(\"Please check your .env file and database credentials.\")\n",
    "    \n",
    "    # Debug information\n",
    "    print(\"\\nDebug information:\")\n",
    "    print(f\"Host: {DATABASE_CONFIG['host']}\")\n",
    "    print(f\"Port: {DATABASE_CONFIG['port']}\")\n",
    "    print(f\"Database: {DATABASE_CONFIG['database']}\")\n",
    "    print(f\"User: {DATABASE_CONFIG['user']}\")\n",
    "    print(f\"Password: {'*' * len(DATABASE_CONFIG['password']) if DATABASE_CONFIG['password'] else 'None'}\")\n",
    "\n",
    "print(\"üîí Security Note: Database credentials loaded from .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "business_application_1"
   },
   "source": [
    "# Business Application 1: Customer Lifetime Value (CLV) Prediction\n",
    "\n",
    "## Scenario: Predicting Customer Value for Marketing Investment\n",
    "\n",
    "We'll build a regression model to predict customer lifetime value based on early purchase behavior, enabling targeted marketing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clv_data_preparation"
   },
   "outputs": [],
   "source": [
    "def prepare_clv_dataset():\n",
    "    \"\"\"\n",
    "    Prepare customer lifetime value dataset for regression modeling.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: CLV dataset with features and target\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH customer_first_order AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            MIN(order_purchase_timestamp) as first_order_date\n",
    "        FROM \"olist_sales_data_set\".\"olist_orders_dataset\"\n",
    "        WHERE order_status = 'delivered'\n",
    "        GROUP BY customer_id\n",
    "    ),\n",
    "    customer_metrics AS (\n",
    "        SELECT \n",
    "            c.customer_id,\n",
    "            c.customer_state,\n",
    "            fo.first_order_date,\n",
    "            \n",
    "            -- Target: Total customer lifetime value\n",
    "            SUM(oi.price + oi.freight_value) as total_clv,\n",
    "            \n",
    "            -- Early behavior features (first 30 days)\n",
    "            COUNT(DISTINCT CASE \n",
    "                WHEN o.order_purchase_timestamp <= fo.first_order_date + INTERVAL '30 days' \n",
    "                THEN o.order_id \n",
    "            END) as orders_first_30_days,\n",
    "            \n",
    "            SUM(CASE \n",
    "                WHEN o.order_purchase_timestamp <= fo.first_order_date + INTERVAL '30 days' \n",
    "                THEN oi.price + oi.freight_value \n",
    "                ELSE 0 \n",
    "            END) as spending_first_30_days,\n",
    "            \n",
    "            -- First order characteristics\n",
    "            COUNT(DISTINCT CASE \n",
    "                WHEN o.order_purchase_timestamp = fo.first_order_date \n",
    "                THEN oi.order_item_id \n",
    "            END) as first_order_items,\n",
    "            \n",
    "            AVG(CASE \n",
    "                WHEN o.order_purchase_timestamp = fo.first_order_date \n",
    "                THEN oi.price \n",
    "            END) as first_order_avg_item_price,\n",
    "            \n",
    "            COUNT(DISTINCT CASE \n",
    "                WHEN o.order_purchase_timestamp = fo.first_order_date \n",
    "                THEN p.product_category_name_english \n",
    "            END) as first_order_categories,\n",
    "            \n",
    "            -- Overall customer behavior\n",
    "            COUNT(DISTINCT o.order_id) as total_orders,\n",
    "            COUNT(DISTINCT p.product_category_name_english) as unique_categories,\n",
    "            AVG(r.review_score) as avg_review_score,\n",
    "            \n",
    "            -- Temporal features\n",
    "            EXTRACT(MONTH FROM fo.first_order_date) as first_order_month,\n",
    "            EXTRACT(DOW FROM fo.first_order_date) as first_order_dow,\n",
    "            \n",
    "            -- Time span\n",
    "            EXTRACT(DAYS FROM (MAX(o.order_purchase_timestamp) - fo.first_order_date)) as customer_lifespan_days\n",
    "            \n",
    "        FROM \"olist_sales_data_set\".\"olist_customers_dataset\" c\n",
    "        INNER JOIN customer_first_order fo ON c.customer_id = fo.customer_id\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_orders_dataset\" o ON c.customer_id = o.customer_id\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_order_items_dataset\" oi ON o.order_id = oi.order_id\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_products_dataset\" p ON oi.product_id = p.product_id\n",
    "        LEFT JOIN \"olist_sales_data_set\".\"olist_order_reviews_dataset\" r ON o.order_id = r.order_id\n",
    "        WHERE o.order_status = 'delivered'\n",
    "        GROUP BY c.customer_id, c.customer_state, fo.first_order_date\n",
    "        HAVING COUNT(DISTINCT o.order_id) >= 1\n",
    "            AND SUM(oi.price + oi.freight_value) > 0\n",
    "            AND EXTRACT(DAYS FROM (MAX(o.order_purchase_timestamp) - fo.first_order_date)) >= 30\n",
    "    ),\n",
    "    regional_mapping AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            CASE \n",
    "                WHEN customer_state IN ('SP', 'RJ', 'ES', 'MG') THEN 'Southeast'\n",
    "                WHEN customer_state IN ('PR', 'SC', 'RS') THEN 'South'\n",
    "                WHEN customer_state IN ('GO', 'MT', 'MS', 'DF') THEN 'Center-West'\n",
    "                WHEN customer_state IN ('BA', 'SE', 'PE', 'AL', 'PB', 'RN', 'CE', 'PI', 'MA') THEN 'Northeast'\n",
    "                WHEN customer_state IN ('AM', 'RR', 'AP', 'PA', 'TO', 'RO', 'AC') THEN 'North'\n",
    "                ELSE 'Other'\n",
    "            END as region\n",
    "        FROM customer_metrics\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM regional_mapping\n",
    "    WHERE region != 'Other'\n",
    "        AND total_clv < 2000  -- Remove extreme outliers\n",
    "        AND customer_lifespan_days <= 365  -- Focus on first year\n",
    "    ORDER BY total_clv DESC\n",
    "    LIMIT 3000\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.read_sql(query, engine)\n",
    "\n",
    "# Load CLV dataset\n",
    "clv_df = prepare_clv_dataset()\n",
    "\n",
    "print(f\"Loaded {len(clv_df):,} customers for CLV analysis\")\n",
    "print(f\"\\nTarget variable (total_clv) statistics:\")\n",
    "print(clv_df['total_clv'].describe())\n",
    "\n",
    "print(f\"\\nEarly behavior insights:\")\n",
    "print(f\"Average spending in first 30 days: R$ {clv_df['spending_first_30_days'].mean():.2f}\")\n",
    "print(f\"Average orders in first 30 days: {clv_df['orders_first_30_days'].mean():.2f}\")\n",
    "\n",
    "# Visualize CLV distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes[0].hist(clv_df['total_clv'], bins=50, alpha=0.7, color='skyblue')\n",
    "axes[0].set_title('Customer Lifetime Value Distribution')\n",
    "axes[0].set_xlabel('Total CLV (R$)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].scatter(clv_df['spending_first_30_days'], clv_df['total_clv'], alpha=0.6, s=20)\n",
    "axes[1].set_title('First 30 Days Spending vs Total CLV')\n",
    "axes[1].set_xlabel('Spending First 30 Days (R$)')\n",
    "axes[1].set_ylabel('Total CLV (R$)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clv_model_building"
   },
   "source": [
    "## Building the CLV Prediction Model\n",
    "\n",
    "We'll create a comprehensive model to predict customer lifetime value using early behavioral indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clv_model_implementation"
   },
   "outputs": [],
   "source": [
    "def build_clv_prediction_model(data):\n",
    "    \"\"\"\n",
    "    Build a customer lifetime value prediction model.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): CLV dataset\n",
    "    \n",
    "    Returns:\n",
    "        dict: Model results and business insights\n",
    "    \"\"\"\n",
    "    print(\"CUSTOMER LIFETIME VALUE PREDICTION MODEL\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Feature engineering\n",
    "    model_data = data.copy()\n",
    "    \n",
    "    # Create derived features\n",
    "    model_data['first_order_value'] = model_data['spending_first_30_days'] / model_data['orders_first_30_days']\n",
    "    model_data['first_order_value'] = model_data['first_order_value'].fillna(0)\n",
    "    \n",
    "    # Categorical encoding\n",
    "    region_dummies = pd.get_dummies(model_data['region'], prefix='region')\n",
    "    model_data = pd.concat([model_data, region_dummies], axis=1)\n",
    "    \n",
    "    # Select features for the model\n",
    "    feature_columns = [\n",
    "        'orders_first_30_days',\n",
    "        'spending_first_30_days', \n",
    "        'first_order_items',\n",
    "        'first_order_avg_item_price',\n",
    "        'first_order_categories',\n",
    "        'first_order_value',\n",
    "        'first_order_month',\n",
    "        'first_order_dow'\n",
    "    ] + [col for col in model_data.columns if col.startswith('region_')]\n",
    "    \n",
    "    # Prepare data\n",
    "    clean_data = model_data[feature_columns + ['total_clv']].dropna()\n",
    "    X = clean_data[feature_columns]\n",
    "    y = clean_data['total_clv']\n",
    "    \n",
    "    print(f\"Features: {len(feature_columns)}\")\n",
    "    print(f\"Sample size: {len(clean_data):,}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Build multiple models for comparison\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Lasso Regression': Lasso(alpha=0.1)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Metrics\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_mae': test_mae,\n",
    "            'predictions': y_test_pred\n",
    "        }\n",
    "    \n",
    "    # Compare models\n",
    "    print(f\"\\n{'Model':<20} {'Train R¬≤':<12} {'Test R¬≤':<12} {'RMSE':<12} {'MAE':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    best_model_name = None\n",
    "    best_test_r2 = -np.inf\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        print(f\"{name:<20} {result['train_r2']:<12.4f} {result['test_r2']:<12.4f} \"\n",
    "              f\"{result['test_rmse']:<12.2f} {result['test_mae']:<12.2f}\")\n",
    "        \n",
    "        if result['test_r2'] > best_test_r2:\n",
    "            best_test_r2 = result['test_r2']\n",
    "            best_model_name = name\n",
    "    \n",
    "    print(f\"\\nüèÜ Best model: {best_model_name} (Test R¬≤ = {best_test_r2:.4f})\")\n",
    "    \n",
    "    # Feature importance for best model\n",
    "    best_model = results[best_model_name]['model']\n",
    "    \n",
    "    if hasattr(best_model, 'coef_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_columns,\n",
    "            'Coefficient': best_model.coef_,\n",
    "            'Abs_Coefficient': np.abs(best_model.coef_)\n",
    "        }).sort_values('Abs_Coefficient', ascending=False)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FEATURE IMPORTANCE ({best_model_name}):\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        for _, row in feature_importance.head(10).iterrows():\n",
    "            direction = \"‚Üë\" if row['Coefficient'] > 0 else \"‚Üì\"\n",
    "            print(f\"{row['Feature']:<25} {row['Coefficient']:<10.3f} {direction}\")\n",
    "    \n",
    "    # Business insights\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BUSINESS INSIGHTS & RECOMMENDATIONS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # CLV segments\n",
    "    y_test_array = y_test.values\n",
    "    predictions = results[best_model_name]['predictions']\n",
    "    \n",
    "    # Define CLV segments\n",
    "    clv_percentiles = np.percentile(y_test_array, [33, 67])\n",
    "    \n",
    "    def classify_clv(value):\n",
    "        if value < clv_percentiles[0]:\n",
    "            return 'Low Value'\n",
    "        elif value < clv_percentiles[1]:\n",
    "            return 'Medium Value'\n",
    "        else:\n",
    "            return 'High Value'\n",
    "    \n",
    "    actual_segments = [classify_clv(v) for v in y_test_array]\n",
    "    predicted_segments = [classify_clv(v) for v in predictions]\n",
    "    \n",
    "    # Accuracy by segment\n",
    "    segment_accuracy = sum(a == p for a, p in zip(actual_segments, predicted_segments)) / len(actual_segments)\n",
    "    \n",
    "    print(f\"‚Ä¢ Model can predict CLV with {best_test_r2*100:.1f}% variance explained\")\n",
    "    print(f\"‚Ä¢ CLV segment classification accuracy: {segment_accuracy*100:.1f}%\")\n",
    "    print(f\"‚Ä¢ Average prediction error: R$ {results[best_model_name]['test_mae']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nTOP PREDICTORS OF HIGH CLV:\")\n",
    "    top_positive = feature_importance[feature_importance['Coefficient'] > 0].head(3)\n",
    "    for _, row in top_positive.iterrows():\n",
    "        feature_name = row['Feature'].replace('_', ' ').title()\n",
    "        print(f\"  ‚úÖ {feature_name}\")\n",
    "    \n",
    "    print(f\"\\nMARKETING RECOMMENDATIONS:\")\n",
    "    print(f\"  üí° Focus acquisition on customers with high early spending\")\n",
    "    print(f\"  üí° Encourage multiple items in first order\")\n",
    "    print(f\"  üí° Target customers who purchase from multiple categories\")\n",
    "    print(f\"  üí° Implement early engagement campaigns for new customers\")\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'best_model': best_model_name,\n",
    "        'feature_importance': feature_importance,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'feature_columns': feature_columns\n",
    "    }\n",
    "\n",
    "# Build CLV prediction model\n",
    "clv_model_results = build_clv_prediction_model(clv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "business_application_2"
   },
   "source": [
    "# Business Application 2: Dynamic Pricing Optimization\n",
    "\n",
    "## Scenario: Price Elasticity Analysis for Revenue Optimization\n",
    "\n",
    "We'll analyze how price changes affect demand and build a pricing optimization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pricing_data_preparation"
   },
   "outputs": [],
   "source": [
    "def prepare_pricing_dataset():\n",
    "    \"\"\"\n",
    "    Prepare dataset for pricing elasticity analysis.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Pricing analysis dataset\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH product_pricing AS (\n",
    "        SELECT \n",
    "            p.product_category_name_english as category,\n",
    "            oi.seller_id,\n",
    "            \n",
    "            -- Price metrics\n",
    "            oi.price,\n",
    "            oi.freight_value,\n",
    "            oi.price + oi.freight_value as total_price,\n",
    "            \n",
    "            -- Demand metrics (order frequency)\n",
    "            COUNT(*) OVER (PARTITION BY p.product_category_name_english, \n",
    "                          DATE_TRUNC('month', o.order_purchase_timestamp)) as monthly_category_demand,\n",
    "            \n",
    "            COUNT(*) OVER (PARTITION BY oi.seller_id, \n",
    "                          DATE_TRUNC('month', o.order_purchase_timestamp)) as monthly_seller_demand,\n",
    "            \n",
    "            -- Competitive context\n",
    "            AVG(oi.price) OVER (PARTITION BY p.product_category_name_english) as category_avg_price,\n",
    "            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY oi.price) \n",
    "                OVER (PARTITION BY p.product_category_name_english) as category_median_price,\n",
    "            \n",
    "            -- Product characteristics\n",
    "            p.product_weight_g,\n",
    "            p.product_length_cm * p.product_height_cm * p.product_width_cm as product_volume,\n",
    "            \n",
    "            -- Customer satisfaction\n",
    "            r.review_score,\n",
    "            \n",
    "            -- Temporal features\n",
    "            EXTRACT(MONTH FROM o.order_purchase_timestamp) as order_month,\n",
    "            EXTRACT(DOW FROM o.order_purchase_timestamp) as order_dow,\n",
    "            \n",
    "            -- Regional context\n",
    "            c.customer_state\n",
    "            \n",
    "        FROM \"olist_sales_data_set\".\"olist_order_items_dataset\" oi\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_orders_dataset\" o ON oi.order_id = o.order_id\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_products_dataset\" p ON oi.product_id = p.product_id\n",
    "        INNER JOIN \"olist_sales_data_set\".\"olist_customers_dataset\" c ON o.customer_id = c.customer_id\n",
    "        LEFT JOIN \"olist_sales_data_set\".\"olist_order_reviews_dataset\" r ON o.order_id = r.order_id\n",
    "        WHERE o.order_status = 'delivered'\n",
    "            AND p.product_category_name_english IS NOT NULL\n",
    "            AND oi.price > 0\n",
    "            AND p.product_weight_g IS NOT NULL\n",
    "            AND oi.price < 500  -- Focus on reasonable price range\n",
    "    ),\n",
    "    enhanced_pricing AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            \n",
    "            -- Price position features\n",
    "            price / category_avg_price as price_vs_category_avg,\n",
    "            price / category_median_price as price_vs_category_median,\n",
    "            \n",
    "            CASE \n",
    "                WHEN price < category_avg_price * 0.8 THEN 'Low Price'\n",
    "                WHEN price > category_avg_price * 1.2 THEN 'High Price'\n",
    "                ELSE 'Market Price'\n",
    "            END as price_positioning,\n",
    "            \n",
    "            -- Regional mapping\n",
    "            CASE \n",
    "                WHEN customer_state IN ('SP', 'RJ', 'ES', 'MG') THEN 'Southeast'\n",
    "                WHEN customer_state IN ('PR', 'SC', 'RS') THEN 'South'\n",
    "                WHEN customer_state IN ('GO', 'MT', 'MS', 'DF') THEN 'Center-West'\n",
    "                WHEN customer_state IN ('BA', 'SE', 'PE', 'AL', 'PB', 'RN', 'CE', 'PI', 'MA') THEN 'Northeast'\n",
    "                WHEN customer_state IN ('AM', 'RR', 'AP', 'PA', 'TO', 'RO', 'AC') THEN 'North'\n",
    "                ELSE 'Other'\n",
    "            END as region\n",
    "            \n",
    "        FROM product_pricing\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM enhanced_pricing\n",
    "    WHERE region != 'Other'\n",
    "        AND category IN (\n",
    "            SELECT category \n",
    "            FROM enhanced_pricing \n",
    "            GROUP BY category \n",
    "            HAVING COUNT(*) >= 500  -- Categories with sufficient data\n",
    "        )\n",
    "    ORDER BY RANDOM()\n",
    "    LIMIT 5000\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.read_sql(query, engine)\n",
    "\n",
    "# Load pricing dataset\n",
    "pricing_df = prepare_pricing_dataset()\n",
    "\n",
    "print(f\"Loaded {len(pricing_df):,} pricing observations\")\n",
    "print(f\"\\nCategories analyzed: {pricing_df['category'].nunique()}\")\n",
    "print(f\"Sellers analyzed: {pricing_df['seller_id'].nunique()}\")\n",
    "\n",
    "print(f\"\\nPrice statistics:\")\n",
    "print(pricing_df['price'].describe())\n",
    "\n",
    "print(f\"\\nTop categories by volume:\")\n",
    "print(pricing_df['category'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "price_elasticity_analysis"
   },
   "source": [
    "## Price Elasticity Analysis\n",
    "\n",
    "We'll analyze how demand responds to price changes across different product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elasticity_model"
   },
   "outputs": [],
   "source": [
    "def analyze_price_elasticity(data):\n",
    "    \"\"\"\n",
    "    Analyze price elasticity of demand across categories.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Pricing dataset\n",
    "    \n",
    "    Returns:\n",
    "        dict: Elasticity analysis results\n",
    "    \"\"\"\n",
    "    print(\"PRICE ELASTICITY ANALYSIS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    elasticity_results = {}\n",
    "    \n",
    "    # Analyze top 5 categories by volume\n",
    "    top_categories = data['category'].value_counts().head(5).index\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, category in enumerate(top_categories):\n",
    "        category_data = data[data['category'] == category].copy()\n",
    "        \n",
    "        # Create price bins for demand analysis\n",
    "        category_data['price_bin'] = pd.cut(category_data['price'], bins=10)\n",
    "        \n",
    "        # Calculate demand by price bin\n",
    "        demand_by_price = category_data.groupby('price_bin').agg({\n",
    "            'monthly_category_demand': 'mean',\n",
    "            'price': 'mean',\n",
    "            'review_score': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        demand_by_price = demand_by_price.dropna()\n",
    "        \n",
    "        if len(demand_by_price) > 3:  # Need sufficient data points\n",
    "            # Log-log regression for elasticity\n",
    "            log_price = np.log(demand_by_price['price'])\n",
    "            log_demand = np.log(demand_by_price['monthly_category_demand'])\n",
    "            \n",
    "            # Remove infinite values\n",
    "            valid_idx = np.isfinite(log_price) & np.isfinite(log_demand)\n",
    "            log_price = log_price[valid_idx]\n",
    "            log_demand = log_demand[valid_idx]\n",
    "            \n",
    "            if len(log_price) > 2:\n",
    "                # Fit elasticity model\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(log_price, log_demand)\n",
    "                elasticity = slope  # Slope in log-log regression is elasticity\n",
    "                \n",
    "                elasticity_results[category] = {\n",
    "                    'elasticity': elasticity,\n",
    "                    'r_squared': r_value**2,\n",
    "                    'p_value': p_value,\n",
    "                    'observations': len(log_price)\n",
    "                }\n",
    "                \n",
    "                # Plot demand curve\n",
    "                axes[i].scatter(demand_by_price['price'], demand_by_price['monthly_category_demand'], \n",
    "                               alpha=0.7, s=50)\n",
    "                \n",
    "                # Add trend line\n",
    "                x_trend = np.linspace(demand_by_price['price'].min(), demand_by_price['price'].max(), 100)\n",
    "                y_trend = np.exp(intercept + slope * np.log(x_trend))\n",
    "                axes[i].plot(x_trend, y_trend, 'r--', alpha=0.8)\n",
    "                \n",
    "                axes[i].set_title(f'{category}\\nElasticity: {elasticity:.2f}')\n",
    "                axes[i].set_xlabel('Price (R$)')\n",
    "                axes[i].set_ylabel('Monthly Demand')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    if len(top_categories) < 6:\n",
    "        fig.delaxes(axes[5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary of elasticity results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PRICE ELASTICITY SUMMARY:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{'Category':<25} {'Elasticity':<12} {'R¬≤':<8} {'Interpretation':<20}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for category, results in elasticity_results.items():\n",
    "        elasticity = results['elasticity']\n",
    "        r_squared = results['r_squared']\n",
    "        \n",
    "        if abs(elasticity) > 1:\n",
    "            interpretation = \"Elastic\"\n",
    "        elif abs(elasticity) > 0.5:\n",
    "            interpretation = \"Moderately Elastic\"\n",
    "        else:\n",
    "            interpretation = \"Inelastic\"\n",
    "        \n",
    "        print(f\"{category[:24]:<25} {elasticity:<12.3f} {r_squared:<8.3f} {interpretation:<20}\")\n",
    "    \n",
    "    # Business insights\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PRICING STRATEGY RECOMMENDATIONS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    elastic_categories = [cat for cat, res in elasticity_results.items() if abs(res['elasticity']) > 1]\n",
    "    inelastic_categories = [cat for cat, res in elasticity_results.items() if abs(res['elasticity']) < 0.5]\n",
    "    \n",
    "    if elastic_categories:\n",
    "        print(f\"üìâ PRICE SENSITIVE categories (|elasticity| > 1):\")\n",
    "        for cat in elastic_categories:\n",
    "            print(f\"   ‚Ä¢ {cat}: Small price increases ‚Üí large demand drops\")\n",
    "        print(f\"   ‚Üí Strategy: Compete on price, volume-based discounts\")\n",
    "    \n",
    "    if inelastic_categories:\n",
    "        print(f\"\\nüìà PRICE INSENSITIVE categories (|elasticity| < 0.5):\")\n",
    "        for cat in inelastic_categories:\n",
    "            print(f\"   ‚Ä¢ {cat}: Price changes have limited demand impact\")\n",
    "        print(f\"   ‚Üí Strategy: Premium pricing, focus on quality/features\")\n",
    "    \n",
    "    return elasticity_results\n",
    "\n",
    "# Analyze price elasticity\n",
    "elasticity_results = analyze_price_elasticity(pricing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "business_application_3"
   },
   "source": [
    "# Business Application 3: Demand Forecasting Model\n",
    "\n",
    "## Scenario: Inventory Management Through Demand Prediction\n",
    "\n",
    "We'll build a model to forecast product demand for inventory optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demand_forecasting"
   },
   "outputs": [],
   "source": [
    "def build_demand_forecasting_model(data):\n",
    "    \"\"\"\n",
    "    Build a demand forecasting model for inventory management.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Pricing dataset with demand metrics\n",
    "    \n",
    "    Returns:\n",
    "        dict: Forecasting model results\n",
    "    \"\"\"\n",
    "    print(\"DEMAND FORECASTING MODEL\")\n",
    "    print(\"=\" * 28)\n",
    "    \n",
    "    # Prepare forecasting dataset\n",
    "    model_data = data.copy()\n",
    "    \n",
    "    # Feature engineering\n",
    "    # Seasonal features\n",
    "    model_data['is_holiday_season'] = model_data['order_month'].isin([11, 12]).astype(int)\n",
    "    model_data['is_weekend'] = model_data['order_dow'].isin([0, 6]).astype(int)\n",
    "    \n",
    "    # Price features\n",
    "    model_data['price_squared'] = model_data['price'] ** 2\n",
    "    model_data['log_price'] = np.log(model_data['price'])\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    category_dummies = pd.get_dummies(model_data['category'], prefix='cat')\n",
    "    region_dummies = pd.get_dummies(model_data['region'], prefix='region')\n",
    "    \n",
    "    model_data = pd.concat([model_data, category_dummies, region_dummies], axis=1)\n",
    "    \n",
    "    # Select features\n",
    "    feature_columns = [\n",
    "        'price', 'price_squared', 'log_price',\n",
    "        'freight_value', 'product_weight_g', 'product_volume',\n",
    "        'category_avg_price', 'price_vs_category_avg',\n",
    "        'order_month', 'order_dow',\n",
    "        'is_holiday_season', 'is_weekend'\n",
    "    ] + [col for col in model_data.columns if col.startswith(('cat_', 'region_'))]\n",
    "    \n",
    "    # Remove columns with all zeros (categories/regions not in sample)\n",
    "    feature_columns = [col for col in feature_columns if col in model_data.columns and model_data[col].sum() > 0]\n",
    "    \n",
    "    # Target variable: monthly demand\n",
    "    target = 'monthly_category_demand'\n",
    "    \n",
    "    # Prepare clean dataset\n",
    "    clean_data = model_data[feature_columns + [target]].dropna()\n",
    "    X = clean_data[feature_columns]\n",
    "    y = clean_data[target]\n",
    "    \n",
    "    print(f\"Features: {len(feature_columns)}\")\n",
    "    print(f\"Sample size: {len(clean_data):,}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale features for regularized models\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Build multiple forecasting models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge (Œ±=1.0)': Ridge(alpha=1.0),\n",
    "        'Ridge (Œ±=10.0)': Ridge(alpha=10.0),\n",
    "        'Lasso (Œ±=0.1)': Lasso(alpha=0.1),\n",
    "        'Polynomial (degree=2)': Pipeline([\n",
    "            ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('ridge', Ridge(alpha=1.0))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if 'Polynomial' in name:\n",
    "            # Use original features for polynomial\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "        else:\n",
    "            # Use scaled features for regularized models\n",
    "            if 'Linear' in name:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "            else:\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        if 'Polynomial' in name or 'Linear' in name:\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "        else:\n",
    "            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'r2': r2,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "    \n",
    "    # Model comparison\n",
    "    print(f\"\\n{'Model':<20} {'R¬≤':<8} {'RMSE':<10} {'MAE':<10} {'CV R¬≤ (¬±std)':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    best_model_name = None\n",
    "    best_cv_score = -np.inf\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        cv_score = result['cv_mean']\n",
    "        cv_std = result['cv_std']\n",
    "        \n",
    "        print(f\"{name:<20} {result['r2']:<8.4f} {result['rmse']:<10.2f} \"\n",
    "              f\"{result['mae']:<10.2f} {cv_score:.3f} (¬±{cv_std:.3f})\")\n",
    "        \n",
    "        if cv_score > best_cv_score:\n",
    "            best_cv_score = cv_score\n",
    "            best_model_name = name\n",
    "    \n",
    "    print(f\"\\nüèÜ Best model: {best_model_name} (CV R¬≤ = {best_cv_score:.4f})\")\n",
    "    \n",
    "    # Feature importance for interpretable models\n",
    "    if 'Linear' in best_model_name or 'Ridge' in best_model_name or 'Lasso' in best_model_name:\n",
    "        best_model = results[best_model_name]['model']\n",
    "        \n",
    "        if hasattr(best_model, 'coef_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_columns,\n",
    "                'Coefficient': best_model.coef_,\n",
    "                'Abs_Coefficient': np.abs(best_model.coef_)\n",
    "            }).sort_values('Abs_Coefficient', ascending=False)\n",
    "            \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"TOP 10 DEMAND DRIVERS ({best_model_name}):\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            for _, row in feature_importance.head(10).iterrows():\n",
    "                direction = \"‚Üë\" if row['Coefficient'] > 0 else \"‚Üì\"\n",
    "                feature_name = row['Feature'].replace('_', ' ').title()\n",
    "                print(f\"{feature_name:<30} {row['Coefficient']:<10.3f} {direction}\")\n",
    "    \n",
    "    # Business insights\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"INVENTORY MANAGEMENT INSIGHTS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Demand prediction accuracy\n",
    "    best_mae = results[best_model_name]['mae']\n",
    "    avg_demand = y_test.mean()\n",
    "    accuracy_percentage = (1 - best_mae / avg_demand) * 100\n",
    "    \n",
    "    print(f\"‚Ä¢ Demand prediction accuracy: {accuracy_percentage:.1f}%\")\n",
    "    print(f\"‚Ä¢ Average prediction error: {best_mae:.1f} units\")\n",
    "    print(f\"‚Ä¢ Model explains {results[best_model_name]['r2']*100:.1f}% of demand variance\")\n",
    "    \n",
    "    print(f\"\\nINVENTORY RECOMMENDATIONS:\")\n",
    "    print(f\"  üì¶ Use model for monthly demand forecasting\")\n",
    "    print(f\"  üì¶ Account for seasonal patterns (holiday season effect)\")\n",
    "    print(f\"  üì¶ Consider price elasticity when setting inventory levels\")\n",
    "    print(f\"  üì¶ Monitor regional demand variations\")\n",
    "    print(f\"  üì¶ Implement safety stock based on prediction uncertainty\")\n",
    "    \n",
    "    # Prediction intervals\n",
    "    residuals = y_test - results[best_model_name]['predictions']\n",
    "    residual_std = residuals.std()\n",
    "    \n",
    "    print(f\"\\nPREDICTION UNCERTAINTY:\")\n",
    "    print(f\"  ¬±1œÉ: {residual_std:.1f} units (68% confidence)\")\n",
    "    print(f\"  ¬±2œÉ: {2*residual_std:.1f} units (95% confidence)\")\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'best_model': best_model_name,\n",
    "        'feature_columns': feature_columns,\n",
    "        'scaler': scaler,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "\n",
    "# Build demand forecasting model\n",
    "demand_model_results = build_demand_forecasting_model(pricing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "session_summary"
   },
   "source": [
    "# Session Summary\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "In this business modeling applications session, we applied regression techniques to three critical business scenarios:\n",
    "\n",
    "### 1. Customer Lifetime Value (CLV) Prediction\n",
    "- Built predictive models using early customer behavior indicators\n",
    "- Compared Linear, Ridge, and Lasso regression approaches\n",
    "- Identified key drivers of high-value customers\n",
    "- Created actionable marketing recommendations\n",
    "- Achieved customer segment classification accuracy for targeted campaigns\n",
    "\n",
    "### 2. Dynamic Pricing Optimization\n",
    "- Conducted price elasticity analysis across product categories\n",
    "- Used log-log regression to measure demand responsiveness to price changes\n",
    "- Identified price-sensitive vs. price-insensitive categories\n",
    "- Developed category-specific pricing strategies\n",
    "- Created visual demand curves for business stakeholders\n",
    "\n",
    "### 3. Demand Forecasting for Inventory Management\n",
    "- Built comprehensive demand prediction models\n",
    "- Implemented multiple regression techniques including polynomial features\n",
    "- Used cross-validation for robust model selection\n",
    "- Incorporated seasonal and temporal factors\n",
    "- Provided prediction intervals for inventory safety stock calculations\n",
    "\n",
    "## Key Business Skills Developed\n",
    "- Customer value prediction and segmentation\n",
    "- Price elasticity analysis and optimization\n",
    "- Demand forecasting for operations\n",
    "- Feature engineering for business contexts\n",
    "- Model comparison and selection\n",
    "- Uncertainty quantification in predictions\n",
    "\n",
    "## Advanced Techniques Applied\n",
    "- Regularization methods (Ridge and Lasso regression)\n",
    "- Polynomial feature engineering\n",
    "- Cross-validation for model selection\n",
    "- Log-log regression for elasticity analysis\n",
    "- Categorical variable encoding\n",
    "- Feature scaling and standardization\n",
    "\n",
    "## Business Impact\n",
    "- **Marketing**: Target high-CLV customer segments\n",
    "- **Pricing**: Optimize prices based on demand elasticity\n",
    "- **Operations**: Improve inventory management with demand forecasts\n",
    "- **Strategy**: Data-driven decision making across business functions\n",
    "\n",
    "## Next Session Preview\n",
    "Tomorrow we'll conclude with **Model Evaluation & Business Implementation**, covering:\n",
    "- Advanced model validation techniques\n",
    "- A/B testing for model deployment\n",
    "- Business performance monitoring\n",
    "- Implementation best practices"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
