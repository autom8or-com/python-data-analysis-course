{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 - Part 2: Advanced Database Integration\n",
    "\n",
    "**Course:** Python Data Analysis for Business Intelligence  \n",
    "**Week:** 9 | **Session:** Thursday | **Part:** 2 of 3  \n",
    "**Duration:** 20 minutes | **Date:** June 5, 2025\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will be able to:\n",
    "- Implement advanced Supabase integration patterns for production applications\n",
    "- Handle real-time data streaming and live updates\n",
    "- Optimize database connections with pooling and caching strategies\n",
    "- Implement robust error handling and fallback mechanisms\n",
    "- Design secure authentication and authorization workflows\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Business Context: Production-Grade Data Integration\n",
    "\n",
    "**Enterprise Challenge**: Olist's business intelligence platform serves 500+ concurrent users across multiple time zones, processing real-time data from:\n",
    "\n",
    "### Data Sources:\n",
    "- **📦 Order Management**: 10,000+ daily transactions\n",
    "- **👥 Customer Database**: 2M+ customer profiles with real-time updates\n",
    "- **💰 Payment Systems**: Real-time payment processing and fraud detection\n",
    "- **📊 Analytics Events**: User behavior tracking and conversion metrics\n",
    "\n",
    "### Performance Requirements:\n",
    "- **⚡ Response Time**: <2 seconds for dashboard loads\n",
    "- **🔄 Real-time Updates**: Live data refreshes without page reload\n",
    "- **📈 Scalability**: Handle 1000+ concurrent dashboard users\n",
    "- **🛡️ Security**: Row-level security and role-based access\n",
    "\n",
    "**Architecture Challenge**: Build a robust, scalable database integration that maintains performance under load while ensuring data security and consistency.\n",
    "\n",
    "**Today's Solution**: Advanced Supabase integration with connection pooling, real-time subscriptions, and enterprise security patterns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Setup: Production Database Environment\n",
    "\n",
    "Let's configure a production-ready database integration environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-grade imports\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import asyncio\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "from typing import Dict, List, Optional, Union\n",
    "import logging\n",
    "\n",
    "# Configure logging for production monitoring\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Course utilities\n",
    "from Utilities.visualization_helper import set_plotting_style\n",
    "from Utilities.colab_helper import setup_colab\n",
    "\n",
    "# Setup\n",
    "plt, sns = set_plotting_style()\n",
    "setup_colab()\n",
    "\n",
    "print(\"✅ Production database environment ready!\")\n",
    "print(\"🔐 Security, performance, and reliability patterns loaded\")\n",
    "print(\"📊 Ready for enterprise-grade data integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔗 Section 1: Advanced Supabase Integration (8 minutes)\n",
    "\n",
    "Let's build a comprehensive Supabase integration with enterprise features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile advanced_supabase_integration.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Dict, List, Optional, Union\n",
    "import logging\n",
    "\n",
    "# Configure page\n",
    "st.set_page_config(\n",
    "    page_title=\"Advanced Supabase Integration\",\n",
    "    page_icon=\"🔗\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Production-grade Supabase connection manager\n",
    "class SupabaseConnectionManager:\n",
    "    \"\"\"\n",
    "    Production-grade Supabase connection manager with pooling,\n",
    "    retry logic, and performance monitoring.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.connection_pool = {}\n",
    "        self.connection_stats = {\n",
    "            'total_queries': 0,\n",
    "            'failed_queries': 0,\n",
    "            'avg_response_time': 0,\n",
    "            'last_connection': None\n",
    "        }\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    @st.cache_resource\n",
    "    def initialize_connection(_self):\n",
    "        \"\"\"\n",
    "        Initialize Supabase connection with error handling.\n",
    "        In production, this would use actual Supabase client.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Simulate connection initialization\n",
    "            _self.connection_stats['last_connection'] = datetime.now()\n",
    "            _self.logger.info(\"Supabase connection initialized successfully\")\n",
    "            \n",
    "            # In production:\n",
    "            # from supabase import create_client, Client\n",
    "            # url = st.secrets[\"supabase\"][\"url\"]\n",
    "            # key = st.secrets[\"supabase\"][\"anon_key\"]\n",
    "            # supabase: Client = create_client(url, key)\n",
    "            # return supabase\n",
    "            \n",
    "            return {'status': 'connected', 'client': 'mock_client'}\n",
    "            \n",
    "        except Exception as e:\n",
    "            _self.logger.error(f\"Failed to initialize Supabase connection: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def execute_query(self, query: str, params: Dict = None, retry_count: int = 3) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Execute database query with retry logic and performance monitoring.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                self.connection_stats['total_queries'] += 1\n",
    "                \n",
    "                # Simulate query execution\n",
    "                # In production, this would execute actual Supabase queries\n",
    "                result_data = self._simulate_query_result(query, params)\n",
    "                \n",
    "                # Update performance stats\n",
    "                response_time = time.time() - start_time\n",
    "                self._update_performance_stats(response_time)\n",
    "                \n",
    "                self.logger.info(f\"Query executed successfully in {response_time:.3f}s\")\n",
    "                return result_data\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.connection_stats['failed_queries'] += 1\n",
    "                self.logger.warning(f\"Query attempt {attempt + 1} failed: {e}\")\n",
    "                \n",
    "                if attempt == retry_count - 1:\n",
    "                    self.logger.error(f\"Query failed after {retry_count} attempts\")\n",
    "                    raise\n",
    "                \n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    \n",
    "    def _simulate_query_result(self, query: str, params: Dict = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Simulate database query results for demonstration.\n",
    "        In production, this would be replaced with actual Supabase queries.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        if 'orders' in query.lower():\n",
    "            # Simulate orders data\n",
    "            n_records = params.get('limit', 1000) if params else 1000\n",
    "            \n",
    "            dates = pd.date_range('2024-01-01', periods=n_records, freq='H')\n",
    "            \n",
    "            data = {\n",
    "                'order_id': [f\"ORD_{i:08d}\" for i in range(1, n_records + 1)],\n",
    "                'customer_id': [f\"CUST_{np.random.randint(1, 10000):06d}\" for _ in range(n_records)],\n",
    "                'order_date': dates,\n",
    "                'total_amount': np.random.exponential(120, n_records),\n",
    "                'status': np.random.choice(['completed', 'processing', 'shipped'], n_records, p=[0.7, 0.2, 0.1]),\n",
    "                'customer_satisfaction': np.random.choice([1, 2, 3, 4, 5], n_records, p=[0.05, 0.1, 0.2, 0.35, 0.3]),\n",
    "                'product_category': np.random.choice([\n",
    "                    'Electronics', 'Fashion', 'Home & Garden', 'Books', 'Sports'\n",
    "                ], n_records),\n",
    "                'seller_state': np.random.choice(['SP', 'RJ', 'MG', 'RS', 'PR'], n_records)\n",
    "            }\n",
    "            \n",
    "            return pd.DataFrame(data)\n",
    "            \n",
    "        elif 'customers' in query.lower():\n",
    "            # Simulate customer data\n",
    "            n_records = params.get('limit', 500) if params else 500\n",
    "            \n",
    "            data = {\n",
    "                'customer_id': [f\"CUST_{i:06d}\" for i in range(1, n_records + 1)],\n",
    "                'registration_date': pd.date_range('2023-01-01', periods=n_records, freq='D'),\n",
    "                'total_orders': np.random.poisson(5, n_records),\n",
    "                'lifetime_value': np.random.exponential(500, n_records),\n",
    "                'customer_segment': np.random.choice(['Premium', 'Standard', 'Budget'], n_records, p=[0.2, 0.5, 0.3]),\n",
    "                'state': np.random.choice(['SP', 'RJ', 'MG', 'RS', 'PR'], n_records),\n",
    "                'active': np.random.choice([True, False], n_records, p=[0.8, 0.2])\n",
    "            }\n",
    "            \n",
    "            return pd.DataFrame(data)\n",
    "            \n",
    "        else:\n",
    "            # Default simulation\n",
    "            return pd.DataFrame({'message': ['Query executed successfully']})\n",
    "    \n",
    "    def _update_performance_stats(self, response_time: float):\n",
    "        \"\"\"\n",
    "        Update performance statistics.\n",
    "        \"\"\"\n",
    "        current_avg = self.connection_stats['avg_response_time']\n",
    "        total_queries = self.connection_stats['total_queries']\n",
    "        \n",
    "        # Calculate rolling average\n",
    "        if total_queries == 1:\n",
    "            self.connection_stats['avg_response_time'] = response_time\n",
    "        else:\n",
    "            self.connection_stats['avg_response_time'] = (\n",
    "                (current_avg * (total_queries - 1) + response_time) / total_queries\n",
    "            )\n",
    "    \n",
    "    def get_connection_health(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get connection health metrics.\n",
    "        \"\"\"\n",
    "        total_queries = self.connection_stats['total_queries']\n",
    "        failed_queries = self.connection_stats['failed_queries']\n",
    "        \n",
    "        success_rate = ((total_queries - failed_queries) / total_queries * 100) if total_queries > 0 else 100\n",
    "        \n",
    "        return {\n",
    "            'status': 'healthy' if success_rate > 95 else 'degraded' if success_rate > 90 else 'unhealthy',\n",
    "            'success_rate': success_rate,\n",
    "            'total_queries': total_queries,\n",
    "            'failed_queries': failed_queries,\n",
    "            'avg_response_time': self.connection_stats['avg_response_time'],\n",
    "            'last_connection': self.connection_stats['last_connection']\n",
    "        }\n",
    "\n",
    "# Initialize connection manager\n",
    "if 'db_manager' not in st.session_state:\n",
    "    st.session_state.db_manager = SupabaseConnectionManager()\n",
    "\n",
    "db_manager = st.session_state.db_manager\n",
    "\n",
    "# Initialize connection\n",
    "connection = db_manager.initialize_connection()\n",
    "\n",
    "st.title(\"🔗 Advanced Supabase Integration\")\n",
    "st.markdown(\"**Production-grade database integration with monitoring and optimization**\")\n",
    "\n",
    "# Connection Health Dashboard\n",
    "st.subheader(\"🏥 Database Health Monitoring\")\n",
    "\n",
    "health_col1, health_col2, health_col3, health_col4 = st.columns(4)\n",
    "\n",
    "health_data = db_manager.get_connection_health()\n",
    "\n",
    "with health_col1:\n",
    "    status_color = {\n",
    "        'healthy': '🟢',\n",
    "        'degraded': '🟡', \n",
    "        'unhealthy': '🔴'\n",
    "    }.get(health_data['status'], '⚫')\n",
    "    \n",
    "    st.metric(\n",
    "        f\"{status_color} Connection Status\",\n",
    "        health_data['status'].title(),\n",
    "        f\"{health_data['success_rate']:.1f}% success rate\"\n",
    "    )\n",
    "\n",
    "with health_col2:\n",
    "    st.metric(\n",
    "        \"⚡ Avg Response Time\",\n",
    "        f\"{health_data['avg_response_time']:.3f}s\",\n",
    "        \"Target: <0.5s\"\n",
    "    )\n",
    "\n",
    "with health_col3:\n",
    "    st.metric(\n",
    "        \"📊 Total Queries\",\n",
    "        f\"{health_data['total_queries']:,}\",\n",
    "        f\"{health_data['failed_queries']} failed\"\n",
    "    )\n",
    "\n",
    "with health_col4:\n",
    "    last_conn = health_data['last_connection']\n",
    "    time_diff = datetime.now() - last_conn if last_conn else timedelta(0)\n",
    "    st.metric(\n",
    "        \"🕒 Last Connection\",\n",
    "        f\"{time_diff.seconds}s ago\" if time_diff.seconds < 60 else f\"{time_diff.seconds//60}m ago\",\n",
    "        \"Active\"\n",
    "    )\n",
    "\n",
    "# Real-time Data Loading\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"📊 Real-time Data Integration\")\n",
    "\n",
    "# Query interface\n",
    "query_col1, query_col2 = st.columns([2, 1])\n",
    "\n",
    "with query_col1:\n",
    "    st.markdown(\"**Query Builder**\")\n",
    "    \n",
    "    query_type = st.selectbox(\n",
    "        \"Select Data Source:\",\n",
    "        ['orders', 'customers', 'products', 'reviews']\n",
    "    )\n",
    "    \n",
    "    # Dynamic query parameters based on selection\n",
    "    if query_type == 'orders':\n",
    "        date_filter = st.date_input(\n",
    "            \"Orders since:\",\n",
    "            value=datetime.now().date() - timedelta(days=30)\n",
    "        )\n",
    "        \n",
    "        status_filter = st.multiselect(\n",
    "            \"Order status:\",\n",
    "            ['completed', 'processing', 'shipped', 'cancelled'],\n",
    "            default=['completed', 'processing']\n",
    "        )\n",
    "        \n",
    "        limit = st.slider(\"Limit results:\", 100, 5000, 1000, 100)\n",
    "        \n",
    "    elif query_type == 'customers':\n",
    "        segment_filter = st.multiselect(\n",
    "            \"Customer segment:\",\n",
    "            ['Premium', 'Standard', 'Budget'],\n",
    "            default=['Premium', 'Standard']\n",
    "        )\n",
    "        \n",
    "        active_only = st.checkbox(\"Active customers only\", value=True)\n",
    "        limit = st.slider(\"Limit results:\", 100, 2000, 500, 100)\n",
    "\n",
    "with query_col2:\n",
    "    st.markdown(\"**Query Actions**\")\n",
    "    \n",
    "    if st.button(\"🔄 Execute Query\", type=\"primary\", use_container_width=True):\n",
    "        with st.spinner(\"Executing database query...\"):\n",
    "            try:\n",
    "                # Build query parameters\n",
    "                query_params = {'limit': limit}\n",
    "                \n",
    "                if query_type == 'orders':\n",
    "                    query_params['date_filter'] = date_filter\n",
    "                    query_params['status_filter'] = status_filter\n",
    "                    \n",
    "                elif query_type == 'customers':\n",
    "                    query_params['segment_filter'] = segment_filter\n",
    "                    query_params['active_only'] = active_only\n",
    "                \n",
    "                # Execute query\n",
    "                result_df = db_manager.execute_query(\n",
    "                    f\"SELECT * FROM {query_type}\",\n",
    "                    query_params\n",
    "                )\n",
    "                \n",
    "                # Store results in session state\n",
    "                st.session_state.query_result = result_df\n",
    "                st.session_state.query_timestamp = datetime.now()\n",
    "                \n",
    "                st.success(f\"✅ Query executed successfully! Retrieved {len(result_df):,} records\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                st.error(f\"❌ Query failed: {str(e)}\")\n",
    "    \n",
    "    if st.button(\"💾 Cache Clear\", use_container_width=True):\n",
    "        st.cache_data.clear()\n",
    "        st.success(\"Cache cleared!\")\n",
    "    \n",
    "    if st.button(\"📊 Performance Stats\", use_container_width=True):\n",
    "        st.info(f\"Total queries: {health_data['total_queries']}\")\n",
    "\n",
    "# Display query results\n",
    "if 'query_result' in st.session_state:\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"📋 Query Results\")\n",
    "    \n",
    "    result_df = st.session_state.query_result\n",
    "    query_time = st.session_state.query_timestamp\n",
    "    \n",
    "    # Results summary\n",
    "    result_col1, result_col2, result_col3 = st.columns(3)\n",
    "    \n",
    "    with result_col1:\n",
    "        st.metric(\"📊 Records Retrieved\", f\"{len(result_df):,}\")\n",
    "    \n",
    "    with result_col2:\n",
    "        st.metric(\"🕒 Query Time\", query_time.strftime('%H:%M:%S'))\n",
    "    \n",
    "    with result_col3:\n",
    "        memory_usage = result_df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "        st.metric(\"💾 Memory Usage\", f\"{memory_usage:.2f} MB\")\n",
    "    \n",
    "    # Data preview tabs\n",
    "    preview_tab1, preview_tab2, preview_tab3 = st.tabs([\"📊 Data Preview\", \"📈 Analytics\", \"💾 Export\"])\n",
    "    \n",
    "    with preview_tab1:\n",
    "        # Interactive data table\n",
    "        st.dataframe(\n",
    "            result_df.head(20),\n",
    "            use_container_width=True,\n",
    "            height=400\n",
    "        )\n",
    "        \n",
    "        # Search functionality\n",
    "        search_term = st.text_input(\"🔍 Search data:\")\n",
    "        if search_term:\n",
    "            # Simple text search across all columns\n",
    "            mask = result_df.astype(str).apply(lambda x: x.str.contains(search_term, case=False, na=False)).any(axis=1)\n",
    "            search_results = result_df[mask]\n",
    "            st.write(f\"Found {len(search_results)} matching records:\")\n",
    "            st.dataframe(search_results.head(10), use_container_width=True)\n",
    "    \n",
    "    with preview_tab2:\n",
    "        # Quick analytics\n",
    "        if query_type == 'orders' and len(result_df) > 0:\n",
    "            analytics_col1, analytics_col2 = st.columns(2)\n",
    "            \n",
    "            with analytics_col1:\n",
    "                # Revenue by category\n",
    "                if 'product_category' in result_df.columns:\n",
    "                    category_revenue = result_df.groupby('product_category')['total_amount'].sum().reset_index()\n",
    "                    fig = px.pie(\n",
    "                        category_revenue,\n",
    "                        values='total_amount',\n",
    "                        names='product_category',\n",
    "                        title=\"Revenue by Category\"\n",
    "                    )\n",
    "                    st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            with analytics_col2:\n",
    "                # Orders by state\n",
    "                if 'seller_state' in result_df.columns:\n",
    "                    state_orders = result_df['seller_state'].value_counts().reset_index()\n",
    "                    state_orders.columns = ['state', 'orders']\n",
    "                    fig = px.bar(\n",
    "                        state_orders.head(10),\n",
    "                        x='state',\n",
    "                        y='orders',\n",
    "                        title=\"Orders by State (Top 10)\"\n",
    "                    )\n",
    "                    st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        elif query_type == 'customers' and len(result_df) > 0:\n",
    "            # Customer analytics\n",
    "            if 'customer_segment' in result_df.columns:\n",
    "                segment_dist = result_df['customer_segment'].value_counts().reset_index()\n",
    "                segment_dist.columns = ['segment', 'count']\n",
    "                \n",
    "                fig = px.bar(\n",
    "                    segment_dist,\n",
    "                    x='segment',\n",
    "                    y='count',\n",
    "                    title=\"Customer Distribution by Segment\"\n",
    "                )\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    with preview_tab3:\n",
    "        # Export options\n",
    "        st.markdown(\"**📊 Export Data**\")\n",
    "        \n",
    "        export_col1, export_col2, export_col3 = st.columns(3)\n",
    "        \n",
    "        with export_col1:\n",
    "            if st.button(\"📄 Export CSV\", use_container_width=True):\n",
    "                csv_data = result_df.to_csv(index=False)\n",
    "                st.download_button(\n",
    "                    label=\"⬇️ Download CSV\",\n",
    "                    data=csv_data,\n",
    "                    file_name=f\"{query_type}_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n",
    "                    mime=\"text/csv\",\n",
    "                    use_container_width=True\n",
    "                )\n",
    "        \n",
    "        with export_col2:\n",
    "            if st.button(\"📊 Export JSON\", use_container_width=True):\n",
    "                json_data = result_df.to_json(orient='records', date_format='iso')\n",
    "                st.download_button(\n",
    "                    label=\"⬇️ Download JSON\",\n",
    "                    data=json_data,\n",
    "                    file_name=f\"{query_type}_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\",\n",
    "                    mime=\"application/json\",\n",
    "                    use_container_width=True\n",
    "                )\n",
    "        \n",
    "        with export_col3:\n",
    "            if st.button(\"📧 Email Report\", use_container_width=True):\n",
    "                st.success(\"📧 Report sent to stakeholders!\")\n",
    "                st.info(\"Email functionality would be implemented here\")\n",
    "\n",
    "# Real-time monitoring sidebar\n",
    "with st.sidebar:\n",
    "    st.header(\"📊 Real-time Monitoring\")\n",
    "    \n",
    "    # Connection status\n",
    "    st.markdown(\"### 🔗 Connection Status\")\n",
    "    st.metric(\"Database\", \"Connected\", \"🟢 Healthy\")\n",
    "    st.metric(\"Response Time\", f\"{health_data['avg_response_time']:.3f}s\")\n",
    "    st.metric(\"Success Rate\", f\"{health_data['success_rate']:.1f}%\")\n",
    "    \n",
    "    # Auto-refresh controls\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### 🔄 Auto-refresh\")\n",
    "    \n",
    "    auto_refresh = st.checkbox(\"Enable auto-refresh\", value=False)\n",
    "    refresh_interval = st.selectbox(\n",
    "        \"Refresh interval:\",\n",
    "        ['30 seconds', '1 minute', '5 minutes', '15 minutes'],\n",
    "        index=1\n",
    "    )\n",
    "    \n",
    "    if auto_refresh:\n",
    "        st.info(f\"⏰ Next refresh in {refresh_interval}\")\n",
    "        # In production, implement actual auto-refresh logic\n",
    "    \n",
    "    # Database tools\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### 🛠️ Database Tools\")\n",
    "    \n",
    "    if st.button(\"🔍 Connection Test\", use_container_width=True):\n",
    "        with st.spinner(\"Testing connection...\"):\n",
    "            time.sleep(1)\n",
    "            st.success(\"✅ Connection test passed!\")\n",
    "    \n",
    "    if st.button(\"📊 Query Statistics\", use_container_width=True):\n",
    "        st.json(health_data)\n",
    "    \n",
    "    if st.button(\"🔄 Reset Stats\", use_container_width=True):\n",
    "        db_manager.connection_stats = {\n",
    "            'total_queries': 0,\n",
    "            'failed_queries': 0,\n",
    "            'avg_response_time': 0,\n",
    "            'last_connection': datetime.now()\n",
    "        }\n",
    "        st.success(\"Statistics reset!\")\n",
    "        st.rerun()\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\n",
    "    \"**🔗 Advanced Supabase Integration** | \"\n",
    "    \"Production-grade database connectivity with monitoring | \"\n",
    "    \"Built for enterprise scalability and reliability\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ Section 2: Real-time Data Streaming (7 minutes)\n",
    "\n",
    "Let's implement real-time data streaming capabilities for live dashboards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile realtime_streaming_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Dict, List, Callable\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "# Configure page\n",
    "st.set_page_config(\n",
    "    page_title=\"Real-time Data Streaming\",\n",
    "    page_icon=\"⚡\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Real-time data streaming manager\n",
    "class RealTimeDataStreamer:\n",
    "    \"\"\"\n",
    "    Manages real-time data streaming from Supabase with\n",
    "    subscription management and data buffering.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.active_subscriptions = {}\n",
    "        self.data_buffer = queue.Queue(maxsize=1000)\n",
    "        self.streaming_stats = {\n",
    "            'messages_received': 0,\n",
    "            'last_message': None,\n",
    "            'connection_status': 'disconnected',\n",
    "            'subscription_count': 0\n",
    "        }\n",
    "    \n",
    "    def create_subscription(self, table_name: str, callback: Callable = None):\n",
    "        \"\"\"\n",
    "        Create a real-time subscription to a Supabase table.\n",
    "        In production, this would use actual Supabase real-time subscriptions.\n",
    "        \"\"\"\n",
    "        subscription_id = f\"{table_name}_{datetime.now().timestamp()}\"\n",
    "        \n",
    "        # Simulate subscription creation\n",
    "        self.active_subscriptions[subscription_id] = {\n",
    "            'table': table_name,\n",
    "            'callback': callback,\n",
    "            'created_at': datetime.now(),\n",
    "            'status': 'active'\n",
    "        }\n",
    "        \n",
    "        self.streaming_stats['subscription_count'] += 1\n",
    "        self.streaming_stats['connection_status'] = 'connected'\n",
    "        \n",
    "        # In production:\n",
    "        # supabase.table(table_name).on('*', callback).subscribe()\n",
    "        \n",
    "        return subscription_id\n",
    "    \n",
    "    def simulate_real_time_data(self, table_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Simulate real-time data updates.\n",
    "        In production, this would be handled by Supabase real-time callbacks.\n",
    "        \"\"\"\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        if table_name == 'orders':\n",
    "            # Simulate new order\n",
    "            data = {\n",
    "                'event_type': 'INSERT',\n",
    "                'table': 'orders',\n",
    "                'timestamp': current_time,\n",
    "                'data': {\n",
    "                    'order_id': f\"ORD_{np.random.randint(100000, 999999)}\",\n",
    "                    'customer_id': f\"CUST_{np.random.randint(1000, 9999):04d}\",\n",
    "                    'total_amount': round(np.random.exponential(120), 2),\n",
    "                    'status': 'processing',\n",
    "                    'created_at': current_time.isoformat()\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        elif table_name == 'metrics':\n",
    "            # Simulate real-time metrics\n",
    "            data = {\n",
    "                'event_type': 'UPDATE',\n",
    "                'table': 'metrics',\n",
    "                'timestamp': current_time,\n",
    "                'data': {\n",
    "                    'active_users': np.random.randint(800, 1200),\n",
    "                    'revenue_today': round(np.random.normal(50000, 10000), 2),\n",
    "                    'orders_pending': np.random.randint(50, 150),\n",
    "                    'system_load': round(np.random.uniform(0.3, 0.9), 2)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            # Generic data\n",
    "            data = {\n",
    "                'event_type': 'UPDATE',\n",
    "                'table': table_name,\n",
    "                'timestamp': current_time,\n",
    "                'data': {'value': np.random.randint(1, 100)}\n",
    "            }\n",
    "        \n",
    "        self.streaming_stats['messages_received'] += 1\n",
    "        self.streaming_stats['last_message'] = current_time\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def get_streaming_stats(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get current streaming statistics.\n",
    "        \"\"\"\n",
    "        return self.streaming_stats.copy()\n",
    "    \n",
    "    def close_subscription(self, subscription_id: str):\n",
    "        \"\"\"\n",
    "        Close a real-time subscription.\n",
    "        \"\"\"\n",
    "        if subscription_id in self.active_subscriptions:\n",
    "            del self.active_subscriptions[subscription_id]\n",
    "            self.streaming_stats['subscription_count'] -= 1\n",
    "            \n",
    "            if self.streaming_stats['subscription_count'] == 0:\n",
    "                self.streaming_stats['connection_status'] = 'disconnected'\n",
    "\n",
    "# Initialize streamer\n",
    "if 'data_streamer' not in st.session_state:\n",
    "    st.session_state.data_streamer = RealTimeDataStreamer()\n",
    "    st.session_state.streaming_data = []\n",
    "    st.session_state.live_metrics = {\n",
    "        'active_users': 0,\n",
    "        'revenue_today': 0,\n",
    "        'orders_pending': 0,\n",
    "        'system_load': 0\n",
    "    }\n",
    "\n",
    "streamer = st.session_state.data_streamer\n",
    "\n",
    "st.title(\"⚡ Real-time Data Streaming Dashboard\")\n",
    "st.markdown(\"**Live data updates with Supabase real-time subscriptions**\")\n",
    "\n",
    "# Streaming controls\n",
    "st.subheader(\"🎛️ Real-time Controls\")\n",
    "\n",
    "control_col1, control_col2, control_col3, control_col4 = st.columns(4)\n",
    "\n",
    "with control_col1:\n",
    "    if st.button(\"▶️ Start Orders Stream\", use_container_width=True):\n",
    "        subscription_id = streamer.create_subscription('orders')\n",
    "        st.success(f\"✅ Orders stream started: {subscription_id[:8]}...\")\n",
    "\n",
    "with control_col2:\n",
    "    if st.button(\"📊 Start Metrics Stream\", use_container_width=True):\n",
    "        subscription_id = streamer.create_subscription('metrics')\n",
    "        st.success(f\"✅ Metrics stream started: {subscription_id[:8]}...\")\n",
    "\n",
    "with control_col3:\n",
    "    if st.button(\"🔄 Simulate Update\", use_container_width=True):\n",
    "        # Simulate receiving new data\n",
    "        new_order = streamer.simulate_real_time_data('orders')\n",
    "        st.session_state.streaming_data.append(new_order)\n",
    "        \n",
    "        new_metrics = streamer.simulate_real_time_data('metrics')\n",
    "        st.session_state.live_metrics.update(new_metrics['data'])\n",
    "        \n",
    "        st.success(\"📡 Real-time data updated!\")\n",
    "\n",
    "with control_col4:\n",
    "    if st.button(\"⏹️ Stop All Streams\", use_container_width=True):\n",
    "        for sub_id in list(streamer.active_subscriptions.keys()):\n",
    "            streamer.close_subscription(sub_id)\n",
    "        st.info(\"🛑 All streams stopped\")\n",
    "\n",
    "# Streaming status\n",
    "streaming_stats = streamer.get_streaming_stats()\n",
    "\n",
    "status_col1, status_col2, status_col3, status_col4 = st.columns(4)\n",
    "\n",
    "with status_col1:\n",
    "    status_emoji = \"🟢\" if streaming_stats['connection_status'] == 'connected' else \"🔴\"\n",
    "    st.metric(\n",
    "        f\"{status_emoji} Connection\",\n",
    "        streaming_stats['connection_status'].title(),\n",
    "        f\"{streaming_stats['subscription_count']} active\"\n",
    "    )\n",
    "\n",
    "with status_col2:\n",
    "    st.metric(\n",
    "        \"📡 Messages Received\",\n",
    "        f\"{streaming_stats['messages_received']:,}\",\n",
    "        \"Total\"\n",
    "    )\n",
    "\n",
    "with status_col3:\n",
    "    last_msg = streaming_stats['last_message']\n",
    "    if last_msg:\n",
    "        time_diff = (datetime.now() - last_msg).seconds\n",
    "        st.metric(\n",
    "            \"🕒 Last Update\",\n",
    "            f\"{time_diff}s ago\" if time_diff < 60 else f\"{time_diff//60}m ago\",\n",
    "            \"Live\"\n",
    "        )\n",
    "    else:\n",
    "        st.metric(\"🕒 Last Update\", \"Never\", \"Waiting\")\n",
    "\n",
    "with status_col4:\n",
    "    buffer_size = len(st.session_state.streaming_data)\n",
    "    st.metric(\n",
    "        \"💾 Buffer Size\",\n",
    "        f\"{buffer_size}\",\n",
    "        \"Messages\"\n",
    "    )\n",
    "\n",
    "# Live metrics dashboard\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"📊 Live Business Metrics\")\n",
    "\n",
    "live_metrics = st.session_state.live_metrics\n",
    "\n",
    "metrics_col1, metrics_col2, metrics_col3, metrics_col4 = st.columns(4)\n",
    "\n",
    "with metrics_col1:\n",
    "    st.metric(\n",
    "        \"👥 Active Users\",\n",
    "        f\"{live_metrics['active_users']:,}\",\n",
    "        \"Real-time\"\n",
    "    )\n",
    "\n",
    "with metrics_col2:\n",
    "    st.metric(\n",
    "        \"💰 Revenue Today\",\n",
    "        f\"R$ {live_metrics['revenue_today']:,.0f}\",\n",
    "        \"Live updates\"\n",
    "    )\n",
    "\n",
    "with metrics_col3:\n",
    "    st.metric(\n",
    "        \"📦 Orders Pending\",\n",
    "        f\"{live_metrics['orders_pending']}\",\n",
    "        \"Processing\"\n",
    "    )\n",
    "\n",
    "with metrics_col4:\n",
    "    load_color = \"🟢\" if live_metrics['system_load'] < 0.7 else \"🟡\" if live_metrics['system_load'] < 0.9 else \"🔴\"\n",
    "    st.metric(\n",
    "        f\"{load_color} System Load\",\n",
    "        f\"{live_metrics['system_load']:.1%}\",\n",
    "        \"Current\"\n",
    "    )\n",
    "\n",
    "# Real-time data visualization\n",
    "if len(st.session_state.streaming_data) > 0:\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"📈 Real-time Data Visualization\")\n",
    "    \n",
    "    # Convert streaming data to DataFrame\n",
    "    streaming_df = pd.DataFrame([\n",
    "        {\n",
    "            'timestamp': item['timestamp'],\n",
    "            'event_type': item['event_type'],\n",
    "            'table': item['table'],\n",
    "            **item['data']\n",
    "        }\n",
    "        for item in st.session_state.streaming_data[-50:]  # Last 50 events\n",
    "    ])\n",
    "    \n",
    "    viz_col1, viz_col2 = st.columns(2)\n",
    "    \n",
    "    with viz_col1:\n",
    "        # Event timeline\n",
    "        if len(streaming_df) > 1:\n",
    "            # Group by minute for timeline\n",
    "            streaming_df['minute'] = streaming_df['timestamp'].dt.floor('min')\n",
    "            timeline_data = streaming_df.groupby(['minute', 'event_type']).size().reset_index(name='count')\n",
    "            \n",
    "            fig = px.line(\n",
    "                timeline_data,\n",
    "                x='minute',\n",
    "                y='count',\n",
    "                color='event_type',\n",
    "                title=\"Real-time Event Timeline\",\n",
    "                labels={'count': 'Events per Minute'}\n",
    "            )\n",
    "            fig.update_layout(height=350)\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    with viz_col2:\n",
    "        # Event type distribution\n",
    "        event_dist = streaming_df['event_type'].value_counts().reset_index()\n",
    "        event_dist.columns = ['event_type', 'count']\n",
    "        \n",
    "        fig = px.pie(\n",
    "            event_dist,\n",
    "            values='count',\n",
    "            names='event_type',\n",
    "            title=\"Event Type Distribution\"\n",
    "        )\n",
    "        fig.update_layout(height=350)\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Recent events table\n",
    "    st.subheader(\"📋 Recent Real-time Events\")\n",
    "    \n",
    "    # Display last 10 events\n",
    "    recent_events = streaming_df.tail(10).sort_values('timestamp', ascending=False)\n",
    "    st.dataframe(\n",
    "        recent_events[['timestamp', 'event_type', 'table']],\n",
    "        use_container_width=True,\n",
    "        height=300\n",
    "    )\n",
    "\n",
    "# Configuration and monitoring\n",
    "with st.sidebar:\n",
    "    st.header(\"⚙️ Streaming Configuration\")\n",
    "    \n",
    "    # Buffer settings\n",
    "    st.markdown(\"### 💾 Buffer Settings\")\n",
    "    buffer_limit = st.slider(\"Buffer limit:\", 10, 1000, 100)\n",
    "    auto_clear = st.checkbox(\"Auto-clear old data\", value=True)\n",
    "    \n",
    "    if auto_clear and len(st.session_state.streaming_data) > buffer_limit:\n",
    "        st.session_state.streaming_data = st.session_state.streaming_data[-buffer_limit:]\n",
    "    \n",
    "    # Connection settings\n",
    "    st.markdown(\"### 🔗 Connection Settings\")\n",
    "    reconnect_attempts = st.slider(\"Reconnect attempts:\", 1, 10, 3)\n",
    "    heartbeat_interval = st.selectbox(\n",
    "        \"Heartbeat interval:\",\n",
    "        ['5 seconds', '10 seconds', '30 seconds', '1 minute']\n",
    "    )\n",
    "    \n",
    "    # Monitoring\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### 📊 Monitoring\")\n",
    "    \n",
    "    if st.button(\"🔍 Connection Health\", use_container_width=True):\n",
    "        st.json(streaming_stats)\n",
    "    \n",
    "    if st.button(\"🗑️ Clear Buffer\", use_container_width=True):\n",
    "        st.session_state.streaming_data = []\n",
    "        st.success(\"Buffer cleared!\")\n",
    "        st.rerun()\n",
    "    \n",
    "    if st.button(\"📊 Reset Stats\", use_container_width=True):\n",
    "        streamer.streaming_stats = {\n",
    "            'messages_received': 0,\n",
    "            'last_message': None,\n",
    "            'connection_status': 'disconnected',\n",
    "            'subscription_count': 0\n",
    "        }\n",
    "        st.success(\"Stats reset!\")\n",
    "        st.rerun()\n",
    "    \n",
    "    # Export streaming data\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### 💾 Export\")\n",
    "    \n",
    "    if len(st.session_state.streaming_data) > 0:\n",
    "        export_data = pd.DataFrame(st.session_state.streaming_data)\n",
    "        csv_data = export_data.to_csv(index=False)\n",
    "        \n",
    "        st.download_button(\n",
    "            label=\"📄 Export Stream Data\",\n",
    "            data=csv_data,\n",
    "            file_name=f\"stream_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n",
    "            mime=\"text/csv\",\n",
    "            use_container_width=True\n",
    "        )\n",
    "\n",
    "# Auto-refresh for demo\n",
    "if streaming_stats['connection_status'] == 'connected':\n",
    "    # Auto-simulate updates every 10 seconds for demo\n",
    "    time.sleep(0.1)  # Small delay for demo\n",
    "    \n",
    "    # Add a refresh button for manual updates\n",
    "    if st.button(\"🔄 Manual Refresh\"):\n",
    "        st.rerun()\n",
    "\n",
    "# Footer\n",
    "st.markdown(\"---\")\n",
    "st.markdown(\n",
    "    \"**⚡ Real-time Data Streaming** | \"\n",
    "    \"Live updates with Supabase subscriptions | \"\n",
    "    \"Enterprise-grade real-time analytics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛡️ Section 3: Security and Error Handling (5 minutes)\n",
    "\n",
    "Production applications require robust security and error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security and error handling patterns\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "st.subheader(\"🛡️ Security and Error Handling Best Practices\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "### 1. Authentication and Authorization\n",
    "\n",
    "**Row-Level Security (RLS) with Supabase:**\n",
    "\n",
    "```sql\n",
    "-- Enable RLS on orders table\n",
    "ALTER TABLE orders ENABLE ROW LEVEL SECURITY;\n",
    "\n",
    "-- Policy: Users can only see their own orders\n",
    "CREATE POLICY \"Users can view own orders\" ON orders\n",
    "    FOR SELECT USING (auth.uid() = customer_id);\n",
    "\n",
    "-- Policy: Managers can see all orders\n",
    "CREATE POLICY \"Managers can view all orders\" ON orders\n",
    "    FOR SELECT USING (\n",
    "        EXISTS (\n",
    "            SELECT 1 FROM user_roles \n",
    "            WHERE user_id = auth.uid() \n",
    "            AND role = 'manager'\n",
    "        )\n",
    "    );\n",
    "```\n",
    "\n",
    "**Streamlit Authentication Integration:**\n",
    "\n",
    "```python\n",
    "# Secure session management\n",
    "def authenticate_user(username: str, password: str) -> bool:\n",
    "    # Hash password securely\n",
    "    password_hash = hashlib.sha256(password.encode()).hexdigest()\n",
    "    \n",
    "    # Verify against Supabase auth\n",
    "    try:\n",
    "        response = supabase.auth.sign_in_with_password({\n",
    "            \"email\": username,\n",
    "            \"password\": password\n",
    "        })\n",
    "        return response.user is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Check authentication before data access\n",
    "if 'authenticated' not in st.session_state:\n",
    "    st.session_state.authenticated = False\n",
    "\n",
    "if not st.session_state.authenticated:\n",
    "    st.error(\"Please log in to access this dashboard\")\n",
    "    st.stop()\n",
    "```\n",
    "\n",
    "### 2. Input Validation and Sanitization\n",
    "\n",
    "```python\n",
    "import re\n",
    "from typing import Union\n",
    "\n",
    "def validate_sql_input(user_input: str) -> bool:\n",
    "    \"\"\"Validate user input to prevent SQL injection.\"\"\"\n",
    "    # Block dangerous SQL keywords\n",
    "    dangerous_patterns = [\n",
    "        r'\\b(DROP|DELETE|INSERT|UPDATE|ALTER|CREATE)\\b',\n",
    "        r'[;\\-\\-]',  # SQL injection patterns\n",
    "        r'\\bUNION\\b.*\\bSELECT\\b',\n",
    "        r'\\bEXEC\\b|\\bEXECUTE\\b'\n",
    "    ]\n",
    "    \n",
    "    for pattern in dangerous_patterns:\n",
    "        if re.search(pattern, user_input, re.IGNORECASE):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def sanitize_user_input(user_input: str) -> str:\n",
    "    \"\"\"Sanitize user input for safe database queries.\"\"\"\n",
    "    # Remove potentially dangerous characters\n",
    "    sanitized = re.sub(r'[^\\w\\s\\-@.]', '', user_input)\n",
    "    return sanitized.strip()[:100]  # Limit length\n",
    "\n",
    "# Usage in Streamlit\n",
    "user_filter = st.text_input(\"Search orders:\")\n",
    "if user_filter:\n",
    "    if not validate_sql_input(user_filter):\n",
    "        st.error(\"Invalid input detected\")\n",
    "        st.stop()\n",
    "    \n",
    "    safe_filter = sanitize_user_input(user_filter)\n",
    "    # Use parameterized queries\n",
    "    query = \"SELECT * FROM orders WHERE description ILIKE %s\"\n",
    "    results = execute_query(query, [f\"%{safe_filter}%\"])\n",
    "```\n",
    "\n",
    "### 3. Error Handling and Recovery\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def handle_database_errors():\n",
    "    \"\"\"Context manager for database error handling.\"\"\"\n",
    "    try:\n",
    "        yield\n",
    "    except ConnectionError:\n",
    "        st.error(\"🔌 Database connection lost. Retrying...\")\n",
    "        time.sleep(2)\n",
    "        # Implement reconnection logic\n",
    "    except TimeoutError:\n",
    "        st.warning(\"⏱️ Query timeout. Try reducing data range.\")\n",
    "    except PermissionError:\n",
    "        st.error(\"🚫 Access denied. Check your permissions.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Database error: {e}\")\n",
    "        st.error(\"❌ An unexpected error occurred. Please try again.\")\n",
    "\n",
    "# Usage\n",
    "with handle_database_errors():\n",
    "    data = load_sensitive_data(user_id)\n",
    "    display_dashboard(data)\n",
    "```\n",
    "\n",
    "### 4. Rate Limiting and Performance Protection\n",
    "\n",
    "```python\n",
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def rate_limit(calls_per_minute: int = 60):\n",
    "    \"\"\"Rate limiting decorator for API calls.\"\"\"\n",
    "    def decorator(func):\n",
    "        if not hasattr(func, 'call_times'):\n",
    "            func.call_times = []\n",
    "        \n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            now = time.time()\n",
    "            # Remove calls older than 1 minute\n",
    "            func.call_times = [t for t in func.call_times if now - t < 60]\n",
    "            \n",
    "            if len(func.call_times) >= calls_per_minute:\n",
    "                st.error(f\"Rate limit exceeded. Max {calls_per_minute} calls per minute.\")\n",
    "                return None\n",
    "            \n",
    "            func.call_times.append(now)\n",
    "            return func(*args, **kwargs)\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@rate_limit(calls_per_minute=30)\n",
    "def expensive_database_operation():\n",
    "    # Expensive operation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "### 5. Data Privacy and Compliance\n",
    "\n",
    "```python\n",
    "def mask_sensitive_data(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Mask sensitive data for display.\"\"\"\n",
    "    masked_df = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in masked_df.columns:\n",
    "            if masked_df[col].dtype == 'object':  # String data\n",
    "                masked_df[col] = masked_df[col].apply(\n",
    "                    lambda x: x[:3] + \"***\" + x[-2:] if len(str(x)) > 5 else \"***\"\n",
    "                )\n",
    "            else:  # Numeric data\n",
    "                masked_df[col] = \"[REDACTED]\"\n",
    "    \n",
    "    return masked_df\n",
    "\n",
    "# Usage\n",
    "if user_role != 'admin':\n",
    "    sensitive_columns = ['customer_email', 'phone_number', 'cpf']\n",
    "    display_data = mask_sensitive_data(raw_data, sensitive_columns)\n",
    "else:\n",
    "    display_data = raw_data\n",
    "\n",
    "st.dataframe(display_data)\n",
    "```\n",
    "\"\"\")\n",
    "\n",
    "# Interactive security demo\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"🔒 Interactive Security Demo\")\n",
    "\n",
    "# Simulate authentication\n",
    "demo_col1, demo_col2 = st.columns(2)\n",
    "\n",
    "with demo_col1:\n",
    "    st.markdown(\"**Authentication Simulation**\")\n",
    "    \n",
    "    username = st.text_input(\"Username:\", value=\"demo@olist.com\")\n",
    "    password = st.text_input(\"Password:\", type=\"password\", value=\"demo123\")\n",
    "    \n",
    "    if st.button(\"🔐 Login\"):\n",
    "        # Simulate authentication\n",
    "        if username == \"demo@olist.com\" and password == \"demo123\":\n",
    "            st.success(\"✅ Authentication successful!\")\n",
    "            st.session_state.demo_authenticated = True\n",
    "        else:\n",
    "            st.error(\"❌ Invalid credentials\")\n",
    "            st.session_state.demo_authenticated = False\n",
    "\n",
    "with demo_col2:\n",
    "    st.markdown(\"**Input Validation Demo**\")\n",
    "    \n",
    "    user_input = st.text_input(\"Enter search term:\", placeholder=\"Try: normal text OR DROP TABLE\")\n",
    "    \n",
    "    if user_input:\n",
    "        # Validate input\n",
    "        dangerous_patterns = [r'\\b(DROP|DELETE|INSERT|UPDATE)\\b', r'[;\\-\\-]']\n",
    "        is_safe = True\n",
    "        \n",
    "        for pattern in dangerous_patterns:\n",
    "            if re.search(pattern, user_input, re.IGNORECASE):\n",
    "                is_safe = False\n",
    "                break\n",
    "        \n",
    "        if is_safe:\n",
    "            st.success(f\"✅ Safe input: '{user_input}'\")\n",
    "        else:\n",
    "            st.error(f\"🚫 Dangerous input detected: '{user_input}'\")\n",
    "\n",
    "# Rate limiting demo\n",
    "st.markdown(\"**Rate Limiting Demo**\")\n",
    "\n",
    "if 'api_calls' not in st.session_state:\n",
    "    st.session_state.api_calls = []\n",
    "\n",
    "if st.button(\"📡 Make API Call\"):\n",
    "    now = time.time()\n",
    "    # Remove calls older than 1 minute\n",
    "    st.session_state.api_calls = [\n",
    "        t for t in st.session_state.api_calls if now - t < 60\n",
    "    ]\n",
    "    \n",
    "    if len(st.session_state.api_calls) >= 5:  # Limit for demo\n",
    "        st.error(\"🚫 Rate limit exceeded! Max 5 calls per minute.\")\n",
    "    else:\n",
    "        st.session_state.api_calls.append(now)\n",
    "        st.success(f\"✅ API call successful! ({len(st.session_state.api_calls)}/5 this minute)\")\n",
    "\n",
    "print(\"✅ Security and error handling patterns ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways\n",
    "\n",
    "✅ **Production Integration**: Connection pooling, retry logic, and performance monitoring  \n",
    "✅ **Real-time Streaming**: Live data updates with Supabase subscriptions  \n",
    "✅ **Security Best Practices**: Authentication, input validation, and data protection  \n",
    "✅ **Error Handling**: Robust error recovery and graceful degradation  \n",
    "✅ **Enterprise Features**: Rate limiting, logging, and compliance considerations  \n",
    "\n",
    "## 🔜 Coming Up in Part 3\n",
    "\n",
    "In the final session today, we'll cover deployment and production considerations:\n",
    "\n",
    "**Preview Topics:**\n",
    "- Streamlit Cloud deployment strategies\n",
    "- Environment management and secrets\n",
    "- Performance optimization for production\n",
    "- Monitoring and alerting setup\n",
    "\n",
    "---\n",
    "\n",
    "## 💼 Practice Exercise\n",
    "\n",
    "**Scenario**: Implement a secure real-time order monitoring system for Olist's operations team.\n",
    "\n",
    "**Requirements**:\n",
    "1. **Real-time Updates**: Live order status changes\n",
    "2. **Role-based Access**: Different views for managers vs analysts\n",
    "3. **Error Handling**: Graceful handling of connection issues\n",
    "4. **Security**: Input validation and data masking\n",
    "5. **Performance**: Efficient data loading and caching\n",
    "\n",
    "**Security Constraints**:\n",
    "- Mask customer PII for non-admin users\n",
    "- Rate limit database queries\n",
    "- Validate all user inputs\n",
    "- Implement session timeout\n",
    "\n",
    "**Time**: 15 minutes\n",
    "\n",
    "---\n",
    "\n",
    "*Next: [Part 3 - Deployment & Production →](02_streamlit_advanced_part3_deployment_production.ipynb)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}