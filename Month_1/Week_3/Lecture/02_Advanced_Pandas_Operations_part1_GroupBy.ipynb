{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Pandas Operations - Part 1: GroupBy Operations\n",
        "\n",
        "## Week 3, Day 2 (Thursday) - April 24th, 2025\n",
        "\n",
        "### Overview\n",
        "This session focuses on GroupBy operations in Pandas, which are similar to SQL's GROUP BY clause. GroupBy allows you to split your data into groups based on some criteria, apply a function to each group independently, and then combine the results. This is one of the most powerful features of Pandas for data analysis.\n",
        "\n",
        "### Learning Objectives\n",
        "- Understand the split-apply-combine paradigm in Pandas\n",
        "- Master GroupBy operations and their SQL equivalents\n",
        "- Learn various aggregation techniques with GroupBy\n",
        "- Apply transformations to groups\n",
        "- Perform complex grouping operations with multiple criteria\n",
        "\n",
        "### Prerequisites\n",
        "- Python fundamentals (Week 1)\n",
        "- Pandas Fundamentals I & II (Week 2, Day 2 & Week 3, Day 1)\n",
        "- SQL knowledge (prior to course)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to GroupBy Operations\n",
        "\n",
        "GroupBy operations are built on the **split-apply-combine** paradigm:\n",
        "\n",
        "1. **Split**: Data is split into groups based on one or more keys\n",
        "2. **Apply**: A function is applied to each group independently\n",
        "3. **Combine**: The results are combined into a new data structure\n",
        "\n",
        "This is analogous to SQL's GROUP BY clause. Let's start by creating a sample dataset to explore these concepts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a sample sales dataset\n",
        "data = {\n",
        "    'date': pd.date_range('2025-01-01', periods=30, freq='D'),\n",
        "    'product_id': np.random.choice(['P001', 'P002', 'P003', 'P004', 'P005'], size=30),\n",
        "    'category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Books'], size=30),\n",
        "    'store_id': np.random.choice(['S01', 'S02', 'S03'], size=30),\n",
        "    'region': np.random.choice(['North', 'South', 'East', 'West'], size=30),\n",
        "    'quantity': np.random.randint(1, 10, size=30),\n",
        "    'unit_price': np.random.uniform(10, 100, size=30).round(2),\n",
        "    'discount': np.random.choice([0, 0.1, 0.2, 0.3], size=30)\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "sales_df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate total price\n",
        "sales_df['total_price'] = (sales_df['quantity'] * sales_df['unit_price'] * (1 - sales_df['discount'])).round(2)\n",
        "\n",
        "# Display the first few rows\n",
        "print(\"Sample Sales DataFrame:\")\n",
        "print(sales_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Basic GroupBy Operations\n",
        "\n",
        "The simplest GroupBy operation involves grouping data by a single column and applying an aggregation function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by product category and calculate total sales\n",
        "category_sales = sales_df.groupby('category')['total_price'].sum().reset_index()\n",
        "print(\"Total sales by category:\")\n",
        "print(category_sales)\n",
        "\n",
        "# SQL equivalent:\n",
        "# SELECT category, SUM(total_price) as total_price\n",
        "# FROM sales\n",
        "# GROUP BY category\n",
        "\n",
        "# Visualize the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(category_sales['category'], category_sales['total_price'])\n",
        "plt.title('Total Sales by Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Total Sales ($)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by store and calculate multiple statistics\n",
        "store_stats = sales_df.groupby('store_id').agg({\n",
        "    'total_price': 'sum',\n",
        "    'quantity': 'sum',\n",
        "    'unit_price': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns for clarity\n",
        "store_stats.columns = ['store_id', 'total_sales', 'total_quantity', 'avg_unit_price']\n",
        "\n",
        "print(\"Store statistics:\")\n",
        "print(store_stats)\n",
        "\n",
        "# SQL equivalent:\n",
        "# SELECT \n",
        "#     store_id, \n",
        "#     SUM(total_price) as total_sales, \n",
        "#     SUM(quantity) as total_quantity, \n",
        "#     AVG(unit_price) as avg_unit_price\n",
        "# FROM sales\n",
        "# GROUP BY store_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Grouping by Multiple Columns\n",
        "\n",
        "Just like in SQL, we can group by multiple columns to create more detailed aggregations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by store and category\n",
        "store_category_sales = sales_df.groupby(['store_id', 'category'])['total_price'].sum().reset_index()\n",
        "print(\"Total sales by store and category:\")\n",
        "print(store_category_sales)\n",
        "\n",
        "# SQL equivalent:\n",
        "# SELECT store_id, category, SUM(total_price) as total_price\n",
        "# FROM sales\n",
        "# GROUP BY store_id, category\n",
        "\n",
        "# Pivot the result for better visualization\n",
        "pivoted = store_category_sales.pivot(index='store_id', columns='category', values='total_price')\n",
        "print(\"\\nPivoted view:\")\n",
        "print(pivoted)\n",
        "\n",
        "# Visualize the results with a grouped bar chart\n",
        "store_category_sales.pivot(index='category', columns='store_id', values='total_price').plot(kind='bar', figsize=(12, 6))\n",
        "plt.title('Sales by Category and Store')\n",
        "plt.ylabel('Total Sales ($)')\n",
        "plt.xlabel('Category')\n",
        "plt.legend(title='Store')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by date (truncated to week) and region\n",
        "sales_df['week'] = sales_df['date'].dt.to_period('W')\n",
        "weekly_region_sales = sales_df.groupby(['week', 'region'])['total_price'].sum().reset_index()\n",
        "print(\"Weekly sales by region:\")\n",
        "print(weekly_region_sales)\n",
        "\n",
        "# SQL equivalent (PostgreSQL):\n",
        "# SELECT \n",
        "#     DATE_TRUNC('week', date) as week,\n",
        "#     region,\n",
        "#     SUM(total_price) as total_price\n",
        "# FROM sales\n",
        "# GROUP BY DATE_TRUNC('week', date), region\n",
        "# ORDER BY week, region\n",
        "\n",
        "# Create a line plot for weekly sales by region\n",
        "weekly_pivot = weekly_region_sales.pivot(index='week', columns='region', values='total_price')\n",
        "weekly_pivot.plot(figsize=(12, 6), marker='o')\n",
        "plt.title('Weekly Sales by Region')\n",
        "plt.ylabel('Total Sales ($)')\n",
        "plt.xlabel('Week')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Aggregation Functions\n",
        "\n",
        "Pandas provides several built-in aggregation functions, similar to SQL's aggregate functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic aggregation functions\n",
        "category_aggs = sales_df.groupby('category').agg({\n",
        "    'total_price': ['sum', 'mean', 'min', 'max', 'count', 'std'],\n",
        "    'quantity': ['sum', 'mean', 'min', 'max']\n",
        "})\n",
        "\n",
        "print(\"Multiple aggregations by category:\")\n",
        "print(category_aggs)\n",
        "\n",
        "# SQL equivalent:\n",
        "# SELECT \n",
        "#     category,\n",
        "#     SUM(total_price) as total_price_sum,\n",
        "#     AVG(total_price) as total_price_mean,\n",
        "#     MIN(total_price) as total_price_min,\n",
        "#     MAX(total_price) as total_price_max,\n",
        "#     COUNT(total_price) as total_price_count,\n",
        "#     STDDEV(total_price) as total_price_std,\n",
        "#     SUM(quantity) as quantity_sum,\n",
        "#     AVG(quantity) as quantity_mean,\n",
        "#     MIN(quantity) as quantity_min,\n",
        "#     MAX(quantity) as quantity_max\n",
        "# FROM sales\n",
        "# GROUP BY category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flattening multi-level column names for easier access\n",
        "category_aggs.columns = ['_'.join(col).strip() for col in category_aggs.columns.values]\n",
        "category_aggs.reset_index(inplace=True)\n",
        "print(\"Flattened column names:\")\n",
        "print(category_aggs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Custom Aggregation Functions\n",
        "\n",
        "In addition to built-in aggregation functions, we can define custom aggregations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define custom aggregation functions\n",
        "def range_calc(x):\n",
        "    return x.max() - x.min()\n",
        "\n",
        "def discount_pct(x):\n",
        "    return (x > 0).mean() * 100  # Percentage of sales with discounts\n",
        "\n",
        "# Apply custom aggregations\n",
        "custom_aggs = sales_df.groupby('category').agg({\n",
        "    'total_price': ['sum', range_calc],  # Mix of built-in and custom\n",
        "    'unit_price': lambda x: x.quantile(0.75) - x.quantile(0.25),  # IQR\n",
        "    'discount': discount_pct  # Custom function\n",
        "})\n",
        "\n",
        "# Rename columns\n",
        "custom_aggs.columns = ['_'.join(col).strip() for col in custom_aggs.columns.values]\n",
        "custom_aggs.rename(columns={\n",
        "    'total_price_range_calc': 'total_price_range',\n",
        "    'unit_price_<lambda>': 'unit_price_iqr',\n",
        "    'discount_discount_pct': 'pct_discounted'\n",
        "}, inplace=True)\n",
        "\n",
        "print(\"Custom aggregations:\")\n",
        "print(custom_aggs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. The GroupBy Object\n",
        "\n",
        "When you call `.groupby()`, Pandas returns a GroupBy object that you can use to perform various operations on the groups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a GroupBy object\n",
        "grouped = sales_df.groupby('category')\n",
        "\n",
        "# Examine the GroupBy object\n",
        "print(f\"Type of grouped: {type(grouped)}\")\n",
        "print(f\"Groups: {list(grouped.groups.keys())}\")\n",
        "\n",
        "# Get the group for 'Electronics'\n",
        "electronics_group = grouped.get_group('Electronics')\n",
        "print(\"\\nElectronics group:\")\n",
        "print(electronics_group.head())\n",
        "\n",
        "# Iterate through groups\n",
        "print(\"\\nIterate through first 2 rows of each group:\")\n",
        "for name, group in grouped:\n",
        "    print(f\"\\nGroup: {name}\")\n",
        "    print(group.head(2))  # Show first 2 rows of each group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Transformation with GroupBy\n",
        "\n",
        "The `.transform()` method applies a function to each group and returns a Series or DataFrame with the same shape as the original, making it ideal for creating features based on group statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create copies of the DataFrame to avoid modifying the original\n",
        "sales_transform = sales_df.copy()\n",
        "\n",
        "# Add group averages to each row\n",
        "sales_transform['category_avg_price'] = sales_transform.groupby('category')['total_price'].transform('mean')\n",
        "\n",
        "# Calculate percentage of category average\n",
        "sales_transform['pct_of_category_avg'] = (sales_transform['total_price'] / sales_transform['category_avg_price'] * 100).round(2)\n",
        "\n",
        "print(\"DataFrame with transformed values:\")\n",
        "print(sales_transform[['category', 'total_price', 'category_avg_price', 'pct_of_category_avg']].head(10))\n",
        "\n",
        "# SQL equivalent (using window functions):\n",
        "# SELECT\n",
        "#     category,\n",
        "#     total_price,\n",
        "#     AVG(total_price) OVER (PARTITION BY category) as category_avg_price,\n",
        "#     (total_price / AVG(total_price) OVER (PARTITION BY category)) * 100 as pct_of_category_avg\n",
        "# FROM sales\n",
        "\n",
        "# Find sales above/below category average\n",
        "above_avg = sales_transform[sales_transform['pct_of_category_avg'] > 100]\n",
        "print(f\"\\nPercentage of sales above category average: {len(above_avg) / len(sales_transform) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multiple transformations at once\n",
        "sales_transform = sales_df.copy()\n",
        "\n",
        "# Apply multiple statistics as transforms\n",
        "category_stats = sales_transform.groupby('category')['total_price'].transform(['mean', 'min', 'max', 'std'])\n",
        "sales_transform[category_stats.columns] = category_stats\n",
        "\n",
        "# Calculate Z-score relative to category\n",
        "sales_transform['z_score'] = (sales_transform['total_price'] - sales_transform['mean']) / sales_transform['std']\n",
        "\n",
        "print(\"DataFrame with multiple transforms and Z-score:\")\n",
        "cols_to_show = ['category', 'total_price', 'mean', 'std', 'z_score']\n",
        "print(sales_transform[cols_to_show].head(10))\n",
        "\n",
        "# Identify outlier sales (Z-score > 1.5 or < -1.5)\n",
        "outliers = sales_transform[abs(sales_transform['z_score']) > 1.5]\n",
        "print(f\"\\nNumber of outlier sales: {len(outliers)}\")\n",
        "if len(outliers) > 0:\n",
        "    print(\"Sample of outliers:\")\n",
        "    print(outliers[cols_to_show].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Filtering with GroupBy\n",
        "\n",
        "The `.filter()` method allows you to filter entire groups based on a condition:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter categories that have more than 8 sales\n",
        "high_volume_categories = sales_df.groupby('category').filter(lambda x: len(x) > 8)\n",
        "print(f\"Categories with more than 8 sales: {high_volume_categories['category'].unique()}\")\n",
        "print(f\"Original DataFrame shape: {sales_df.shape}, Filtered shape: {high_volume_categories.shape}\")\n",
        "\n",
        "# Filter stores with total sales > $400\n",
        "high_sales_stores = sales_df.groupby('store_id').filter(lambda x: x['total_price'].sum() > 400)\n",
        "print(f\"\\nStores with total sales > $400: {high_sales_stores['store_id'].unique()}\")\n",
        "print(f\"Original DataFrame shape: {sales_df.shape}, Filtered shape: {high_sales_stores.shape}\")\n",
        "\n",
        "# SQL equivalent (for the high sales stores):\n",
        "# SELECT *\n",
        "# FROM sales\n",
        "# WHERE store_id IN (\n",
        "#     SELECT store_id \n",
        "#     FROM sales \n",
        "#     GROUP BY store_id \n",
        "#     HAVING SUM(total_price) > 400\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. GroupBy with Time Series Data\n",
        "\n",
        "GroupBy is particularly useful with time series data, allowing you to aggregate data by various time periods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by day\n",
        "daily_sales = sales_df.groupby(sales_df['date'].dt.date)['total_price'].sum().reset_index()\n",
        "daily_sales.columns = ['date', 'daily_sales']\n",
        "print(\"Daily sales:\")\n",
        "print(daily_sales.head())\n",
        "\n",
        "# Group by week\n",
        "weekly_sales = sales_df.groupby(pd.Grouper(key='date', freq='W'))['total_price'].sum().reset_index()\n",
        "weekly_sales.columns = ['week_ending', 'weekly_sales']\n",
        "print(\"\\nWeekly sales:\")\n",
        "print(weekly_sales)\n",
        "\n",
        "# Group by day of week\n",
        "sales_df['day_of_week'] = sales_df['date'].dt.day_name()\n",
        "day_of_week_sales = sales_df.groupby('day_of_week')['total_price'].sum().reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
        "print(\"\\nSales by day of week:\")\n",
        "print(day_of_week_sales)\n",
        "\n",
        "# Plot daily and weekly sales\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# Daily sales\n",
        "axes[0].plot(daily_sales['date'], daily_sales['daily_sales'], marker='o')\n",
        "axes[0].set_title('Daily Sales')\n",
        "axes[0].set_ylabel('Sales ($)')\n",
        "axes[0].grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Weekly sales\n",
        "axes[1].plot(weekly_sales['week_ending'], weekly_sales['weekly_sales'], marker='s', linewidth=2)\n",
        "axes[1].set_title('Weekly Sales')\n",
        "axes[1].set_ylabel('Sales ($)')\n",
        "axes[1].grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot sales by day of week\n",
        "plt.figure(figsize=(10, 6))\n",
        "day_of_week_sales.plot(kind='bar')\n",
        "plt.title('Sales by Day of Week')\n",
        "plt.ylabel('Total Sales ($)')\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Applying Multiple Functions with Named Aggregation\n",
        "\n",
        "Pandas 0.25+ introduced a more readable way to perform named aggregations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Named aggregation\n",
        "named_aggs = sales_df.groupby('category').agg(\n",
        "    total_sales=('total_price', 'sum'),\n",
        "    avg_price=('total_price', 'mean'),\n",
        "    num_sales=('total_price', 'count'),\n",
        "    min_price=('total_price', 'min'),\n",
        "    max_price=('total_price', 'max'),\n",
        "    total_quantity=('quantity', 'sum'),\n",
        "    avg_quantity=('quantity', 'mean')\n",
        ")\n",
        "\n",
        "print(\"Named aggregation results:\")\n",
        "print(named_aggs)\n",
        "\n",
        "# SQL equivalent:\n",
        "# SELECT \n",
        "#     category,\n",
        "#     SUM(total_price) as total_sales,\n",
        "#     AVG(total_price) as avg_price,\n",
        "#     COUNT(total_price) as num_sales,\n",
        "#     MIN(total_price) as min_price,\n",
        "#     MAX(total_price) as max_price,\n",
        "#     SUM(quantity) as total_quantity,\n",
        "#     AVG(quantity) as avg_quantity\n",
        "# FROM sales\n",
        "# GROUP BY category"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. SQL to Pandas GroupBy Translation Guide\n",
        "\n",
        "Here's a reference guide for translating common SQL GROUP BY operations to Pandas GroupBy:\n",
        "\n",
        "| SQL Operation | Pandas Equivalent |\n",
        "|--------------|-------------------|\n",
        "| `SELECT col1, SUM(col2) FROM table GROUP BY col1` | `df.groupby('col1')['col2'].sum()` |\n",
        "| `SELECT col1, col2, SUM(col3) FROM table GROUP BY col1, col2` | `df.groupby(['col1', 'col2'])['col3'].sum()` |\n",
        "| `SELECT col1, AVG(col2) FROM table GROUP BY col1` | `df.groupby('col1')['col2'].mean()` |\n",
        "| `SELECT col1, COUNT(*) FROM table GROUP BY col1` | `df.groupby('col1').size()` |\n",
        "| `SELECT col1, COUNT(col2) FROM table GROUP BY col1` | `df.groupby('col1')['col2'].count()` |\n",
        "| `SELECT col1, SUM(col2), AVG(col3) FROM table GROUP BY col1` | `df.groupby('col1').agg({'col2': 'sum', 'col3': 'mean'})` |\n",
        "| `SELECT col1, SUM(col2) FROM table GROUP BY col1 HAVING SUM(col2) > 100` | `df.groupby('col1').filter(lambda x: x['col2'].sum() > 100)` |\n",
        "| `SELECT col1, SUM(col2) AS sum_col2 FROM table GROUP BY col1 ORDER BY sum_col2 DESC` | `df.groupby('col1')['col2'].sum().sort_values(ascending=False)` |\n",
        "| `SELECT col1, col2, AVG(col3) OVER (PARTITION BY col1)` | `df['avg_col3'] = df.groupby('col1')['col3'].transform('mean')` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Practice Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Basic GroupBy\n",
        "Calculate the total quantity and average unit price for each region in the `sales_df` DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Multiple Group Keys\n",
        "Group the `sales_df` DataFrame by both `region` and `store_id`, and calculate the total sales. Sort the results by total sales in descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Transform\n",
        "Add a column to the `sales_df` DataFrame that shows what percentage each transaction's quantity is of the store's average quantity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4: SQL to Pandas Translation\n",
        "Translate the following SQL query to Pandas code:\n",
        "```sql\n",
        "SELECT \n",
        "    category,\n",
        "    region,\n",
        "    COUNT(*) as num_sales,\n",
        "    SUM(total_price) as total_sales,\n",
        "    AVG(discount) as avg_discount\n",
        "FROM sales\n",
        "GROUP BY category, region\n",
        "HAVING COUNT(*) > 2\n",
        "ORDER BY total_sales DESC\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5: Time-Based Grouping\n",
        "Group the `sales_df` DataFrame by week and calculate the average daily sales for each week. Then, identify the week with the highest average daily sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "In the next parts of today's session, we'll continue with:\n",
        "- Part 2: More advanced aggregation functions\n",
        "- Part 3: Pivot tables and cross-tabulations\n",
        "\n",
        "Continue to Part 2: Advanced Aggregation Functions when you're ready to proceed."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
