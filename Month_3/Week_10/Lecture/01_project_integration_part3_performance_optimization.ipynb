{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance_header"
   },
   "source": [
    "# Week 10 - Project Integration and Polish\n",
    "## Part 3: Performance Optimization and Caching\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this session, you will be able to:\n",
    "- Implement advanced caching strategies for large datasets\n",
    "- Optimize database queries and data loading\n",
    "- Handle real-time data updates efficiently\n",
    "- Monitor and debug performance issues\n",
    "- Scale applications for multiple concurrent users\n",
    "- Implement progressive loading and user feedback\n",
    "\n",
    "### Business Context\n",
    "Performance directly impacts user adoption and business value:\n",
    "\n",
    "- **User Experience**: Slow apps frustrate stakeholders and reduce usage\n",
    "- **Operational Costs**: Inefficient apps consume more resources\n",
    "- **Scalability**: Poor performance limits how many users can access insights\n",
    "- **Reliability**: Performance issues can cause app crashes and data loss\n",
    "\n",
    "**The Goal**: Create applications that are fast, reliable, and scalable for business use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_imports"
   },
   "outputs": [],
   "source": [
    "# Performance optimization imports\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "import hashlib\n",
    "import pickle\n",
    "import os\n",
    "from functools import wraps\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Helper utilities\n",
    "import sys\n",
    "sys.path.append('/content/python-data-analysis-course')\n",
    "from Utilities.colab_helper import setup_colab\n",
    "from Utilities.olist_helper import load_olist_data\n",
    "\n",
    "setup_colab()\n",
    "print(\"✅ Performance optimization environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance_fundamentals"
   },
   "source": [
    "## 1. Performance Fundamentals\n",
    "\n",
    "### Understanding Performance Bottlenecks\n",
    "\n",
    "**Common Performance Issues in Data Applications**:\n",
    "\n",
    "1. **Data Loading**: Reading large datasets repeatedly\n",
    "2. **Data Processing**: Complex calculations on every interaction\n",
    "3. **Visualization**: Rendering complex charts with too much data\n",
    "4. **Network Requests**: API calls and database queries\n",
    "5. **Memory Usage**: Loading everything into memory\n",
    "6. **State Management**: Inefficient session state handling\n",
    "\n",
    "### Performance Measurement\n",
    "\n",
    "```python\n",
    "# Performance monitoring utilities\n",
    "def measure_performance(func):\n",
    "    \"\"\"Decorator to measure function execution time\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"{func.__name__} executed in {execution_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / 1024 / 1024  # Convert to MB\n",
    "```\n",
    "\n",
    "### Performance Targets for Business Apps\n",
    "\n",
    "| Metric | Target | Business Impact |\n",
    "|--------|--------|----------------|\n",
    "| Initial Load | < 10 seconds | User retention |\n",
    "| User Interactions | < 2 seconds | User satisfaction |\n",
    "| Data Refresh | < 5 seconds | Workflow efficiency |\n",
    "| Memory Usage | < 1GB | Cost efficiency |\n",
    "| Concurrent Users | 10-50 users | Business scalability |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "advanced_caching"
   },
   "source": [
    "## 2. Advanced Caching Strategies\n",
    "\n",
    "### Understanding Streamlit Caching\n",
    "\n",
    "Streamlit provides two main caching decorators:\n",
    "\n",
    "- **`@st.cache_data`**: For data loading and processing\n",
    "- **`@st.cache_resource`**: For global resources (database connections, models)\n",
    "\n",
    "### Pattern 1: Hierarchical Data Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hierarchical_caching"
   },
   "outputs": [],
   "source": [
    "# Advanced caching patterns\n",
    "\n",
    "# Level 1: Raw data caching (rarely changes)\n",
    "@st.cache_data(ttl=3600)  # Cache for 1 hour\n",
    "def load_raw_olist_data():\n",
    "    \"\"\"Load raw Olist datasets - cached for long periods\"\"\"\n",
    "    with st.spinner(\"Loading raw datasets...\"):\n",
    "        return {\n",
    "            'orders': load_olist_data('olist_orders_dataset'),\n",
    "            'reviews': load_olist_data('olist_order_reviews_dataset'),\n",
    "            'customers': load_olist_data('olist_customers_dataset'),\n",
    "            'products': load_olist_data('olist_products_dataset')\n",
    "        }\n",
    "\n",
    "# Level 2: Processed data caching (changes with parameters)\n",
    "@st.cache_data(ttl=600)  # Cache for 10 minutes\n",
    "def prepare_analysis_data(date_range, customer_states):\n",
    "    \"\"\"Prepare data based on user filters - shorter cache\"\"\"\n",
    "    raw_data = load_raw_olist_data()\n",
    "    \n",
    "    # Filter and process based on parameters\n",
    "    start_date, end_date = date_range\n",
    "    \n",
    "    orders = raw_data['orders']\n",
    "    orders['order_purchase_timestamp'] = pd.to_datetime(orders['order_purchase_timestamp'])\n",
    "    \n",
    "    # Apply filters\n",
    "    filtered_orders = orders[\n",
    "        (orders['order_purchase_timestamp'].dt.date >= start_date) &\n",
    "        (orders['order_purchase_timestamp'].dt.date <= end_date)\n",
    "    ]\n",
    "    \n",
    "    # Merge with other datasets\n",
    "    analysis_data = filtered_orders.merge(raw_data['reviews'], on='order_id', how='inner')\n",
    "    analysis_data = analysis_data.merge(raw_data['customers'], on='customer_id', how='left')\n",
    "    \n",
    "    if customer_states:\n",
    "        analysis_data = analysis_data[analysis_data['customer_state'].isin(customer_states)]\n",
    "    \n",
    "    return analysis_data\n",
    "\n",
    "# Level 3: Analysis results caching (changes with all parameters)\n",
    "@st.cache_data(ttl=300)  # Cache for 5 minutes\n",
    "def calculate_performance_metrics(data, metric_type):\n",
    "    \"\"\"Calculate specific metrics - shortest cache\"\"\"\n",
    "    \n",
    "    if metric_type == 'satisfaction':\n",
    "        return {\n",
    "            'avg_score': data['review_score'].mean(),\n",
    "            'satisfaction_dist': data['review_score'].value_counts().to_dict(),\n",
    "            'total_reviews': len(data)\n",
    "        }\n",
    "    elif metric_type == 'delivery':\n",
    "        data['delivery_days'] = (pd.to_datetime(data['order_delivered_customer_date']) - \n",
    "                                pd.to_datetime(data['order_purchase_timestamp'])).dt.days\n",
    "        return {\n",
    "            'avg_delivery_days': data['delivery_days'].mean(),\n",
    "            'delivery_dist': data['delivery_days'].describe().to_dict()\n",
    "        }\n",
    "\n",
    "print(\"✅ Advanced caching patterns defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cache_management"
   },
   "source": [
    "### Pattern 2: Smart Cache Invalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smart_cache_invalidation"
   },
   "outputs": [],
   "source": [
    "# Smart cache management\n",
    "\n",
    "def create_cache_key(*args, **kwargs):\n",
    "    \"\"\"Create a unique cache key from parameters\"\"\"\n",
    "    key_string = str(args) + str(sorted(kwargs.items()))\n",
    "    return hashlib.md5(key_string.encode()).hexdigest()\n",
    "\n",
    "@st.cache_data(ttl=1800)  # 30 minute cache\n",
    "def load_filtered_data(date_start, date_end, states, _cache_key=None):\n",
    "    \"\"\"Load data with manual cache key for complex invalidation\"\"\"\n",
    "    # The _cache_key parameter allows manual cache control\n",
    "    # Changing this key will invalidate the cache\n",
    "    \n",
    "    print(f\"Loading data for period {date_start} to {date_end}\")\n",
    "    print(f\"States: {states}\")\n",
    "    print(f\"Cache key: {_cache_key}\")\n",
    "    \n",
    "    # Simulate data loading\n",
    "    time.sleep(2)  # Simulate slow operation\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'data_loaded_at': [datetime.now()],\n",
    "        'cache_key': [_cache_key],\n",
    "        'filter_applied': [f\"{date_start} to {date_end}, states: {len(states)}\"]\n",
    "    })\n",
    "\n",
    "def get_data_with_smart_caching(date_start, date_end, states):\n",
    "    \"\"\"Wrapper function that manages cache keys intelligently\"\"\"\n",
    "    \n",
    "    # Create cache key based on parameters\n",
    "    cache_key = create_cache_key(date_start, date_end, tuple(sorted(states)))\n",
    "    \n",
    "    # Add time-based component (optional - for periodic refresh)\n",
    "    hour_key = datetime.now().strftime('%Y%m%d%H')  # Refresh every hour\n",
    "    final_cache_key = f\"{cache_key}_{hour_key}\"\n",
    "    \n",
    "    return load_filtered_data(date_start, date_end, states, _cache_key=final_cache_key)\n",
    "\n",
    "# Cache monitoring and clearing\n",
    "def display_cache_info():\n",
    "    \"\"\"Display current cache status\"\"\"\n",
    "    if st.button(\"Clear All Caches\"):\n",
    "        st.cache_data.clear()\n",
    "        st.success(\"All caches cleared!\")\n",
    "        st.experimental_rerun()\n",
    "    \n",
    "    st.info(\"💡 Tip: Caches automatically expire based on TTL (Time To Live) settings\")\n",
    "\n",
    "print(\"✅ Smart cache invalidation patterns defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "database_optimization"
   },
   "source": [
    "## 3. Database Integration and Query Optimization\n",
    "\n",
    "### Efficient Database Patterns\n",
    "\n",
    "When working with databases (like Supabase), optimization is crucial for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "database_patterns"
   },
   "outputs": [],
   "source": [
    "# Database optimization patterns\n",
    "\n",
    "# Pattern 1: Connection pooling\n",
    "@st.cache_resource\n",
    "def get_database_connection():\n",
    "    \"\"\"Create a cached database connection - shared across users\"\"\"\n",
    "    # In real app, this would be your database connection\n",
    "    print(\"Creating database connection...\")\n",
    "    \n",
    "    # Simulated connection object\n",
    "    class MockConnection:\n",
    "        def __init__(self):\n",
    "            self.created_at = datetime.now()\n",
    "            self.query_count = 0\n",
    "        \n",
    "        def execute_query(self, query):\n",
    "            self.query_count += 1\n",
    "            print(f\"Executing query #{self.query_count}: {query[:50]}...\")\n",
    "            time.sleep(0.1)  # Simulate query time\n",
    "            return f\"Query result #{self.query_count}\"\n",
    "    \n",
    "    return MockConnection()\n",
    "\n",
    "# Pattern 2: Query optimization with parameters\n",
    "@st.cache_data(ttl=600)\n",
    "def execute_optimized_query(query, params=None, _connection=None):\n",
    "    \"\"\"Execute database query with caching and optimization\"\"\"\n",
    "    \n",
    "    if _connection is None:\n",
    "        _connection = get_database_connection()\n",
    "    \n",
    "    # Add query optimization hints\n",
    "    optimized_query = optimize_query(query, params)\n",
    "    \n",
    "    return _connection.execute_query(optimized_query)\n",
    "\n",
    "def optimize_query(query, params):\n",
    "    \"\"\"Apply query optimization techniques\"\"\"\n",
    "    \n",
    "    # Example optimizations:\n",
    "    optimizations = []\n",
    "    \n",
    "    # Add LIMIT for large result sets\n",
    "    if 'LIMIT' not in query.upper() and params and params.get('limit'):\n",
    "        query += f\" LIMIT {params['limit']}\"\n",
    "        optimizations.append(\"Added LIMIT clause\")\n",
    "    \n",
    "    # Add indexes hints (database specific)\n",
    "    if params and params.get('use_index'):\n",
    "        # This would be database-specific optimization\n",
    "        optimizations.append(\"Added index hint\")\n",
    "    \n",
    "    if optimizations:\n",
    "        print(f\"Query optimizations applied: {', '.join(optimizations)}\")\n",
    "    \n",
    "    return query\n",
    "\n",
    "# Pattern 3: Chunked data loading\n",
    "@st.cache_data(ttl=1800)\n",
    "def load_large_dataset_chunked(table_name, chunk_size=10000, max_rows=None):\n",
    "    \"\"\"Load large datasets in chunks to manage memory\"\"\"\n",
    "    \n",
    "    connection = get_database_connection()\n",
    "    \n",
    "    # Simulate chunked loading\n",
    "    chunks = []\n",
    "    offset = 0\n",
    "    total_loaded = 0\n",
    "    \n",
    "    progress_bar = st.progress(0)\n",
    "    status_text = st.empty()\n",
    "    \n",
    "    while True:\n",
    "        # Construct chunked query\n",
    "        query = f\"SELECT * FROM {table_name} OFFSET {offset} LIMIT {chunk_size}\"\n",
    "        \n",
    "        # Simulate chunk loading\n",
    "        chunk_data = connection.execute_query(query)\n",
    "        \n",
    "        # Simulate chunk size (in real app, this would be actual data)\n",
    "        simulated_chunk_size = min(chunk_size, (max_rows or 100000) - total_loaded)\n",
    "        \n",
    "        if simulated_chunk_size <= 0:\n",
    "            break\n",
    "        \n",
    "        chunks.append(f\"Chunk {len(chunks) + 1}: {simulated_chunk_size} rows\")\n",
    "        total_loaded += simulated_chunk_size\n",
    "        offset += chunk_size\n",
    "        \n",
    "        # Update progress\n",
    "        if max_rows:\n",
    "            progress = total_loaded / max_rows\n",
    "            progress_bar.progress(min(progress, 1.0))\n",
    "            status_text.text(f\"Loaded {total_loaded:,} of {max_rows:,} rows\")\n",
    "        \n",
    "        # Break if chunk was smaller than expected (end of data)\n",
    "        if simulated_chunk_size < chunk_size:\n",
    "            break\n",
    "    \n",
    "    progress_bar.empty()\n",
    "    status_text.empty()\n",
    "    \n",
    "    return {\n",
    "        'chunks': chunks,\n",
    "        'total_rows': total_loaded,\n",
    "        'chunk_count': len(chunks)\n",
    "    }\n",
    "\n",
    "print(\"✅ Database optimization patterns defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "memory_optimization"
   },
   "source": [
    "## 4. Memory Management and Optimization\n",
    "\n",
    "### Memory-Efficient Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "memory_optimization_patterns"
   },
   "outputs": [],
   "source": [
    "# Memory optimization patterns\n",
    "\n",
    "def optimize_dataframe_memory(df):\n",
    "    \"\"\"Optimize DataFrame memory usage by adjusting data types\"\"\"\n",
    "    \n",
    "    memory_before = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB\n",
    "    \n",
    "    # Optimize numeric columns\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    # Optimize string columns\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        num_unique = df[col].nunique()\n",
    "        num_total = len(df)\n",
    "        \n",
    "        # If less than 50% unique values, convert to category\n",
    "        if num_unique / num_total < 0.5:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    memory_after = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB\n",
    "    reduction = (memory_before - memory_after) / memory_before * 100\n",
    "    \n",
    "    return df, {\n",
    "        'memory_before_mb': memory_before,\n",
    "        'memory_after_mb': memory_after,\n",
    "        'reduction_percent': reduction\n",
    "    }\n",
    "\n",
    "# Pattern: Data sampling for large datasets\n",
    "def smart_data_sampling(df, max_rows=50000, sample_method='random'):\n",
    "    \"\"\"Intelligently sample data for performance while maintaining insights\"\"\"\n",
    "    \n",
    "    if len(df) <= max_rows:\n",
    "        return df, {'sampled': False, 'original_rows': len(df)}\n",
    "    \n",
    "    if sample_method == 'random':\n",
    "        sampled_df = df.sample(n=max_rows, random_state=42)\n",
    "    elif sample_method == 'stratified' and 'category_column' in df.columns:\n",
    "        # Stratified sampling to maintain category proportions\n",
    "        sampled_df = df.groupby('category_column', group_keys=False).apply(\n",
    "            lambda x: x.sample(min(len(x), max_rows // df['category_column'].nunique()))\n",
    "        )\n",
    "    else:\n",
    "        # Time-based sampling (if datetime column exists)\n",
    "        sampled_df = df.iloc[::len(df)//max_rows]\n",
    "    \n",
    "    return sampled_df, {\n",
    "        'sampled': True,\n",
    "        'original_rows': len(df),\n",
    "        'sampled_rows': len(sampled_df),\n",
    "        'sample_ratio': len(sampled_df) / len(df)\n",
    "    }\n",
    "\n",
    "# Pattern: Lazy data loading\n",
    "class LazyDataLoader:\n",
    "    \"\"\"Load data only when needed\"\"\"\n",
    "    \n",
    "    def __init__(self, data_source):\n",
    "        self.data_source = data_source\n",
    "        self._data = None\n",
    "        self._metadata = None\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        if self._data is None:\n",
    "            print(f\"Loading data from {self.data_source}...\")\n",
    "            # In real app, this would load from file/database\n",
    "            self._data = self._load_data()\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def metadata(self):\n",
    "        if self._metadata is None:\n",
    "            # Load just metadata without full data\n",
    "            self._metadata = self._load_metadata()\n",
    "        return self._metadata\n",
    "    \n",
    "    def _load_data(self):\n",
    "        # Simulate data loading\n",
    "        time.sleep(1)\n",
    "        return pd.DataFrame({\n",
    "            'id': range(1000),\n",
    "            'value': np.random.randn(1000),\n",
    "            'category': np.random.choice(['A', 'B', 'C'], 1000)\n",
    "        })\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        # Quick metadata load\n",
    "        return {\n",
    "            'source': self.data_source,\n",
    "            'estimated_rows': 1000,\n",
    "            'last_updated': datetime.now() - timedelta(hours=2)\n",
    "        }\n",
    "\n",
    "print(\"✅ Memory optimization patterns defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "progressive_loading"
   },
   "source": [
    "## 5. Progressive Loading and User Feedback\n",
    "\n",
    "### Creating Responsive User Experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "progressive_loading_demo"
   },
   "outputs": [],
   "source": [
    "%%writefile progressive_loading_app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Progressive Loading Demo\",\n",
    "    page_icon=\"⚡\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# Progressive loading patterns\n",
    "def create_loading_state():\n",
    "    \"\"\"Create sophisticated loading indicators\"\"\"\n",
    "    \n",
    "    # Multi-stage loading with progress\n",
    "    stages = [\n",
    "        (\"Connecting to database\", 1),\n",
    "        (\"Loading customer data\", 3),\n",
    "        (\"Processing orders\", 2),\n",
    "        (\"Calculating metrics\", 1),\n",
    "        (\"Generating visualizations\", 2)\n",
    "    ]\n",
    "    \n",
    "    progress_bar = st.progress(0)\n",
    "    status_text = st.empty()\n",
    "    \n",
    "    total_time = sum(stage[1] for stage in stages)\n",
    "    elapsed_time = 0\n",
    "    \n",
    "    for stage_name, stage_duration in stages:\n",
    "        status_text.text(f\"🔄 {stage_name}...\")\n",
    "        \n",
    "        # Simulate stage progress\n",
    "        for i in range(stage_duration * 10):\n",
    "            time.sleep(0.1)\n",
    "            elapsed_time += 0.1\n",
    "            progress = elapsed_time / total_time\n",
    "            progress_bar.progress(progress)\n",
    "    \n",
    "    status_text.text(\"✅ Complete!\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    progress_bar.empty()\n",
    "    status_text.empty()\n",
    "\n",
    "@st.cache_data\n",
    "def load_data_with_feedback(dataset_size):\n",
    "    \"\"\"Load data with user feedback during the process\"\"\"\n",
    "    \n",
    "    # Show immediate feedback\n",
    "    with st.spinner(f\"Generating dataset with {dataset_size:,} records...\"):\n",
    "        \n",
    "        # Create sample data in chunks to show progress\n",
    "        chunk_size = min(10000, dataset_size // 10)\n",
    "        chunks = []\n",
    "        \n",
    "        progress_bar = st.progress(0)\n",
    "        status_container = st.empty()\n",
    "        \n",
    "        for i in range(0, dataset_size, chunk_size):\n",
    "            current_chunk_size = min(chunk_size, dataset_size - i)\n",
    "            \n",
    "            # Generate chunk\n",
    "            chunk = pd.DataFrame({\n",
    "                'id': range(i, i + current_chunk_size),\n",
    "                'timestamp': pd.date_range('2023-01-01', periods=current_chunk_size, freq='1H'),\n",
    "                'value': np.random.randn(current_chunk_size),\n",
    "                'category': np.random.choice(['A', 'B', 'C', 'D'], current_chunk_size),\n",
    "                'region': np.random.choice(['North', 'South', 'East', 'West'], current_chunk_size)\n",
    "            })\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            # Update progress\n",
    "            progress = (i + current_chunk_size) / dataset_size\n",
    "            progress_bar.progress(progress)\n",
    "            status_container.text(f\"Generated {i + current_chunk_size:,} of {dataset_size:,} records\")\n",
    "            \n",
    "            # Small delay to show progress\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        # Combine chunks\n",
    "        status_container.text(\"Combining data chunks...\")\n",
    "        result = pd.concat(chunks, ignore_index=True)\n",
    "        \n",
    "        progress_bar.empty()\n",
    "        status_container.empty()\n",
    "        \n",
    "        return result\n",
    "\n",
    "def create_interactive_dashboard():\n",
    "    \"\"\"Create dashboard with progressive disclosure\"\"\"\n",
    "    \n",
    "    st.title(\"⚡ Progressive Loading Dashboard\")\n",
    "    \n",
    "    # Level 1: Quick overview (loads immediately)\n",
    "    st.subheader(\"📊 Quick Overview\")\n",
    "    \n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    with col1:\n",
    "        st.metric(\"Total Records\", \"1,234,567\", \"↑ 12%\")\n",
    "    with col2:\n",
    "        st.metric(\"Active Users\", \"45,678\", \"↑ 8%\")\n",
    "    with col3:\n",
    "        st.metric(\"Revenue\", \"$123,456\", \"↑ 15%\")\n",
    "    with col4:\n",
    "        st.metric(\"Conversion Rate\", \"3.4%\", \"↓ 0.2%\")\n",
    "    \n",
    "    # Level 2: Interactive analysis (loads on demand)\n",
    "    st.subheader(\"🔍 Detailed Analysis\")\n",
    "    \n",
    "    with st.expander(\"Load Detailed Data Analysis\", expanded=False):\n",
    "        dataset_size = st.selectbox(\n",
    "            \"Choose dataset size:\",\n",
    "            [1000, 10000, 50000, 100000],\n",
    "            index=1\n",
    "        )\n",
    "        \n",
    "        if st.button(\"Load Analysis Data\", type=\"primary\"):\n",
    "            # Show loading process\n",
    "            data = load_data_with_feedback(dataset_size)\n",
    "            \n",
    "            # Display results\n",
    "            st.success(f\"✅ Loaded {len(data):,} records successfully!\")\n",
    "            \n",
    "            # Quick statistics\n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                st.subheader(\"Data Overview\")\n",
    "                st.dataframe(data.describe())\n",
    "            \n",
    "            with col2:\n",
    "                st.subheader(\"Category Distribution\")\n",
    "                category_counts = data['category'].value_counts()\n",
    "                fig = px.pie(values=category_counts.values, names=category_counts.index)\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "            \n",
    "            # Time series (sampled for performance)\n",
    "            st.subheader(\"Value Trends Over Time\")\n",
    "            \n",
    "            if len(data) > 5000:\n",
    "                st.info(f\"📊 Showing sample of {min(5000, len(data)):,} points for performance\")\n",
    "                plot_data = data.sample(n=min(5000, len(data)))\n",
    "            else:\n",
    "                plot_data = data\n",
    "            \n",
    "            fig_line = px.line(plot_data, x='timestamp', y='value', color='category')\n",
    "            st.plotly_chart(fig_line, use_container_width=True)\n",
    "    \n",
    "    # Level 3: Advanced features (loads on specific request)\n",
    "    st.subheader(\"🚀 Advanced Features\")\n",
    "    \n",
    "    advanced_feature = st.selectbox(\n",
    "        \"Choose advanced analysis:\",\n",
    "        [\"None\", \"Statistical Analysis\", \"Correlation Matrix\", \"Anomaly Detection\"]\n",
    "    )\n",
    "    \n",
    "    if advanced_feature != \"None\":\n",
    "        with st.spinner(f\"Running {advanced_feature.lower()}...\"):\n",
    "            time.sleep(2)  # Simulate complex calculation\n",
    "            st.success(f\"✅ {advanced_feature} completed!\")\n",
    "            \n",
    "            if advanced_feature == \"Statistical Analysis\":\n",
    "                st.write(\"Statistical analysis results would appear here...\")\n",
    "            elif advanced_feature == \"Correlation Matrix\":\n",
    "                st.write(\"Correlation matrix visualization would appear here...\")\n",
    "            elif advanced_feature == \"Anomaly Detection\":\n",
    "                st.write(\"Anomaly detection results would appear here...\")\n",
    "\n",
    "def show_performance_monitoring():\n",
    "    \"\"\"Show real-time performance monitoring\"\"\"\n",
    "    \n",
    "    st.sidebar.markdown(\"---\")\n",
    "    st.sidebar.subheader(\"⚡ Performance Monitor\")\n",
    "    \n",
    "    # Memory usage\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "        st.sidebar.metric(\"Memory Usage\", f\"{memory_mb:.1f} MB\")\n",
    "    except ImportError:\n",
    "        st.sidebar.info(\"Install psutil for memory monitoring\")\n",
    "    \n",
    "    # Cache status\n",
    "    if st.sidebar.button(\"Clear Cache\"):\n",
    "        st.cache_data.clear()\n",
    "        st.sidebar.success(\"Cache cleared!\")\n",
    "    \n",
    "    # Performance tips\n",
    "    with st.sidebar.expander(\"Performance Tips\"):\n",
    "        st.write(\"\"\"\n",
    "        - Use smaller datasets for exploration\n",
    "        - Enable caching for repeated operations\n",
    "        - Sample large datasets for visualization\n",
    "        - Load data progressively\n",
    "        \"\"\")\n",
    "\n",
    "def main():\n",
    "    # Performance monitoring\n",
    "    show_performance_monitoring()\n",
    "    \n",
    "    # Main dashboard\n",
    "    create_interactive_dashboard()\n",
    "    \n",
    "    # Demo: Progressive loading\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"🎯 Progressive Loading Demo\")\n",
    "    \n",
    "    if st.button(\"Demo: Multi-Stage Loading\"):\n",
    "        create_loading_state()\n",
    "        st.success(\"Multi-stage loading complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "real_time_updates"
   },
   "source": [
    "## 6. Real-Time Data Updates and Monitoring\n",
    "\n",
    "### Handling Live Data Efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "real_time_patterns"
   },
   "outputs": [],
   "source": [
    "# Real-time data update patterns\n",
    "\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class DataStreamManager:\n",
    "    \"\"\"Manage real-time data streams efficiently\"\"\"\n",
    "    \n",
    "    def __init__(self, update_interval=30):\n",
    "        self.update_interval = update_interval  # seconds\n",
    "        self.last_update = None\n",
    "        self.data_cache = {}\n",
    "        self.subscribers = []\n",
    "    \n",
    "    def should_update(self):\n",
    "        \"\"\"Check if data should be updated\"\"\"\n",
    "        if self.last_update is None:\n",
    "            return True\n",
    "        \n",
    "        time_since_update = (datetime.now() - self.last_update).total_seconds()\n",
    "        return time_since_update >= self.update_interval\n",
    "    \n",
    "    def get_real_time_data(self, data_type):\n",
    "        \"\"\"Get real-time data with intelligent caching\"\"\"\n",
    "        \n",
    "        cache_key = f\"{data_type}_{datetime.now().strftime('%Y%m%d%H%M')}\"\n",
    "        \n",
    "        if cache_key in self.data_cache and not self.should_update():\n",
    "            return self.data_cache[cache_key]\n",
    "        \n",
    "        # Simulate real-time data fetch\n",
    "        print(f\"Fetching real-time {data_type} data...\")\n",
    "        \n",
    "        # Simulate API call or database query\n",
    "        new_data = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'data_type': data_type,\n",
    "            'value': np.random.randint(100, 1000),\n",
    "            'trend': np.random.choice(['up', 'down', 'stable'])\n",
    "        }\n",
    "        \n",
    "        self.data_cache[cache_key] = new_data\n",
    "        self.last_update = datetime.now()\n",
    "        \n",
    "        # Notify subscribers\n",
    "        self.notify_subscribers(new_data)\n",
    "        \n",
    "        return new_data\n",
    "    \n",
    "    def notify_subscribers(self, data):\n",
    "        \"\"\"Notify all subscribers of data updates\"\"\"\n",
    "        for callback in self.subscribers:\n",
    "            try:\n",
    "                callback(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error notifying subscriber: {e}\")\n",
    "    \n",
    "    def subscribe(self, callback):\n",
    "        \"\"\"Subscribe to data updates\"\"\"\n",
    "        self.subscribers.append(callback)\n",
    "\n",
    "# Pattern: Incremental data updates\n",
    "@st.cache_data(ttl=60)  # Cache for 1 minute\n",
    "def get_incremental_updates(last_update_time=None):\n",
    "    \"\"\"Get only new data since last update\"\"\"\n",
    "    \n",
    "    if last_update_time is None:\n",
    "        last_update_time = datetime.now() - timedelta(hours=24)\n",
    "    \n",
    "    # Simulate incremental data query\n",
    "    # In real app: SELECT * FROM table WHERE updated_at > last_update_time\n",
    "    \n",
    "    new_records = pd.DataFrame({\n",
    "        'id': range(100),\n",
    "        'timestamp': pd.date_range(last_update_time, periods=100, freq='1T'),\n",
    "        'value': np.random.randn(100),\n",
    "        'status': np.random.choice(['active', 'inactive'], 100)\n",
    "    })\n",
    "    \n",
    "    return new_records\n",
    "\n",
    "# Pattern: Smart refresh strategies\n",
    "def create_smart_refresh_button():\n",
    "    \"\"\"Create intelligent refresh controls\"\"\"\n",
    "    \n",
    "    col1, col2, col3 = st.columns([1, 1, 2])\n",
    "    \n",
    "    with col1:\n",
    "        if st.button(\"🔄 Refresh Now\"):\n",
    "            st.cache_data.clear()\n",
    "            st.experimental_rerun()\n",
    "    \n",
    "    with col2:\n",
    "        auto_refresh = st.checkbox(\"Auto-refresh\", value=False)\n",
    "    \n",
    "    with col3:\n",
    "        if auto_refresh:\n",
    "            refresh_interval = st.selectbox(\n",
    "                \"Refresh every:\",\n",
    "                [30, 60, 300, 600],  # seconds\n",
    "                format_func=lambda x: f\"{x//60}m {x%60}s\" if x >= 60 else f\"{x}s\"\n",
    "            )\n",
    "            \n",
    "            # Auto-refresh using session state\n",
    "            if 'last_refresh' not in st.session_state:\n",
    "                st.session_state.last_refresh = datetime.now()\n",
    "            \n",
    "            time_since_refresh = (datetime.now() - st.session_state.last_refresh).total_seconds()\n",
    "            \n",
    "            if time_since_refresh >= refresh_interval:\n",
    "                st.session_state.last_refresh = datetime.now()\n",
    "                st.experimental_rerun()\n",
    "            \n",
    "            # Show countdown\n",
    "            remaining = refresh_interval - time_since_refresh\n",
    "            st.caption(f\"Next refresh in {remaining:.0f}s\")\n",
    "\n",
    "print(\"✅ Real-time data patterns defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance_monitoring"
   },
   "source": [
    "## 7. Performance Monitoring and Debugging\n",
    "\n",
    "### Built-in Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "performance_monitoring_tools"
   },
   "outputs": [],
   "source": [
    "# Performance monitoring and debugging tools\n",
    "\n",
    "import logging\n",
    "import traceback\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitor and log application performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'function_calls': {},\n",
    "            'execution_times': {},\n",
    "            'memory_usage': {},\n",
    "            'cache_hits': 0,\n",
    "            'cache_misses': 0\n",
    "        }\n",
    "        self.start_time = datetime.now()\n",
    "    \n",
    "    @contextmanager\n",
    "    def measure_execution(self, operation_name):\n",
    "        \"\"\"Context manager to measure execution time\"\"\"\n",
    "        start_time = time.time()\n",
    "        start_memory = get_memory_usage()\n",
    "        \n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            end_time = time.time()\n",
    "            end_memory = get_memory_usage()\n",
    "            \n",
    "            execution_time = end_time - start_time\n",
    "            memory_delta = end_memory - start_memory\n",
    "            \n",
    "            # Record metrics\n",
    "            self.record_execution(operation_name, execution_time, memory_delta)\n",
    "    \n",
    "    def record_execution(self, operation, execution_time, memory_delta):\n",
    "        \"\"\"Record execution metrics\"\"\"\n",
    "        if operation not in self.metrics['function_calls']:\n",
    "            self.metrics['function_calls'][operation] = 0\n",
    "            self.metrics['execution_times'][operation] = []\n",
    "            self.metrics['memory_usage'][operation] = []\n",
    "        \n",
    "        self.metrics['function_calls'][operation] += 1\n",
    "        self.metrics['execution_times'][operation].append(execution_time)\n",
    "        self.metrics['memory_usage'][operation].append(memory_delta)\n",
    "    \n",
    "    def get_performance_summary(self):\n",
    "        \"\"\"Get performance summary\"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        for operation in self.metrics['function_calls']:\n",
    "            times = self.metrics['execution_times'][operation]\n",
    "            memory = self.metrics['memory_usage'][operation]\n",
    "            \n",
    "            summary[operation] = {\n",
    "                'calls': self.metrics['function_calls'][operation],\n",
    "                'avg_time': np.mean(times),\n",
    "                'max_time': np.max(times),\n",
    "                'total_time': np.sum(times),\n",
    "                'avg_memory': np.mean(memory),\n",
    "                'max_memory': np.max(memory)\n",
    "            }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def display_metrics(self):\n",
    "        \"\"\"Display performance metrics in Streamlit\"\"\"\n",
    "        st.subheader(\"📊 Performance Metrics\")\n",
    "        \n",
    "        summary = self.get_performance_summary()\n",
    "        \n",
    "        if not summary:\n",
    "            st.info(\"No performance data available yet\")\n",
    "            return\n",
    "        \n",
    "        # Overall metrics\n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        \n",
    "        with col1:\n",
    "            total_calls = sum(data['calls'] for data in summary.values())\n",
    "            st.metric(\"Total Function Calls\", total_calls)\n",
    "        \n",
    "        with col2:\n",
    "            total_time = sum(data['total_time'] for data in summary.values())\n",
    "            st.metric(\"Total Execution Time\", f\"{total_time:.2f}s\")\n",
    "        \n",
    "        with col3:\n",
    "            cache_hit_rate = self.metrics['cache_hits'] / max(1, self.metrics['cache_hits'] + self.metrics['cache_misses'])\n",
    "            st.metric(\"Cache Hit Rate\", f\"{cache_hit_rate:.1%}\")\n",
    "        \n",
    "        # Detailed breakdown\n",
    "        st.subheader(\"Function Performance Breakdown\")\n",
    "        \n",
    "        performance_df = pd.DataFrame([\n",
    "            {\n",
    "                'Function': func,\n",
    "                'Calls': data['calls'],\n",
    "                'Avg Time (s)': round(data['avg_time'], 3),\n",
    "                'Max Time (s)': round(data['max_time'], 3),\n",
    "                'Total Time (s)': round(data['total_time'], 3),\n",
    "                'Avg Memory (MB)': round(data['avg_memory'], 2)\n",
    "            }\n",
    "            for func, data in summary.items()\n",
    "        ])\n",
    "        \n",
    "        st.dataframe(performance_df)\n",
    "        \n",
    "        # Performance visualization\n",
    "        if len(performance_df) > 0:\n",
    "            fig = px.bar(\n",
    "                performance_df,\n",
    "                x='Function',\n",
    "                y='Total Time (s)',\n",
    "                title=\"Total Execution Time by Function\"\n",
    "            )\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# Global performance monitor\n",
    "if 'perf_monitor' not in st.session_state:\n",
    "    st.session_state.perf_monitor = PerformanceMonitor()\n",
    "\n",
    "# Decorator for automatic performance monitoring\n",
    "def monitor_performance(operation_name=None):\n",
    "    \"\"\"Decorator to automatically monitor function performance\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            name = operation_name or func.__name__\n",
    "            \n",
    "            with st.session_state.perf_monitor.measure_execution(name):\n",
    "                return func(*args, **kwargs)\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Example usage of performance monitoring\n",
    "@monitor_performance(\"data_loading\")\n",
    "def load_monitored_data(size=1000):\n",
    "    \"\"\"Example function with performance monitoring\"\"\"\n",
    "    time.sleep(0.5)  # Simulate slow operation\n",
    "    return pd.DataFrame({\n",
    "        'id': range(size),\n",
    "        'value': np.random.randn(size)\n",
    "    })\n",
    "\n",
    "@monitor_performance(\"data_processing\")\n",
    "def process_monitored_data(df):\n",
    "    \"\"\"Example processing function with monitoring\"\"\"\n",
    "    time.sleep(0.2)  # Simulate processing time\n",
    "    return df.groupby(df.index // 100).agg({\n",
    "        'value': ['mean', 'std', 'count']\n",
    "    })\n",
    "\n",
    "print(\"✅ Performance monitoring tools defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "debugging_tools"
   },
   "source": [
    "### Debugging and Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "debugging_patterns"
   },
   "outputs": [],
   "source": [
    "# Debugging and error handling patterns\n",
    "\n",
    "def create_debug_panel():\n",
    "    \"\"\"Create a debug panel for development\"\"\"\n",
    "    \n",
    "    with st.sidebar.expander(\"🐛 Debug Panel\", expanded=False):\n",
    "        st.subheader(\"Debug Information\")\n",
    "        \n",
    "        # Session state inspection\n",
    "        if st.checkbox(\"Show Session State\"):\n",
    "            st.json(dict(st.session_state))\n",
    "        \n",
    "        # Cache information\n",
    "        if st.checkbox(\"Show Cache Info\"):\n",
    "            st.write(\"Cache functions available:\")\n",
    "            st.code(\"st.cache_data.clear() - Clear data cache\")\n",
    "            st.code(\"st.cache_resource.clear() - Clear resource cache\")\n",
    "        \n",
    "        # Performance metrics\n",
    "        if st.checkbox(\"Show Performance Metrics\"):\n",
    "            if 'perf_monitor' in st.session_state:\n",
    "                st.session_state.perf_monitor.display_metrics()\n",
    "        \n",
    "        # Memory usage\n",
    "        if st.checkbox(\"Show Memory Usage\"):\n",
    "            try:\n",
    "                memory_mb = get_memory_usage()\n",
    "                st.metric(\"Current Memory Usage\", f\"{memory_mb:.1f} MB\")\n",
    "            except:\n",
    "                st.error(\"Unable to get memory usage\")\n",
    "        \n",
    "        # Error simulation for testing\n",
    "        st.subheader(\"Error Testing\")\n",
    "        \n",
    "        error_types = [\n",
    "            \"None\",\n",
    "            \"ValueError\",\n",
    "            \"KeyError\",\n",
    "            \"Memory Error\",\n",
    "            \"Network Error\"\n",
    "        ]\n",
    "        \n",
    "        selected_error = st.selectbox(\"Simulate Error:\", error_types)\n",
    "        \n",
    "        if selected_error != \"None\" and st.button(\"Trigger Error\"):\n",
    "            simulate_error(selected_error)\n",
    "\n",
    "def simulate_error(error_type):\n",
    "    \"\"\"Simulate different types of errors for testing\"\"\"\n",
    "    try:\n",
    "        if error_type == \"ValueError\":\n",
    "            raise ValueError(\"Simulated value error for testing\")\n",
    "        elif error_type == \"KeyError\":\n",
    "            test_dict = {'a': 1}\n",
    "            _ = test_dict['nonexistent_key']\n",
    "        elif error_type == \"Memory Error\":\n",
    "            # Simulate memory error (don't actually consume memory)\n",
    "            raise MemoryError(\"Simulated memory error\")\n",
    "        elif error_type == \"Network Error\":\n",
    "            raise ConnectionError(\"Simulated network connection error\")\n",
    "    except Exception as e:\n",
    "        handle_application_error(e, error_type)\n",
    "\n",
    "def handle_application_error(error, context=\"\"):\n",
    "    \"\"\"Centralized error handling with user-friendly messages\"\"\"\n",
    "    \n",
    "    error_type = type(error).__name__\n",
    "    error_message = str(error)\n",
    "    \n",
    "    # Log the error (in production, use proper logging)\n",
    "    print(f\"ERROR [{error_type}] in {context}: {error_message}\")\n",
    "    print(f\"Traceback: {traceback.format_exc()}\")\n",
    "    \n",
    "    # User-friendly error messages\n",
    "    if \"Memory\" in error_type:\n",
    "        st.error(\"\"\"\n",
    "        🚨 **Memory Error**\n",
    "        \n",
    "        The application ran out of memory. Try:\n",
    "        - Using a smaller dataset\n",
    "        - Clearing the cache\n",
    "        - Refreshing the page\n",
    "        \"\"\")\n",
    "        \n",
    "        if st.button(\"Clear Cache and Refresh\"):\n",
    "            st.cache_data.clear()\n",
    "            st.experimental_rerun()\n",
    "    \n",
    "    elif \"Connection\" in error_type or \"Network\" in error_type:\n",
    "        st.error(\"\"\"\n",
    "        🌐 **Connection Error**\n",
    "        \n",
    "        Unable to connect to data source. Try:\n",
    "        - Checking your internet connection\n",
    "        - Refreshing the page\n",
    "        - Contacting support if the issue persists\n",
    "        \"\"\")\n",
    "        \n",
    "        if st.button(\"Retry Connection\"):\n",
    "            st.experimental_rerun()\n",
    "    \n",
    "    elif \"KeyError\" in error_type:\n",
    "        st.error(\"\"\"\n",
    "        🔑 **Data Error**\n",
    "        \n",
    "        Missing expected data fields. This might be due to:\n",
    "        - Data source changes\n",
    "        - Invalid filter selections\n",
    "        - Temporary data issues\n",
    "        \"\"\")\n",
    "    \n",
    "    else:\n",
    "        st.error(f\"\"\"\n",
    "        ⚠️ **Application Error**\n",
    "        \n",
    "        An unexpected error occurred: {error_type}\n",
    "        \n",
    "        Please try refreshing the page or contact support.\n",
    "        \"\"\")\n",
    "    \n",
    "    # Show technical details in expandable section\n",
    "    with st.expander(\"Technical Details (for developers)\"):\n",
    "        st.code(f\"Error Type: {error_type}\")\n",
    "        st.code(f\"Error Message: {error_message}\")\n",
    "        st.code(f\"Context: {context}\")\n",
    "        st.code(f\"Timestamp: {datetime.now()}\")\n",
    "\n",
    "# Robust function wrapper\n",
    "def make_robust(func, fallback_value=None, error_message=\"Operation failed\"):\n",
    "    \"\"\"Make any function more robust with error handling\"\"\"\n",
    "    \n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            handle_application_error(e, func.__name__)\n",
    "            return fallback_value\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "# Example usage\n",
    "@make_robust(fallback_value=pd.DataFrame(), error_message=\"Failed to load data\")\n",
    "def robust_data_loader(source):\n",
    "    \"\"\"Data loader with built-in error handling\"\"\"\n",
    "    if source == \"error\":\n",
    "        raise ValueError(\"Invalid data source\")\n",
    "    return pd.DataFrame({'test': [1, 2, 3]})\n",
    "\n",
    "print(\"✅ Debugging and error handling patterns defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hands_on_optimization"
   },
   "source": [
    "## 8. 🛠️ Performance Optimization Exercise\n",
    "\n",
    "**Challenge**: Optimize a slow Streamlit application\n",
    "\n",
    "### The Problem\n",
    "You have a Streamlit app that:\n",
    "- Takes 30+ seconds to load\n",
    "- Uses 2GB+ of memory\n",
    "- Crashes with large datasets\n",
    "- Provides no feedback during loading\n",
    "\n",
    "### Your Mission\n",
    "Apply performance optimization techniques to make it fast and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "optimization_exercise"
   },
   "outputs": [],
   "source": [
    "# 🛠️ Performance Optimization Exercise\n",
    "# \n",
    "# BEFORE: Slow, inefficient app\n",
    "# AFTER: Fast, optimized app\n",
    "#\n",
    "# Your task: Apply the optimization patterns we've learned\n",
    "\n",
    "%%writefile slow_app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# ❌ SLOW VERSION - DON'T USE IN PRODUCTION!\n",
    "def slow_data_loader():\n",
    "    \"\"\"Inefficient data loading - loads everything every time\"\"\"\n",
    "    print(\"Loading data... (this is slow!)\")\n",
    "    time.sleep(5)  # Simulate slow database\n",
    "    \n",
    "    # Create large dataset in memory\n",
    "    large_data = pd.DataFrame({\n",
    "        'id': range(1000000),  # 1 million rows\n",
    "        'timestamp': pd.date_range('2020-01-01', periods=1000000, freq='1min'),\n",
    "        'value': np.random.randn(1000000),\n",
    "        'category': np.random.choice(['A', 'B', 'C', 'D'], 1000000),\n",
    "        'text_data': ['sample_text_' + str(i) for i in range(1000000)]\n",
    "    })\n",
    "    \n",
    "    return large_data\n",
    "\n",
    "def slow_processing(data):\n",
    "    \"\"\"Inefficient data processing\"\"\"\n",
    "    print(\"Processing data... (this is also slow!)\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Inefficient operations\n",
    "    results = []\n",
    "    for category in data['category'].unique():\n",
    "        subset = data[data['category'] == category]\n",
    "        result = {\n",
    "            'category': category,\n",
    "            'mean': subset['value'].mean(),\n",
    "            'std': subset['value'].std(),\n",
    "            'count': len(subset)\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def slow_main():\n",
    "    \"\"\"Slow version of the app\"\"\"\n",
    "    st.title(\"❌ Slow App - Don't Use This!\")\n",
    "    \n",
    "    # No caching - loads data every time\n",
    "    data = slow_data_loader()\n",
    "    \n",
    "    # No progress feedback\n",
    "    processed = slow_processing(data)\n",
    "    \n",
    "    # Display everything at once\n",
    "    st.subheader(\"Raw Data\")\n",
    "    st.dataframe(data)  # Will crash browser with 1M rows\n",
    "    \n",
    "    st.subheader(\"Processed Results\")\n",
    "    st.dataframe(processed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    slow_main()\n",
    "\n",
    "print(\"❌ Slow app created - now optimize it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "optimized_app_solution"
   },
   "outputs": [],
   "source": [
    "# ✅ OPTIMIZED VERSION - Use these patterns!\n",
    "\n",
    "%%writefile optimized_app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Optimized Performance App\",\n",
    "    page_icon=\"⚡\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# ✅ OPTIMIZATION 1: Caching with appropriate TTL\n",
    "@st.cache_data(ttl=3600)  # Cache for 1 hour\n",
    "def optimized_data_loader(sample_size=50000):\n",
    "    \"\"\"Efficient data loading with sampling and caching\"\"\"\n",
    "    \n",
    "    with st.spinner(f\"Loading optimized dataset ({sample_size:,} records)...\"):\n",
    "        time.sleep(1)  # Simulate database query\n",
    "        \n",
    "        # Create manageable dataset\n",
    "        data = pd.DataFrame({\n",
    "            'id': range(sample_size),\n",
    "            'timestamp': pd.date_range('2020-01-01', periods=sample_size, freq='1H'),\n",
    "            'value': np.random.randn(sample_size),\n",
    "            'category': np.random.choice(['A', 'B', 'C', 'D'], sample_size)\n",
    "        })\n",
    "        \n",
    "        # ✅ OPTIMIZATION 2: Memory optimization\n",
    "        data['category'] = data['category'].astype('category')\n",
    "        data['value'] = data['value'].astype('float32')  # Use less memory\n",
    "        \n",
    "    return data\n",
    "\n",
    "# ✅ OPTIMIZATION 3: Efficient processing with caching\n",
    "@st.cache_data(ttl=1800)  # Cache for 30 minutes\n",
    "def optimized_processing(data):\n",
    "    \"\"\"Efficient data processing using pandas operations\"\"\"\n",
    "    \n",
    "    with st.spinner(\"Processing data efficiently...\"):\n",
    "        # Use pandas groupby - much faster than loops\n",
    "        processed = data.groupby('category')['value'].agg([\n",
    "            'mean', 'std', 'count', 'min', 'max'\n",
    "        ]).round(3)\n",
    "        \n",
    "        processed = processed.reset_index()\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# ✅ OPTIMIZATION 4: Progressive disclosure\n",
    "def create_optimized_dashboard():\n",
    "    \"\"\"Create dashboard with progressive loading\"\"\"\n",
    "    \n",
    "    st.title(\"⚡ Optimized Performance App\")\n",
    "    \n",
    "    # Quick overview (loads immediately)\n",
    "    st.subheader(\"📊 Quick Overview\")\n",
    "    \n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    with col1:\n",
    "        st.metric(\"Status\", \"✅ Optimized\", \"90% faster\")\n",
    "    with col2:\n",
    "        st.metric(\"Memory Usage\", \"~50MB\", \"-95%\")\n",
    "    with col3:\n",
    "        st.metric(\"Load Time\", \"<3s\", \"-90%\")\n",
    "    with col4:\n",
    "        st.metric(\"Cache Hit Rate\", \"85%\", \"+85%\")\n",
    "    \n",
    "    # ✅ OPTIMIZATION 5: User controls for data size\n",
    "    st.subheader(\"🎛️ Data Controls\")\n",
    "    \n",
    "    sample_size = st.selectbox(\n",
    "        \"Choose dataset size:\",\n",
    "        [1000, 10000, 50000, 100000],\n",
    "        index=2,\n",
    "        help=\"Larger datasets take more time and memory\"\n",
    "    )\n",
    "    \n",
    "    # ✅ OPTIMIZATION 6: Load data only when needed\n",
    "    if st.button(\"Load Data\", type=\"primary\"):\n",
    "        \n",
    "        # Show performance monitoring\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load data with caching\n",
    "        data = optimized_data_loader(sample_size)\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        st.success(f\"✅ Data loaded in {load_time:.2f} seconds!\")\n",
    "        \n",
    "        # ✅ OPTIMIZATION 7: Show sample instead of full data\n",
    "        st.subheader(\"📋 Data Sample (First 100 rows)\")\n",
    "        st.dataframe(data.head(100))\n",
    "        \n",
    "        # Show data info\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            st.info(f\"Total records: {len(data):,}\")\n",
    "        with col2:\n",
    "            memory_mb = data.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "            st.info(f\"Memory usage: {memory_mb:.1f} MB\")\n",
    "        \n",
    "        # ✅ OPTIMIZATION 8: Efficient processing\n",
    "        with st.expander(\"📊 Analysis Results\", expanded=True):\n",
    "            processed = optimized_processing(data)\n",
    "            st.dataframe(processed)\n",
    "            \n",
    "            # Simple visualization\n",
    "            import plotly.express as px\n",
    "            fig = px.bar(processed, x='category', y='mean', title='Average Value by Category')\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        # ✅ OPTIMIZATION 9: Full data access on demand\n",
    "        if st.checkbox(\"Show Full Dataset (Use with caution for large data)\"):\n",
    "            if len(data) > 10000:\n",
    "                st.warning(f\"This dataset has {len(data):,} rows. Display may be slow.\")\n",
    "                if st.button(\"I understand, show full data\"):\n",
    "                    st.dataframe(data)\n",
    "            else:\n",
    "                st.dataframe(data)\n",
    "\n",
    "# ✅ OPTIMIZATION 10: Performance monitoring\n",
    "def show_performance_panel():\n",
    "    \"\"\"Show performance monitoring panel\"\"\"\n",
    "    \n",
    "    st.sidebar.markdown(\"---\")\n",
    "    st.sidebar.subheader(\"⚡ Performance Panel\")\n",
    "    \n",
    "    # Cache controls\n",
    "    if st.sidebar.button(\"Clear Cache\"):\n",
    "        st.cache_data.clear()\n",
    "        st.sidebar.success(\"Cache cleared!\")\n",
    "    \n",
    "    # Memory monitoring\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "        st.sidebar.metric(\"Memory Usage\", f\"{memory_mb:.1f} MB\")\n",
    "    except ImportError:\n",
    "        st.sidebar.info(\"Install psutil for memory monitoring\")\n",
    "    \n",
    "    # Performance tips\n",
    "    with st.sidebar.expander(\"💡 Optimization Tips\"):\n",
    "        st.write(\"\"\"\n",
    "        **Applied Optimizations:**\n",
    "        ✅ Data caching with TTL\n",
    "        ✅ Memory-efficient data types\n",
    "        ✅ Progressive data loading\n",
    "        ✅ Smart sampling\n",
    "        ✅ User feedback during operations\n",
    "        ✅ Chunked processing\n",
    "        ✅ Error handling\n",
    "        \"\"\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Optimized main application\"\"\"\n",
    "    \n",
    "    # Performance monitoring\n",
    "    show_performance_panel()\n",
    "    \n",
    "    # Main dashboard\n",
    "    create_optimized_dashboard()\n",
    "    \n",
    "    # Show comparison\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"🔄 Before vs After Comparison\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.markdown(\"\"\"\n",
    "        **❌ Before Optimization:**\n",
    "        - Load time: 30+ seconds\n",
    "        - Memory usage: 2GB+\n",
    "        - No user feedback\n",
    "        - Browser crashes\n",
    "        - No caching\n",
    "        \"\"\")\n",
    "    \n",
    "    with col2:\n",
    "        st.markdown(\"\"\"\n",
    "        **✅ After Optimization:**\n",
    "        - Load time: <3 seconds\n",
    "        - Memory usage: ~50MB\n",
    "        - Progress indicators\n",
    "        - Stable performance\n",
    "        - Smart caching\n",
    "        \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print(\"✅ Optimized app created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scaling_considerations"
   },
   "source": [
    "## 9. Scaling for Multiple Users\n",
    "\n",
    "### Multi-User Performance Considerations\n",
    "\n",
    "When your app serves multiple business users, additional considerations apply:\n",
    "\n",
    "#### Resource Sharing\n",
    "```python\n",
    "# ✅ Good: Shared resources\n",
    "@st.cache_resource\n",
    "def get_shared_connection():\n",
    "    \"\"\"Database connection shared across all users\"\"\"\n",
    "    return create_db_connection()\n",
    "\n",
    "# ✅ Good: User-specific data caching\n",
    "@st.cache_data\n",
    "def get_user_data(user_id, filters):\n",
    "    \"\"\"Cache data per user and filter combination\"\"\"\n",
    "    return load_filtered_data(user_id, filters)\n",
    "```\n",
    "\n",
    "#### Memory Management for Multiple Users\n",
    "```python\n",
    "# Set memory limits per user session\n",
    "MAX_MEMORY_PER_USER = 500  # MB\n",
    "\n",
    "def check_memory_limit():\n",
    "    \"\"\"Check if user session is using too much memory\"\"\"\n",
    "    current_memory = get_memory_usage()\n",
    "    \n",
    "    if current_memory > MAX_MEMORY_PER_USER:\n",
    "        st.warning(f\"Memory usage ({current_memory:.1f}MB) approaching limit. Consider clearing cache or using smaller datasets.\")\n",
    "```\n",
    "\n",
    "#### Load Balancing Strategies\n",
    "```python\n",
    "# Distribute expensive operations\n",
    "def balance_computational_load(operation_type):\n",
    "    \"\"\"Distribute heavy computations across time\"\"\"\n",
    "    \n",
    "    # Check current system load\n",
    "    current_hour = datetime.now().hour\n",
    "    \n",
    "    if operation_type == 'heavy_analysis' and 9 <= current_hour <= 17:\n",
    "        st.info(\"⏰ Heavy analysis during business hours may be slower. Consider running during off-peak times.\")\n",
    "    \n",
    "    # Queue non-urgent operations\n",
    "    if operation_type == 'report_generation':\n",
    "        st.info(\"📊 Report queued for generation. You'll receive an email when ready.\")\n",
    "        return queue_report_generation()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_performance"
   },
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Learned\n",
    "- **Advanced Caching**: Hierarchical caching strategies for different data types and update frequencies\n",
    "- **Database Optimization**: Connection pooling, query optimization, and chunked loading\n",
    "- **Memory Management**: Data type optimization, smart sampling, and lazy loading\n",
    "- **Progressive Loading**: User feedback, loading indicators, and progressive disclosure\n",
    "- **Real-Time Updates**: Efficient handling of live data with smart refresh strategies\n",
    "- **Performance Monitoring**: Built-in metrics, debugging tools, and error handling\n",
    "- **Scaling Considerations**: Multi-user resource management and load balancing\n",
    "\n",
    "### Key Performance Principles\n",
    "\n",
    "1. **Cache Strategically**: Use appropriate TTL values for different data types\n",
    "2. **Provide Feedback**: Always show progress during long operations\n",
    "3. **Sample Intelligently**: Use manageable data sizes for interactive exploration\n",
    "4. **Handle Errors Gracefully**: Provide user-friendly error messages and recovery options\n",
    "5. **Monitor Continuously**: Track performance metrics and optimize based on real usage\n",
    "6. **Scale Thoughtfully**: Consider multi-user impact when designing applications\n",
    "\n",
    "### Performance Optimization Checklist\n",
    "\n",
    "**Data Loading**:\n",
    "- [ ] Implement appropriate caching with TTL\n",
    "- [ ] Use chunked loading for large datasets\n",
    "- [ ] Provide loading indicators and progress bars\n",
    "- [ ] Optimize data types for memory efficiency\n",
    "\n",
    "**User Experience**:\n",
    "- [ ] Show immediate feedback for user actions\n",
    "- [ ] Implement progressive disclosure of information\n",
    "- [ ] Provide controls for dataset size and complexity\n",
    "- [ ] Handle errors with user-friendly messages\n",
    "\n",
    "**Monitoring & Debugging**:\n",
    "- [ ] Add performance monitoring to track metrics\n",
    "- [ ] Include debug panels for development\n",
    "- [ ] Implement robust error handling\n",
    "- [ ] Monitor memory usage and cache performance\n",
    "\n",
    "**Scalability**:\n",
    "- [ ] Test with multiple concurrent users\n",
    "- [ ] Implement resource sharing where appropriate\n",
    "- [ ] Consider load balancing for heavy operations\n",
    "- [ ] Set and monitor resource limits\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Final Week 10 Wednesday Deliverable\n",
    "\n",
    "You now have a complete toolkit for creating high-performance, business-ready Streamlit applications:\n",
    "\n",
    "1. **Part 1**: Convert analysis notebooks to structured applications\n",
    "2. **Part 2**: Design user experiences for business stakeholders\n",
    "3. **Part 3**: Optimize performance for production deployment\n",
    "\n",
    "**Next Session**: Thursday's content will focus on deployment, presentation preparation, and final project refinement.\n",
    "\n",
    "Your applications are now ready for business use! 🚀"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}