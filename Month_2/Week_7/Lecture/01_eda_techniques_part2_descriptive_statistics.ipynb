{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "descriptive_stats_title"
      },
      "source": [
        "# Week 7: EDA Techniques - Part 2: Descriptive Statistics and Summary Insights\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this session, you will be able to:\n",
        "- Apply comprehensive descriptive statistics to business datasets\n",
        "- Generate meaningful summary insights for stakeholders\n",
        "- Identify outliers and anomalies in data\n",
        "- Create automated reporting functions for EDA\n",
        "\n",
        "## Business Context\n",
        "Building on our structured EDA framework, we now focus on **extracting quantitative insights** from our Olist e-commerce data. This session emphasizes translating statistical measures into actionable business intelligence.\n",
        "\n",
        "**Key Business Questions:**\n",
        "- What are the typical order values and customer behaviors?\n",
        "- How do our product categories perform financially?\n",
        "- What outliers or anomalies need attention?\n",
        "- How can we summarize complex data for executive reporting?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## 1. Environment Setup and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_imports"
      },
      "outputs": [],
      "source": [
        "# Standard imports for data analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Database connection\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "# Enhanced plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"‚úÖ Environment setup complete for descriptive statistics analysis!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "database_connection"
      },
      "outputs": [],
      "source": [
        "# Supabase connection\n",
        "DATABASE_URL = \"postgresql://postgres.pzykoxdiwsyclwfqfiii:L3tMeQuery123!@aws-0-us-east-1.pooler.supabase.com:6543/postgres\"\n",
        "engine = create_engine(DATABASE_URL)\n",
        "\n",
        "print(\"üîÑ Loading comprehensive datasets for analysis...\")\n",
        "\n",
        "# Load main datasets with business focus\n",
        "orders_query = \"\"\"\n",
        "SELECT \n",
        "    o.*,\n",
        "    EXTRACT(YEAR FROM order_purchase_timestamp) as order_year,\n",
        "    EXTRACT(MONTH FROM order_purchase_timestamp) as order_month,\n",
        "    EXTRACT(DOW FROM order_purchase_timestamp) as order_dow\n",
        "FROM olist_sales_data_set.olist_orders_dataset o\n",
        "WHERE order_status = 'delivered'\n",
        "LIMIT 10000\n",
        "\"\"\"\n",
        "\n",
        "order_items_query = \"\"\"\n",
        "SELECT \n",
        "    oi.*,\n",
        "    (oi.price + oi.freight_value) as total_item_value\n",
        "FROM olist_sales_data_set.olist_order_items_dataset oi\n",
        "LIMIT 15000\n",
        "\"\"\"\n",
        "\n",
        "# Load product data with category translations\n",
        "products_query = \"\"\"\n",
        "SELECT \n",
        "    p.*,\n",
        "    COALESCE(t.product_category_name_english, p.product_category_name) as category_english\n",
        "FROM olist_sales_data_set.olist_products_dataset p\n",
        "LEFT JOIN olist_sales_data_set.product_category_name_translation t\n",
        "    ON p.product_category_name = t.product_category_name\n",
        "\"\"\"\n",
        "\n",
        "# Execute queries\n",
        "orders_df = pd.read_sql(orders_query, engine)\n",
        "order_items_df = pd.read_sql(order_items_query, engine)\n",
        "products_df = pd.read_sql(products_query, engine)\n",
        "\n",
        "print(f\"‚úÖ Data loaded:\")\n",
        "print(f\"   üì¶ Orders: {len(orders_df):,} delivered orders\")\n",
        "print(f\"   üõí Order Items: {len(order_items_df):,} line items\")\n",
        "print(f\"   üìã Products: {len(products_df):,} unique products\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "business_dataset_creation"
      },
      "source": [
        "## 2. Business Dataset Creation\n",
        "\n",
        "Let's create a comprehensive business dataset by joining our tables for deeper analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_business_dataset"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive business dataset\n",
        "print(\"üîß Creating comprehensive business dataset...\")\n",
        "\n",
        "# Merge order items with product information\n",
        "business_data = order_items_df.merge(\n",
        "    products_df[['product_id', 'category_english', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']], \n",
        "    on='product_id', \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Merge with orders information\n",
        "business_data = business_data.merge(\n",
        "    orders_df[['order_id', 'order_year', 'order_month', 'order_dow', 'order_purchase_timestamp']], \n",
        "    on='order_id', \n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Calculate additional business metrics\n",
        "business_data['profit_margin'] = (business_data['price'] - business_data['freight_value']) / business_data['price']\n",
        "business_data['freight_ratio'] = business_data['freight_value'] / business_data['price']\n",
        "business_data['product_volume'] = (\n",
        "    business_data['product_length_cm'] * \n",
        "    business_data['product_height_cm'] * \n",
        "    business_data['product_width_cm']\n",
        ") / 1000  # Convert to liters\n",
        "\n",
        "# Clean category names\n",
        "business_data['category_clean'] = business_data['category_english'].fillna('Unknown').str.title()\n",
        "\n",
        "print(f\"‚úÖ Business dataset created with {len(business_data):,} records\")\n",
        "print(f\"   üìä Columns: {business_data.shape[1]}\")\n",
        "print(f\"   üè∑Ô∏è Product categories: {business_data['category_clean'].nunique()}\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nüìã Sample Business Data:\")\n",
        "display(business_data[['order_id', 'product_id', 'category_clean', 'price', 'freight_value', 'total_item_value', 'profit_margin']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "descriptive_stats_section"
      },
      "source": [
        "## 3. Comprehensive Descriptive Statistics\n",
        "\n",
        "Now let's dive deep into descriptive statistics to understand our business metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "financial_descriptive_stats"
      },
      "outputs": [],
      "source": [
        "# Financial Metrics Analysis\n",
        "print(\"üí∞ Financial Metrics - Descriptive Statistics\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Key financial columns\n",
        "financial_cols = ['price', 'freight_value', 'total_item_value', 'profit_margin', 'freight_ratio']\n",
        "\n",
        "def enhanced_describe(df, columns, title):\n",
        "    \"\"\"\n",
        "    Enhanced descriptive statistics function\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìä {title}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    stats_df = pd.DataFrame({\n",
        "        'Mean': df[columns].mean(),\n",
        "        'Median': df[columns].median(),\n",
        "        'Std Dev': df[columns].std(),\n",
        "        'Min': df[columns].min(),\n",
        "        'Max': df[columns].max(),\n",
        "        'Q1': df[columns].quantile(0.25),\n",
        "        'Q3': df[columns].quantile(0.75),\n",
        "        'IQR': df[columns].quantile(0.75) - df[columns].quantile(0.25),\n",
        "        'Skewness': df[columns].skew(),\n",
        "        'Kurtosis': df[columns].kurtosis()\n",
        "    })\n",
        "    \n",
        "    return stats_df\n",
        "\n",
        "# Financial statistics\n",
        "financial_stats = enhanced_describe(business_data, financial_cols, \"Financial Metrics Summary\")\n",
        "display(financial_stats.round(4))\n",
        "\n",
        "# Business insights from financial data\n",
        "avg_order_value = business_data['total_item_value'].mean()\n",
        "median_order_value = business_data['total_item_value'].median()\n",
        "avg_freight_ratio = business_data['freight_ratio'].mean() * 100\n",
        "avg_profit_margin = business_data['profit_margin'].mean() * 100\n",
        "\n",
        "print(f\"\\nüí° Key Financial Insights:\")\n",
        "print(f\"   ‚Ä¢ Average order value: R$ {avg_order_value:.2f}\")\n",
        "print(f\"   ‚Ä¢ Median order value: R$ {median_order_value:.2f}\")\n",
        "print(f\"   ‚Ä¢ Average freight ratio: {avg_freight_ratio:.1f}% of item price\")\n",
        "print(f\"   ‚Ä¢ Average profit margin: {avg_profit_margin:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "category_analysis"
      },
      "outputs": [],
      "source": [
        "# Product Category Performance Analysis\n",
        "print(\"üè∑Ô∏è Product Category Performance Analysis\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# Category-level statistics\n",
        "category_stats = business_data.groupby('category_clean').agg({\n",
        "    'price': ['count', 'mean', 'median', 'std'],\n",
        "    'total_item_value': ['mean', 'sum'],\n",
        "    'freight_value': 'mean',\n",
        "    'profit_margin': 'mean',\n",
        "    'product_weight_g': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "# Flatten column names\n",
        "category_stats.columns = [f'{col[0]}_{col[1]}' if col[1] else col[0] for col in category_stats.columns]\n",
        "\n",
        "# Add total revenue per category\n",
        "category_stats = category_stats.sort_values('total_item_value_sum', ascending=False)\n",
        "\n",
        "print(\"\\nüèÜ Top 10 Categories by Total Revenue:\")\n",
        "top_categories = category_stats.head(10)\n",
        "display(top_categories[['price_count', 'price_mean', 'total_item_value_sum', 'profit_margin_mean']])\n",
        "\n",
        "# Category insights\n",
        "most_popular_category = category_stats['price_count'].idxmax()\n",
        "highest_revenue_category = category_stats['total_item_value_sum'].idxmax()\n",
        "highest_margin_category = category_stats['profit_margin_mean'].idxmax()\n",
        "highest_avg_price_category = category_stats['price_mean'].idxmax()\n",
        "\n",
        "print(f\"\\nüìà Category Performance Insights:\")\n",
        "print(f\"   ‚Ä¢ Most popular category: {most_popular_category} ({category_stats.loc[most_popular_category, 'price_count']:,} orders)\")\n",
        "print(f\"   ‚Ä¢ Highest revenue category: {highest_revenue_category} (R$ {category_stats.loc[highest_revenue_category, 'total_item_value_sum']:,.2f})\")\n",
        "print(f\"   ‚Ä¢ Best profit margin: {highest_margin_category} ({category_stats.loc[highest_margin_category, 'profit_margin_mean']*100:.1f}%)\")\n",
        "print(f\"   ‚Ä¢ Highest average price: {highest_avg_price_category} (R$ {category_stats.loc[highest_avg_price_category, 'price_mean']:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "distribution_visualization"
      },
      "source": [
        "## 4. Distribution Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "price_distribution_analysis"
      },
      "outputs": [],
      "source": [
        "# Price Distribution Analysis\n",
        "print(\"üí∏ Price Distribution Analysis\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Create comprehensive price distribution plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Price Distribution Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Histogram with KDE\n",
        "axes[0, 0].hist(business_data['price'], bins=50, alpha=0.7, color='skyblue', density=True)\n",
        "business_data['price'].plot(kind='kde', ax=axes[0, 0], color='red', linewidth=2)\n",
        "axes[0, 0].set_title('Price Distribution (Histogram + KDE)')\n",
        "axes[0, 0].set_xlabel('Price (R$)')\n",
        "axes[0, 0].set_ylabel('Density')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Box plot\n",
        "axes[0, 1].boxplot(business_data['price'], vert=True)\n",
        "axes[0, 1].set_title('Price Distribution (Box Plot)')\n",
        "axes[0, 1].set_ylabel('Price (R$)')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Log-scale histogram\n",
        "log_prices = np.log1p(business_data['price'])\n",
        "axes[1, 0].hist(log_prices, bins=50, alpha=0.7, color='lightgreen')\n",
        "axes[1, 0].set_title('Log-Transformed Price Distribution')\n",
        "axes[1, 0].set_xlabel('Log(Price + 1)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Q-Q plot for normality assessment\n",
        "stats.probplot(business_data['price'], dist=\"norm\", plot=axes[1, 1])\n",
        "axes[1, 1].set_title('Q-Q Plot (Price vs Normal Distribution)')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical tests for normality\n",
        "from scipy.stats import jarque_bera, shapiro\n",
        "\n",
        "# Sample for Shapiro-Wilk test (max 5000 samples)\n",
        "price_sample = business_data['price'].sample(min(5000, len(business_data)))\n",
        "\n",
        "jb_stat, jb_pvalue = jarque_bera(business_data['price'])\n",
        "sw_stat, sw_pvalue = shapiro(price_sample)\n",
        "\n",
        "print(f\"\\nüìä Normality Tests for Price Distribution:\")\n",
        "print(f\"   ‚Ä¢ Jarque-Bera Test: statistic = {jb_stat:.2f}, p-value = {jb_pvalue:.2e}\")\n",
        "print(f\"   ‚Ä¢ Shapiro-Wilk Test: statistic = {sw_stat:.4f}, p-value = {sw_pvalue:.2e}\")\n",
        "print(f\"   ‚Ä¢ Interpretation: {'Not normally distributed' if jb_pvalue < 0.05 else 'Potentially normally distributed'} (Œ± = 0.05)\")\n",
        "\n",
        "# Price percentiles\n",
        "price_percentiles = business_data['price'].quantile([0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n",
        "print(f\"\\nüí∞ Price Percentiles:\")\n",
        "for percentile, value in price_percentiles.items():\n",
        "    print(f\"   ‚Ä¢ {percentile*100:2.0f}th percentile: R$ {value:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "outlier_analysis"
      },
      "outputs": [],
      "source": [
        "# Outlier Detection and Analysis\n",
        "print(\"üîç Outlier Detection and Analysis\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "def detect_outliers(data, method='iqr'):\n",
        "    \"\"\"\n",
        "    Detect outliers using different methods\n",
        "    \"\"\"\n",
        "    if method == 'iqr':\n",
        "        Q1 = data.quantile(0.25)\n",
        "        Q3 = data.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        return data[(data < lower_bound) | (data > upper_bound)]\n",
        "    \n",
        "    elif method == 'zscore':\n",
        "        z_scores = np.abs(stats.zscore(data))\n",
        "        return data[z_scores > 3]\n",
        "    \n",
        "    elif method == 'modified_zscore':\n",
        "        median = data.median()\n",
        "        mad = np.median(np.abs(data - median))\n",
        "        modified_z_scores = 0.6745 * (data - median) / mad\n",
        "        return data[np.abs(modified_z_scores) > 3.5]\n",
        "\n",
        "# Detect outliers in price data\n",
        "price_outliers_iqr = detect_outliers(business_data['price'], 'iqr')\n",
        "price_outliers_zscore = detect_outliers(business_data['price'], 'zscore')\n",
        "price_outliers_modified = detect_outliers(business_data['price'], 'modified_zscore')\n",
        "\n",
        "print(f\"\\nüéØ Outlier Detection Results for Price:\")\n",
        "print(f\"   ‚Ä¢ IQR Method: {len(price_outliers_iqr):,} outliers ({len(price_outliers_iqr)/len(business_data)*100:.2f}%)\")\n",
        "print(f\"   ‚Ä¢ Z-Score Method: {len(price_outliers_zscore):,} outliers ({len(price_outliers_zscore)/len(business_data)*100:.2f}%)\")\n",
        "print(f\"   ‚Ä¢ Modified Z-Score: {len(price_outliers_modified):,} outliers ({len(price_outliers_modified)/len(business_data)*100:.2f}%)\")\n",
        "\n",
        "# Analyze outlier characteristics\n",
        "if len(price_outliers_iqr) > 0:\n",
        "    outlier_indices = business_data[business_data['price'].isin(price_outliers_iqr)].index\n",
        "    outlier_categories = business_data.loc[outlier_indices, 'category_clean'].value_counts().head(5)\n",
        "    \n",
        "    print(f\"\\nüìä Top 5 Categories with High-Price Outliers:\")\n",
        "    for category, count in outlier_categories.items():\n",
        "        print(f\"   ‚Ä¢ {category}: {count} outliers\")\n",
        "    \n",
        "    print(f\"\\nüí∞ Outlier Price Statistics:\")\n",
        "    print(f\"   ‚Ä¢ Minimum outlier price: R$ {price_outliers_iqr.min():.2f}\")\n",
        "    print(f\"   ‚Ä¢ Maximum outlier price: R$ {price_outliers_iqr.max():.2f}\")\n",
        "    print(f\"   ‚Ä¢ Average outlier price: R$ {price_outliers_iqr.mean():.2f}\")\n",
        "\n",
        "# Visualize outliers\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.boxplot(business_data['price'])\n",
        "plt.title('Price Outliers\\n(Box Plot)')\n",
        "plt.ylabel('Price (R$)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(range(len(business_data)), business_data['price'], alpha=0.5, s=1)\n",
        "plt.axhline(y=business_data['price'].quantile(0.75) + 1.5*(business_data['price'].quantile(0.75) - business_data['price'].quantile(0.25)), \n",
        "           color='red', linestyle='--', label='Upper Outlier Threshold')\n",
        "plt.title('Price Scatter Plot\\nwith Outlier Threshold')\n",
        "plt.xlabel('Data Point Index')\n",
        "plt.ylabel('Price (R$)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "business_data['price'].plot(kind='hist', bins=50, alpha=0.7, color='lightcoral')\n",
        "plt.axvline(x=price_outliers_iqr.min(), color='red', linestyle='--', label='Outlier Threshold')\n",
        "plt.title('Price Distribution\\nwith Outlier Boundary')\n",
        "plt.xlabel('Price (R$)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "correlation_analysis"
      },
      "source": [
        "## 5. Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "correlation_matrix"
      },
      "outputs": [],
      "source": [
        "# Correlation Analysis\n",
        "print(\"üîó Correlation Analysis\")\n",
        "print(\"=\" * 25)\n",
        "\n",
        "# Select numeric columns for correlation analysis\n",
        "numeric_cols = ['price', 'freight_value', 'total_item_value', 'profit_margin', \n",
        "                'freight_ratio', 'product_weight_g', 'product_volume']\n",
        "\n",
        "# Remove rows with missing values for correlation analysis\n",
        "correlation_data = business_data[numeric_cols].dropna()\n",
        "\n",
        "print(f\"üìä Analyzing correlations for {len(correlation_data):,} complete records\")\n",
        "\n",
        "# Calculate correlation matrices\n",
        "pearson_corr = correlation_data.corr(method='pearson')\n",
        "spearman_corr = correlation_data.corr(method='spearman')\n",
        "\n",
        "# Visualize correlation matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# Pearson correlation heatmap\n",
        "mask = np.triu(np.ones_like(pearson_corr, dtype=bool))\n",
        "sns.heatmap(pearson_corr, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
        "            square=True, fmt='.3f', cbar_kws={\"shrink\": .8}, ax=axes[0])\n",
        "axes[0].set_title('Pearson Correlation Matrix\\n(Linear Relationships)', fontweight='bold')\n",
        "\n",
        "# Spearman correlation heatmap\n",
        "sns.heatmap(spearman_corr, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
        "            square=True, fmt='.3f', cbar_kws={\"shrink\": .8}, ax=axes[1])\n",
        "axes[1].set_title('Spearman Correlation Matrix\\n(Monotonic Relationships)', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Identify strong correlations\n",
        "def find_strong_correlations(corr_matrix, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Find pairs of variables with strong correlations\n",
        "    \"\"\"\n",
        "    strong_corrs = []\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            corr_value = corr_matrix.iloc[i, j]\n",
        "            if abs(corr_value) >= threshold:\n",
        "                strong_corrs.append({\n",
        "                    'Variable 1': corr_matrix.columns[i],\n",
        "                    'Variable 2': corr_matrix.columns[j],\n",
        "                    'Correlation': corr_value\n",
        "                })\n",
        "    return pd.DataFrame(strong_corrs).sort_values('Correlation', key=abs, ascending=False)\n",
        "\n",
        "strong_pearson = find_strong_correlations(pearson_corr, 0.5)\n",
        "strong_spearman = find_strong_correlations(spearman_corr, 0.5)\n",
        "\n",
        "print(f\"\\nüîç Strong Pearson Correlations (|r| ‚â• 0.5):\")\n",
        "if not strong_pearson.empty:\n",
        "    display(strong_pearson)\n",
        "else:\n",
        "    print(\"   No strong linear correlations found.\")\n",
        "\n",
        "print(f\"\\nüîç Strong Spearman Correlations (|œÅ| ‚â• 0.5):\")\n",
        "if not strong_spearman.empty:\n",
        "    display(strong_spearman)\n",
        "else:\n",
        "    print(\"   No strong monotonic correlations found.\")\n",
        "\n",
        "# Business interpretation of correlations\n",
        "print(f\"\\nüí° Business Insights from Correlations:\")\n",
        "price_freight_corr = pearson_corr.loc['price', 'freight_value']\n",
        "price_volume_corr = pearson_corr.loc['price', 'product_volume'] if 'product_volume' in pearson_corr.columns else 0\n",
        "\n",
        "print(f\"   ‚Ä¢ Price-Freight correlation: {price_freight_corr:.3f}\")\n",
        "if abs(price_freight_corr) > 0.3:\n",
        "    print(f\"     ‚Üí Moderate relationship between item price and shipping cost\")\n",
        "else:\n",
        "    print(f\"     ‚Üí Weak relationship between item price and shipping cost\")\n",
        "\n",
        "if 'product_volume' in correlation_data.columns:\n",
        "    print(f\"   ‚Ä¢ Price-Volume correlation: {price_volume_corr:.3f}\")\n",
        "    if abs(price_volume_corr) > 0.3:\n",
        "        print(f\"     ‚Üí Product size influences pricing\")\n",
        "    else:\n",
        "        print(f\"     ‚Üí Product size has little impact on pricing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "executive_summary"
      },
      "source": [
        "## 6. Executive Summary Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "executive_dashboard"
      },
      "outputs": [],
      "source": [
        "# Executive Summary Dashboard\n",
        "print(\"üìà EXECUTIVE SUMMARY - OLIST E-COMMERCE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Key Performance Indicators\n",
        "total_revenue = business_data['total_item_value'].sum()\n",
        "total_orders = business_data['order_id'].nunique()\n",
        "total_products_sold = len(business_data)\n",
        "avg_order_value = business_data.groupby('order_id')['total_item_value'].sum().mean()\n",
        "unique_products = business_data['product_id'].nunique()\n",
        "unique_categories = business_data['category_clean'].nunique()\n",
        "\n",
        "print(f\"\\nüìä KEY PERFORMANCE INDICATORS:\")\n",
        "print(f\"   üí∞ Total Revenue: R$ {total_revenue:,.2f}\")\n",
        "print(f\"   üì¶ Total Orders: {total_orders:,}\")\n",
        "print(f\"   üõí Total Products Sold: {total_products_sold:,}\")\n",
        "print(f\"   üíµ Average Order Value: R$ {avg_order_value:.2f}\")\n",
        "print(f\"   üìã Unique Products: {unique_products:,}\")\n",
        "print(f\"   üè∑Ô∏è Product Categories: {unique_categories}\")\n",
        "\n",
        "# Top performers\n",
        "print(f\"\\nüèÜ TOP PERFORMERS:\")\n",
        "top_category_by_revenue = business_data.groupby('category_clean')['total_item_value'].sum().idxmax()\n",
        "top_category_revenue = business_data.groupby('category_clean')['total_item_value'].sum().max()\n",
        "print(f\"   ü•á Top Category by Revenue: {top_category_by_revenue} (R$ {top_category_revenue:,.2f})\")\n",
        "\n",
        "most_expensive_category = business_data.groupby('category_clean')['price'].mean().idxmax()\n",
        "highest_avg_price = business_data.groupby('category_clean')['price'].mean().max()\n",
        "print(f\"   üíé Highest Average Price Category: {most_expensive_category} (R$ {highest_avg_price:.2f})\")\n",
        "\n",
        "# Risk indicators\n",
        "print(f\"\\n‚ö†Ô∏è RISK INDICATORS:\")\n",
        "high_freight_orders = (business_data['freight_ratio'] > 0.3).sum()\n",
        "high_freight_pct = (high_freight_orders / len(business_data)) * 100\n",
        "print(f\"   üöõ High Freight Ratio Orders: {high_freight_orders:,} ({high_freight_pct:.1f}%)\")\n",
        "print(f\"   üìä Price Distribution Skewness: {business_data['price'].skew():.2f} (highly right-skewed)\")\n",
        "\n",
        "# Recommendations\n",
        "print(f\"\\nüéØ STRATEGIC RECOMMENDATIONS:\")\n",
        "print(f\"   1. Focus marketing efforts on top-performing category: {top_category_by_revenue}\")\n",
        "print(f\"   2. Investigate {high_freight_pct:.1f}% of orders with high freight costs\")\n",
        "print(f\"   3. Consider premium pricing strategy for {most_expensive_category} category\")\n",
        "print(f\"   4. Implement outlier detection system for price anomalies\")\n",
        "print(f\"   5. Optimize product mix based on profit margin analysis\")\n",
        "\n",
        "# Create summary visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Executive Summary Dashboard', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Revenue by category (top 10)\n",
        "category_revenue = business_data.groupby('category_clean')['total_item_value'].sum().sort_values(ascending=False).head(10)\n",
        "category_revenue.plot(kind='bar', ax=axes[0, 0], color='steelblue')\n",
        "axes[0, 0].set_title('Top 10 Categories by Revenue')\n",
        "axes[0, 0].set_xlabel('Category')\n",
        "axes[0, 0].set_ylabel('Revenue (R$)')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Order value distribution\n",
        "order_values = business_data.groupby('order_id')['total_item_value'].sum()\n",
        "axes[0, 1].hist(order_values, bins=50, alpha=0.7, color='lightcoral')\n",
        "axes[0, 1].axvline(order_values.mean(), color='red', linestyle='--', label=f'Mean: R${order_values.mean():.2f}')\n",
        "axes[0, 1].set_title('Order Value Distribution')\n",
        "axes[0, 1].set_xlabel('Order Value (R$)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Freight ratio distribution\n",
        "axes[1, 0].hist(business_data['freight_ratio'], bins=50, alpha=0.7, color='lightgreen')\n",
        "axes[1, 0].axvline(business_data['freight_ratio'].mean(), color='red', linestyle='--', \n",
        "                  label=f'Mean: {business_data[\"freight_ratio\"].mean()*100:.1f}%')\n",
        "axes[1, 0].set_title('Freight Ratio Distribution')\n",
        "axes[1, 0].set_xlabel('Freight as % of Price')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Monthly revenue trend\n",
        "monthly_revenue = business_data.groupby('order_month')['total_item_value'].sum()\n",
        "monthly_revenue.plot(kind='line', marker='o', ax=axes[1, 1], color='purple', linewidth=2)\n",
        "axes[1, 1].set_title('Monthly Revenue Trend')\n",
        "axes[1, 1].set_xlabel('Month')\n",
        "axes[1, 1].set_ylabel('Revenue (R$)')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "automated_reporting"
      },
      "source": [
        "## 7. Automated EDA Report Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "automated_eda_function"
      },
      "outputs": [],
      "source": [
        "def generate_eda_report(dataframe, target_column=None, categorical_threshold=10):\n",
        "    \"\"\"\n",
        "    Generate comprehensive EDA report for any dataset\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    dataframe : pd.DataFrame\n",
        "        The dataset to analyze\n",
        "    target_column : str, optional\n",
        "        Name of target variable for supervised learning analysis\n",
        "    categorical_threshold : int\n",
        "        Maximum unique values to consider a column categorical\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"üîç AUTOMATED EDA REPORT\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Dataset overview\n",
        "    print(f\"\\nüìä DATASET OVERVIEW:\")\n",
        "    print(f\"   Shape: {dataframe.shape[0]:,} rows √ó {dataframe.shape[1]} columns\")\n",
        "    print(f\"   Memory usage: {dataframe.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "    \n",
        "    # Data types summary\n",
        "    print(f\"\\nüìã DATA TYPES:\")\n",
        "    dtype_counts = dataframe.dtypes.value_counts()\n",
        "    for dtype, count in dtype_counts.items():\n",
        "        print(f\"   {dtype}: {count} columns\")\n",
        "    \n",
        "    # Missing values analysis\n",
        "    print(f\"\\n‚ùì MISSING VALUES:\")\n",
        "    missing_summary = dataframe.isnull().sum()\n",
        "    missing_pct = (missing_summary / len(dataframe)) * 100\n",
        "    \n",
        "    if missing_summary.sum() > 0:\n",
        "        missing_df = pd.DataFrame({\n",
        "            'Missing Count': missing_summary[missing_summary > 0],\n",
        "            'Missing %': missing_pct[missing_summary > 0]\n",
        "        }).sort_values('Missing %', ascending=False)\n",
        "        display(missing_df)\n",
        "    else:\n",
        "        print(\"   ‚úÖ No missing values found!\")\n",
        "    \n",
        "    # Identify column types\n",
        "    numeric_cols = dataframe.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = []\n",
        "    \n",
        "    for col in dataframe.columns:\n",
        "        if col not in numeric_cols:\n",
        "            if dataframe[col].nunique() <= categorical_threshold:\n",
        "                categorical_cols.append(col)\n",
        "    \n",
        "    # Numeric variables analysis\n",
        "    if numeric_cols:\n",
        "        print(f\"\\nüìà NUMERIC VARIABLES SUMMARY:\")\n",
        "        numeric_summary = dataframe[numeric_cols].describe()\n",
        "        display(numeric_summary)\n",
        "        \n",
        "        # Skewness analysis\n",
        "        skewness = dataframe[numeric_cols].skew()\n",
        "        print(f\"\\nüìä SKEWNESS ANALYSIS:\")\n",
        "        for col, skew_val in skewness.items():\n",
        "            if abs(skew_val) > 1:\n",
        "                skew_type = \"highly skewed\"\n",
        "            elif abs(skew_val) > 0.5:\n",
        "                skew_type = \"moderately skewed\"\n",
        "            else:\n",
        "                skew_type = \"approximately normal\"\n",
        "            print(f\"   {col}: {skew_val:.2f} ({skew_type})\")\n",
        "    \n",
        "    # Categorical variables analysis\n",
        "    if categorical_cols:\n",
        "        print(f\"\\nüè∑Ô∏è CATEGORICAL VARIABLES SUMMARY:\")\n",
        "        for col in categorical_cols:\n",
        "            unique_count = dataframe[col].nunique()\n",
        "            most_common = dataframe[col].mode().iloc[0] if len(dataframe[col].mode()) > 0 else 'N/A'\n",
        "            print(f\"   {col}: {unique_count} unique values, most common: '{most_common}'\")\n",
        "    \n",
        "    # Correlation analysis for numeric variables\n",
        "    if len(numeric_cols) > 1:\n",
        "        print(f\"\\nüîó CORRELATION ANALYSIS:\")\n",
        "        corr_matrix = dataframe[numeric_cols].corr()\n",
        "        \n",
        "        # Find strong correlations\n",
        "        strong_corrs = []\n",
        "        for i in range(len(corr_matrix.columns)):\n",
        "            for j in range(i+1, len(corr_matrix.columns)):\n",
        "                corr_value = corr_matrix.iloc[i, j]\n",
        "                if abs(corr_value) >= 0.5:\n",
        "                    strong_corrs.append({\n",
        "                        'Variable 1': corr_matrix.columns[i],\n",
        "                        'Variable 2': corr_matrix.columns[j],\n",
        "                        'Correlation': corr_value\n",
        "                    })\n",
        "        \n",
        "        if strong_corrs:\n",
        "            strong_corr_df = pd.DataFrame(strong_corrs).sort_values('Correlation', key=abs, ascending=False)\n",
        "            display(strong_corr_df)\n",
        "        else:\n",
        "            print(\"   No strong correlations (|r| ‚â• 0.5) found.\")\n",
        "    \n",
        "    # Outlier analysis for numeric variables\n",
        "    if numeric_cols:\n",
        "        print(f\"\\nüéØ OUTLIER ANALYSIS (IQR Method):\")\n",
        "        for col in numeric_cols:\n",
        "            Q1 = dataframe[col].quantile(0.25)\n",
        "            Q3 = dataframe[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            outliers = dataframe[(dataframe[col] < lower_bound) | (dataframe[col] > upper_bound)]\n",
        "            outlier_pct = (len(outliers) / len(dataframe)) * 100\n",
        "            print(f\"   {col}: {len(outliers):,} outliers ({outlier_pct:.2f}%)\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ EDA Report Complete!\")\n",
        "    return {\n",
        "        'numeric_columns': numeric_cols,\n",
        "        'categorical_columns': categorical_cols,\n",
        "        'missing_summary': missing_summary,\n",
        "        'correlation_matrix': corr_matrix if len(numeric_cols) > 1 else None\n",
        "    }\n",
        "\n",
        "# Test the automated EDA function\n",
        "print(\"üß™ Testing Automated EDA Function on Sample Data\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create sample dataset\n",
        "sample_data = business_data[['price', 'freight_value', 'total_item_value', \n",
        "                           'category_clean', 'profit_margin', 'order_year']].sample(1000)\n",
        "\n",
        "eda_results = generate_eda_report(sample_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary_conclusions"
      },
      "source": [
        "## Summary and Conclusions\n",
        "\n",
        "### What We've Accomplished\n",
        "\n",
        "1. **‚úÖ Comprehensive Descriptive Statistics**: Applied advanced statistical measures to understand our business data\n",
        "2. **‚úÖ Business-Focused Analysis**: Translated statistical insights into actionable business intelligence\n",
        "3. **‚úÖ Outlier Detection**: Identified anomalies that require business attention\n",
        "4. **‚úÖ Correlation Analysis**: Discovered relationships between key business metrics\n",
        "5. **‚úÖ Executive Dashboard**: Created summary visualizations for stakeholder reporting\n",
        "6. **‚úÖ Automated EDA Function**: Built reusable tools for future analysis\n",
        "\n",
        "### Key Business Insights\n",
        "\n",
        "**Financial Performance:**\n",
        "- Clear understanding of order value distributions and pricing patterns\n",
        "- Identification of high and low-performing product categories\n",
        "- Freight cost analysis revealing potential optimization opportunities\n",
        "\n",
        "**Risk Management:**\n",
        "- Systematic outlier detection for price anomalies\n",
        "- Statistical validation of data quality\n",
        "- Identification of categories requiring attention\n",
        "\n",
        "**Strategic Recommendations:**\n",
        "- Data-driven category performance insights\n",
        "- Pricing strategy recommendations based on statistical analysis\n",
        "- Operational efficiency opportunities in freight management\n",
        "\n",
        "### Next Steps\n",
        "In Part 3, we'll explore:\n",
        "- Advanced distribution analysis techniques\n",
        "- Deep correlation exploration with visualization\n",
        "- Statistical hypothesis testing for business questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "practice_exercises_part2"
      },
      "source": [
        "## üéØ Practice Exercises - Part 2\n",
        "\n",
        "Apply your descriptive statistics knowledge:\n",
        "\n",
        "1. **Category Deep Dive**: Choose a product category and perform comprehensive descriptive analysis\n",
        "\n",
        "2. **Profit Margin Analysis**: Calculate and analyze profit margins by different business dimensions\n",
        "\n",
        "3. **Custom EDA Function**: Enhance the automated EDA function with additional statistical tests\n",
        "\n",
        "4. **Business Metric Creation**: Define and calculate new business KPIs from the available data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise_space_part2"
      },
      "outputs": [],
      "source": [
        "# Exercise space for Part 2 - Descriptive Statistics\n",
        "\n",
        "# Exercise 1: Category Deep Dive\n",
        "# Choose a category and analyze its complete statistical profile\n",
        "\n",
        "# Exercise 2: Profit Margin Analysis\n",
        "# Calculate profit margins across different dimensions\n",
        "\n",
        "# Exercise 3: Enhanced EDA Function\n",
        "# Add statistical tests to the automated EDA function\n",
        "\n",
        "# Exercise 4: Business KPI Creation\n",
        "# Define new metrics relevant to e-commerce business"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}