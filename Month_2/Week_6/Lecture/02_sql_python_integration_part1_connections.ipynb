{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - SQL and Python Integration Part 1: Database Connections\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Establish PostgreSQL database connections from Python using SQLAlchemy\n",
    "2. Connect to cloud databases (Supabase) for real-world data analysis\n",
    "3. Execute SQL queries from Python notebooks using real e-commerce data\n",
    "4. Understand the relationship between SQL databases and Python DataFrames\n",
    "5. Implement proper connection management and error handling\n",
    "6. Compare SQL and Pandas approaches for business analytics\n",
    "\n",
    "## Business Context: Bridging SQL and Python\n",
    "\n",
    "In modern business environments, data often lives in **cloud databases** while analysis happens in **Python**. The ability to seamlessly bridge these two worlds is essential for:\n",
    "\n",
    "- **Real-time Data Access** - Connect directly to live business systems\n",
    "- **Scalability** - Handle enterprise-scale datasets\n",
    "- **Collaboration** - Multiple analysts accessing the same data source\n",
    "- **Performance** - Leverage database engines for heavy computation\n",
    "- **Integration** - Combine SQL's querying power with Python's analytical capabilities\n",
    "\n",
    "Today we'll master connecting Python to **PostgreSQL databases** using **Supabase** (a cloud database platform) and work with real Olist e-commerce data that's already stored in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries for PostgreSQL database connectivity\nimport pandas as pd\nimport numpy as np\nimport sqlalchemy\nfrom sqlalchemy import create_engine, text, inspect\nfrom datetime import datetime, timedelta\nimport warnings\nimport os\nfrom dotenv import load_dotenv\n\nwarnings.filterwarnings('ignore')\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Supabase PostgreSQL Database Configuration from environment variables\nDATABASE_CONFIG = {\n    'host': os.getenv('POSTGRES_HOST'),\n    'port': int(os.getenv('POSTGRES_PORT', 5432)),\n    'database': os.getenv('POSTGRES_DATABASE'),\n    'user': os.getenv('POSTGRES_USER'),\n    'password': os.getenv('POSTGRES_PASSWORD'),\n    'connection_timeout': 30,\n    'echo': False  # Set to True to see SQL queries\n}\n\n# Verify that environment variables were loaded\nif not all([DATABASE_CONFIG['host'], DATABASE_CONFIG['user'], DATABASE_CONFIG['password']]):\n    raise ValueError(\"Missing required database credentials. Please check your .env file.\")\n\n# PostgreSQL connection string\nPOSTGRES_URL = f\"postgresql://{DATABASE_CONFIG['user']}:{DATABASE_CONFIG['password']}@{DATABASE_CONFIG['host']}:{DATABASE_CONFIG['port']}/{DATABASE_CONFIG['database']}\"\n\nprint(\"üêò PostgreSQL-Python Integration Environment Ready!\")\nprint(f\"SQLAlchemy version: {sqlalchemy.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(\"‚úÖ Connecting to Supabase PostgreSQL Database...\")\nprint(\"üóÑÔ∏è Real Olist E-commerce & Marketing data awaits!\")\nprint(\"üîí Database credentials loaded securely from .env file\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SQLAlchemy Basics and PostgreSQL Connection\n",
    "\n",
    "**SQLAlchemy** is Python's most popular database toolkit. It provides:\n",
    "- **Connection Management**: Handle database connections efficiently\n",
    "- **SQL Query Execution**: Run SQL directly from Python\n",
    "- **ORM (Object-Relational Mapping)**: Map Python objects to database tables\n",
    "- **Database Abstraction**: Work with different databases using the same API\n",
    "\n",
    "**PostgreSQL** is an enterprise-grade database that excels at:\n",
    "- **Complex Queries**: Advanced SQL features like window functions, CTEs\n",
    "- **Scalability**: Handle millions of rows efficiently  \n",
    "- **Data Integrity**: ACID compliance for business-critical data\n",
    "- **JSON Support**: Store and query semi-structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostgreSQLManager:\n",
    "    \"\"\"\n",
    "    Professional PostgreSQL connection manager for production-ready applications.\n",
    "    Handles connection pooling, error handling, and resource management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, connection_url=None):\n",
    "        self.connection_url = connection_url or POSTGRES_URL\n",
    "        self.engine = None\n",
    "        self._connect()\n",
    "    \n",
    "    def _connect(self):\n",
    "        \"\"\"\n",
    "        Establish PostgreSQL connection with optimal configuration.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create engine with connection pooling and timeout settings\n",
    "            self.engine = create_engine(\n",
    "                self.connection_url,\n",
    "                echo=DATABASE_CONFIG['echo'],\n",
    "                pool_size=5,                    # Connection pool size\n",
    "                max_overflow=10,                # Additional connections if needed\n",
    "                pool_timeout=DATABASE_CONFIG['connection_timeout'],\n",
    "                pool_recycle=3600,              # Recycle connections every hour\n",
    "                connect_args={\n",
    "                    \"connect_timeout\": DATABASE_CONFIG['connection_timeout'],\n",
    "                    \"application_name\": \"Python_Data_Analysis_Course\"\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Test connection\n",
    "            with self.engine.connect() as conn:\n",
    "                result = conn.execute(text(\"SELECT version()\"))\n",
    "                version = result.scalar()\n",
    "                print(\"‚úÖ PostgreSQL connection established successfully\")\n",
    "                print(f\"üêò Database version: {version[:50]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå PostgreSQL connection failed: {e}\")\n",
    "            print(\"üîß Troubleshooting tips:\")\n",
    "            print(\"  ‚Ä¢ Check your internet connection\")\n",
    "            print(\"  ‚Ä¢ Verify database credentials\")\n",
    "            print(\"  ‚Ä¢ Ensure Supabase database is running\")\n",
    "            raise\n",
    "    \n",
    "    def get_table_info(self):\n",
    "        \"\"\"\n",
    "        Get comprehensive information about all tables in the database.\n",
    "        \"\"\"\n",
    "        inspector = inspect(self.engine)\n",
    "        tables = inspector.get_table_names()\n",
    "        \n",
    "        table_info = {}\n",
    "        \n",
    "        print(\"üìã Discovering database schema...\")\n",
    "        \n",
    "        for table in tables:\n",
    "            try:\n",
    "                with self.engine.connect() as conn:\n",
    "                    # Get row count\n",
    "                    result = conn.execute(text(f'SELECT COUNT(*) FROM \"{table}\"'))\n",
    "                    row_count = result.scalar()\n",
    "                    \n",
    "                    # Get column information\n",
    "                    columns = inspector.get_columns(table)\n",
    "                    \n",
    "                    table_info[table] = {\n",
    "                        'rows': row_count,\n",
    "                        'columns': [col['name'] for col in columns],\n",
    "                        'column_types': {col['name']: str(col['type']) for col in columns}\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  üìä {table}: {row_count:,} rows, {len(columns)} columns\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Could not access {table}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return table_info\n",
    "    \n",
    "    def execute_query(self, query, params=None):\n",
    "        \"\"\"\n",
    "        Execute a SQL query with proper error handling and return results as DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.engine.connect() as conn:\n",
    "                if params:\n",
    "                    result = pd.read_sql(text(query), conn, params=params)\n",
    "                else:\n",
    "                    result = pd.read_sql(text(query), conn)\n",
    "                \n",
    "                return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Query execution failed: {e}\")\n",
    "            print(f\"üìù Query: {query[:100]}...\")\n",
    "            raise\n",
    "    \n",
    "    def get_sample_data(self, table_name, limit=5):\n",
    "        \"\"\"\n",
    "        Get sample data from a table for exploration.\n",
    "        \"\"\"\n",
    "        query = f'SELECT * FROM \"{table_name}\" LIMIT {limit}'\n",
    "        return self.execute_query(query)\n",
    "    \n",
    "    def get_table_schema(self, table_name):\n",
    "        \"\"\"\n",
    "        Get detailed schema information for a specific table.\n",
    "        \"\"\"\n",
    "        inspector = inspect(self.engine)\n",
    "        columns = inspector.get_columns(table_name)\n",
    "        \n",
    "        schema_df = pd.DataFrame([\n",
    "            {\n",
    "                'column_name': col['name'],\n",
    "                'data_type': str(col['type']),\n",
    "                'nullable': col['nullable'],\n",
    "                'default': col.get('default'),\n",
    "                'primary_key': col.get('primary_key', False)\n",
    "            }\n",
    "            for col in columns\n",
    "        ])\n",
    "        \n",
    "        return schema_df\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Properly close database connections.\n",
    "        \"\"\"\n",
    "        if self.engine:\n",
    "            self.engine.dispose()\n",
    "            print(\"üîí PostgreSQL connections closed\")\n",
    "\n",
    "# Create database manager instance and connect to Supabase\n",
    "print(\"üöÄ Connecting to Supabase PostgreSQL Database...\")\n",
    "db = PostgreSQLManager()\n",
    "\n",
    "# Display database information\n",
    "print(\"\\nüìä Olist E-commerce Database Overview:\")\n",
    "db_info = db.get_table_info()\n",
    "\n",
    "print(f\"\\nüóÉÔ∏è Total tables discovered: {len(db_info)}\")\n",
    "total_rows = sum(info['rows'] for info in db_info.values())\n",
    "print(f\"üìè Total rows across all tables: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Database Schema\n",
    "\n",
    "Let's explore the structure of our Olist e-commerce database to understand the business data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the structure of key business tables\n",
    "print(\"üîç Database Schema Exploration\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Identify the main datasets\n",
    "sales_tables = [table for table in db_info.keys() if 'olist_sales_data_set' in table]\n",
    "marketing_tables = [table for table in db_info.keys() if 'olist_marketing_data_set' in table]\n",
    "\n",
    "print(f\"\\nüìä OLIST SALES DATASET Tables:\")\n",
    "for table in sales_tables:\n",
    "    if table in db_info:\n",
    "        info = db_info[table]\n",
    "        print(f\"  ‚Ä¢ {table}: {info['rows']:,} rows, {len(info['columns'])} columns\")\n",
    "\n",
    "print(f\"\\nüìà OLIST MARKETING DATASET Tables:\")\n",
    "for table in marketing_tables:\n",
    "    if table in db_info:\n",
    "        info = db_info[table]\n",
    "        print(f\"  ‚Ä¢ {table}: {info['rows']:,} rows, {len(info['columns'])} columns\")\n",
    "\n",
    "# Let's explore the main sales tables if they exist\n",
    "if sales_tables:\n",
    "    main_sales_table = sales_tables[0]  # Assume first table is main one\n",
    "    print(f\"\\nüìã {main_sales_table.upper()} Schema:\")\n",
    "    sales_schema = db.get_table_schema(main_sales_table)\n",
    "    display(sales_schema)\n",
    "    \n",
    "    print(f\"\\nüì¶ Sample data from {main_sales_table}:\")\n",
    "    sales_sample = db.get_sample_data(main_sales_table, 3)\n",
    "    display(sales_sample)\n",
    "\n",
    "# Let's explore the main marketing tables if they exist\n",
    "if marketing_tables:\n",
    "    main_marketing_table = marketing_tables[0]  # Assume first table is main one\n",
    "    print(f\"\\nüìã {main_marketing_table.upper()} Schema:\")\n",
    "    marketing_schema = db.get_table_schema(main_marketing_table)\n",
    "    display(marketing_schema)\n",
    "    \n",
    "    print(f\"\\nüì¶ Sample data from {main_marketing_table}:\")\n",
    "    marketing_sample = db.get_sample_data(main_marketing_table, 3)\n",
    "    display(marketing_sample)\n",
    "\n",
    "print(\"\\nüí° Business Data Model Understanding:\")\n",
    "print(\"  üîó Sales dataset contains: customer orders, products, payments, reviews\")\n",
    "print(\"  üîó Marketing dataset contains: lead generation, conversions, channel data\")\n",
    "print(\"  üîó These datasets can be joined to analyze customer acquisition and behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running SQL Queries from Python\n",
    "\n",
    "Now let's execute SQL queries directly from Python and see how they work with real cloud data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with basic data exploration using SQL\n",
    "print(\"üîç SQL Query Execution Examples\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# First, let's check what columns are available in our main datasets\n",
    "def explore_dataset_structure():\n",
    "    \"\"\"Explore the structure of our main datasets\"\"\"\n",
    "    \n",
    "    # Check sales dataset structure\n",
    "    if sales_tables:\n",
    "        print(f\"\\nüìä SALES DATASET - {sales_tables[0]} columns:\")\n",
    "        sales_cols = db_info[sales_tables[0]]['columns']\n",
    "        print(f\"  {sales_cols[:10]}...\")  # Show first 10 columns\n",
    "    \n",
    "    # Check marketing dataset structure\n",
    "    if marketing_tables:\n",
    "        print(f\"\\nüìà MARKETING DATASET - {marketing_tables[0]} columns:\")\n",
    "        marketing_cols = db_info[marketing_tables[0]]['columns']\n",
    "        print(f\"  {marketing_cols[:10]}...\")  # Show first 10 columns\n",
    "\n",
    "explore_dataset_structure()\n",
    "\n",
    "# Example 1: Basic data exploration\n",
    "print(\"\\nüìã Example 1: Basic Data Exploration\")\n",
    "\n",
    "if sales_tables:\n",
    "    # Get basic statistics from sales data\n",
    "    basic_stats_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(DISTINCT CASE WHEN \"customer_id\" IS NOT NULL THEN \"customer_id\" END) as unique_customers,\n",
    "        MIN(\"order_purchase_timestamp\") as earliest_order,\n",
    "        MAX(\"order_purchase_timestamp\") as latest_order\n",
    "    FROM \"{sales_tables[0]}\"\n",
    "    WHERE \"order_purchase_timestamp\" IS NOT NULL\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        basic_stats = db.execute_query(basic_stats_query)\n",
    "        print(\"‚úÖ Basic Sales Statistics:\")\n",
    "        display(basic_stats)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not execute basic stats query: {e}\")\n",
    "        print(\"Trying alternative approach...\")\n",
    "        \n",
    "        # Fallback: just count total records\n",
    "        simple_query = f'SELECT COUNT(*) as total_records FROM \"{sales_tables[0]}\"'\n",
    "        try:\n",
    "            simple_stats = db.execute_query(simple_query)\n",
    "            print(\"‚úÖ Simple Record Count:\")\n",
    "            display(simple_stats)\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Could not execute any query: {e2}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Customer analysis (adapting to actual schema)\n",
    "print(\"\\nüíº Example 2: Customer Analysis\")\n",
    "print(\"Business Question: What can we learn about our customer base?\")\n",
    "\n",
    "if sales_tables:\n",
    "    # Let's examine the actual structure first\n",
    "    sample_data = db.get_sample_data(sales_tables[0], 1)\n",
    "    print(f\"\\nActual columns in {sales_tables[0]}:\")\n",
    "    print(list(sample_data.columns))\n",
    "    \n",
    "    # Adapt query based on available columns\n",
    "    available_columns = db_info[sales_tables[0]]['columns']\n",
    "    \n",
    "    # Look for customer-related columns\n",
    "    customer_cols = [col for col in available_columns if 'customer' in col.lower()]\n",
    "    state_cols = [col for col in available_columns if 'state' in col.lower()]\n",
    "    \n",
    "    print(f\"\\nCustomer-related columns: {customer_cols}\")\n",
    "    print(f\"State-related columns: {state_cols}\")\n",
    "    \n",
    "    if customer_cols and state_cols:\n",
    "        # Build a customer analysis query with available columns\n",
    "        customer_analysis_query = f\"\"\"\n",
    "        SELECT \n",
    "            \"{state_cols[0]}\" as customer_state,\n",
    "            COUNT(*) as order_count,\n",
    "            COUNT(DISTINCT \"{customer_cols[0]}\") as unique_customers\n",
    "        FROM \"{sales_tables[0]}\"\n",
    "        WHERE \"{state_cols[0]}\" IS NOT NULL\n",
    "        GROUP BY \"{state_cols[0]}\"\n",
    "        ORDER BY order_count DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            customer_analysis = db.execute_query(customer_analysis_query)\n",
    "            print(\"\\n‚úÖ Customer Analysis by State:\")\n",
    "            display(customer_analysis)\n",
    "            \n",
    "            if len(customer_analysis) > 0:\n",
    "                top_state = customer_analysis.iloc[0]\n",
    "                print(f\"\\nüí° Key Insights:\")\n",
    "                print(f\"  ‚Ä¢ Top state: {top_state['customer_state']} ({top_state['order_count']:,} orders)\")\n",
    "                print(f\"  ‚Ä¢ Total states analyzed: {len(customer_analysis)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Customer analysis failed: {e}\")\n",
    "    \n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Marketing funnel analysis\n",
    "print(\"\\nüìà Example 3: Marketing Analysis\")\n",
    "print(\"Business Question: How effective are our marketing channels?\")\n",
    "\n",
    "if marketing_tables:\n",
    "    # Examine marketing table structure\n",
    "    marketing_sample = db.get_sample_data(marketing_tables[0], 1)\n",
    "    print(f\"\\nActual columns in {marketing_tables[0]}:\")\n",
    "    print(list(marketing_sample.columns))\n",
    "    \n",
    "    marketing_columns = db_info[marketing_tables[0]]['columns']\n",
    "    \n",
    "    # Look for relevant marketing columns\n",
    "    channel_cols = [col for col in marketing_columns if any(keyword in col.lower() for keyword in ['origin', 'source', 'channel', 'medium'])]\n",
    "    lead_cols = [col for col in marketing_columns if any(keyword in col.lower() for keyword in ['lead', 'mql', 'conversion'])]\n",
    "    \n",
    "    print(f\"\\nChannel-related columns: {channel_cols}\")\n",
    "    print(f\"Lead-related columns: {lead_cols}\")\n",
    "    \n",
    "    if channel_cols:\n",
    "        # Build marketing analysis query\n",
    "        marketing_query = f\"\"\"\n",
    "        SELECT \n",
    "            \"{channel_cols[0]}\" as marketing_channel,\n",
    "            COUNT(*) as total_leads,\n",
    "            ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "        FROM \"{marketing_tables[0]}\"\n",
    "        WHERE \"{channel_cols[0]}\" IS NOT NULL\n",
    "        GROUP BY \"{channel_cols[0]}\"\n",
    "        ORDER BY total_leads DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            marketing_analysis = db.execute_query(marketing_query)\n",
    "            print(\"\\n‚úÖ Marketing Channel Performance:\")\n",
    "            display(marketing_analysis)\n",
    "            \n",
    "            if len(marketing_analysis) > 0:\n",
    "                top_channel = marketing_analysis.iloc[0]\n",
    "                print(f\"\\nüí° Marketing Insights:\")\n",
    "                print(f\"  ‚Ä¢ Top channel: {top_channel['marketing_channel']} ({top_channel['percentage']}% of leads)\")\n",
    "                print(f\"  ‚Ä¢ Total channels: {len(marketing_analysis)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Marketing analysis failed: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No obvious channel columns found, showing sample data:\")\n",
    "        display(marketing_sample)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced SQL Features\n",
    "\n",
    "Let's explore more sophisticated SQL queries that are common in business intelligence scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced SQL Example: Time-based analysis with window functions\n",
    "print(\"üß† Advanced SQL Analysis\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\nüìÖ Example: Time-Based Trend Analysis\")\n",
    "print(\"Business Question: How have our key metrics evolved over time?\")\n",
    "\n",
    "if sales_tables:\n",
    "    # Look for date columns\n",
    "    available_columns = db_info[sales_tables[0]]['columns']\n",
    "    date_cols = [col for col in available_columns if any(keyword in col.lower() for keyword in ['date', 'timestamp', 'time'])]\n",
    "    \n",
    "    print(f\"\\nDate-related columns found: {date_cols}\")\n",
    "    \n",
    "    if date_cols:\n",
    "        # Build time-series analysis query\n",
    "        time_analysis_query = f\"\"\"\n",
    "        SELECT \n",
    "            DATE_TRUNC('month', \"{date_cols[0]}\") as month,\n",
    "            COUNT(*) as monthly_records,\n",
    "            LAG(COUNT(*), 1) OVER (ORDER BY DATE_TRUNC('month', \"{date_cols[0]}\")) as prev_month_records,\n",
    "            ROUND(\n",
    "                (COUNT(*) - LAG(COUNT(*), 1) OVER (ORDER BY DATE_TRUNC('month', \"{date_cols[0]}\"))) * 100.0 / \n",
    "                NULLIF(LAG(COUNT(*), 1) OVER (ORDER BY DATE_TRUNC('month', \"{date_cols[0]}\")), 0), \n",
    "                2\n",
    "            ) as month_over_month_growth\n",
    "        FROM \"{sales_tables[0]}\"\n",
    "        WHERE \"{date_cols[0]}\" IS NOT NULL\n",
    "        GROUP BY DATE_TRUNC('month', \"{date_cols[0]}\")\n",
    "        ORDER BY month\n",
    "        LIMIT 12\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            time_analysis = db.execute_query(time_analysis_query)\n",
    "            print(\"\\n‚úÖ Monthly Trend Analysis with Growth Rates:\")\n",
    "            display(time_analysis)\n",
    "            \n",
    "            if len(time_analysis) > 1:\n",
    "                avg_growth = time_analysis['month_over_month_growth'].dropna().mean()\n",
    "                print(f\"\\nüìà Trend Insights:\")\n",
    "                print(f\"  ‚Ä¢ Average monthly growth: {avg_growth:.2f}%\")\n",
    "                print(f\"  ‚Ä¢ Analysis period: {time_analysis['month'].min()} to {time_analysis['month'].max()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Time analysis failed: {e}\")\n",
    "            print(\"Trying simpler date-based query...\")\n",
    "            \n",
    "            # Fallback to simpler query\n",
    "            simple_date_query = f\"\"\"\n",
    "            SELECT \n",
    "                DATE_TRUNC('month', \"{date_cols[0]}\") as month,\n",
    "                COUNT(*) as monthly_records\n",
    "            FROM \"{sales_tables[0]}\"\n",
    "            WHERE \"{date_cols[0]}\" IS NOT NULL\n",
    "            GROUP BY DATE_TRUNC('month', \"{date_cols[0]}\")\n",
    "            ORDER BY month\n",
    "            LIMIT 12\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                simple_time_analysis = db.execute_query(simple_date_query)\n",
    "                print(\"\\n‚úÖ Simple Monthly Trends:\")\n",
    "                display(simple_time_analysis)\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Simple time analysis also failed: {e2}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced SQL Example: Common Table Expressions (CTEs)\n",
    "print(\"\\nüíº Advanced SQL: CTEs for Complex Business Logic\")\n",
    "print(\"Business Question: Can we segment our data for deeper insights?\")\n",
    "\n",
    "if sales_tables:\n",
    "    # Build a more complex query with CTEs\n",
    "    available_columns = db_info[sales_tables[0]]['columns']\n",
    "    \n",
    "    # Look for numeric columns that might represent values\n",
    "    numeric_cols = [col for col in available_columns if any(keyword in col.lower() for keyword in ['price', 'value', 'amount', 'cost'])]\n",
    "    \n",
    "    print(f\"\\nNumeric value columns found: {numeric_cols}\")\n",
    "    \n",
    "    if numeric_cols and state_cols:\n",
    "        # Build CTE query for business segmentation\n",
    "        cte_query = f\"\"\"\n",
    "        WITH regional_stats AS (\n",
    "            SELECT \n",
    "                \"{state_cols[0]}\" as region,\n",
    "                COUNT(*) as total_records,\n",
    "                AVG(\"{numeric_cols[0]}\") as avg_value,\n",
    "                STDDEV(\"{numeric_cols[0]}\") as value_stddev\n",
    "            FROM \"{sales_tables[0]}\"\n",
    "            WHERE \"{state_cols[0]}\" IS NOT NULL \n",
    "                AND \"{numeric_cols[0]}\" IS NOT NULL\n",
    "                AND \"{numeric_cols[0]}\" > 0\n",
    "            GROUP BY \"{state_cols[0]}\"\n",
    "        ),\n",
    "        regional_segments AS (\n",
    "            SELECT \n",
    "                region,\n",
    "                total_records,\n",
    "                ROUND(avg_value, 2) as avg_value,\n",
    "                CASE \n",
    "                    WHEN avg_value > (SELECT AVG(avg_value) FROM regional_stats) THEN 'High Value'\n",
    "                    WHEN total_records > (SELECT AVG(total_records) FROM regional_stats) THEN 'High Volume'\n",
    "                    ELSE 'Standard'\n",
    "                END as segment\n",
    "            FROM regional_stats\n",
    "        )\n",
    "        SELECT \n",
    "            segment,\n",
    "            COUNT(*) as region_count,\n",
    "            SUM(total_records) as total_records,\n",
    "            ROUND(AVG(avg_value), 2) as segment_avg_value\n",
    "        FROM regional_segments\n",
    "        GROUP BY segment\n",
    "        ORDER BY segment_avg_value DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            cte_analysis = db.execute_query(cte_query)\n",
    "            print(\"\\n‚úÖ Regional Segmentation Analysis (using CTEs):\")\n",
    "            display(cte_analysis)\n",
    "            \n",
    "            print(f\"\\nüéØ Segmentation Insights:\")\n",
    "            for _, row in cte_analysis.iterrows():\n",
    "                print(f\"  ‚Ä¢ {row['segment']}: {row['region_count']} regions, avg value: {row['segment_avg_value']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå CTE analysis failed: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Insufficient columns for segmentation analysis\")\n",
    "\n",
    "print(\"\\nüí° Advanced SQL Features Demonstrated:\")\n",
    "print(\"  ‚Ä¢ Window functions (LAG, OVER) for time-series analysis\")\n",
    "print(\"  ‚Ä¢ Date functions (DATE_TRUNC) for temporal grouping\")\n",
    "print(\"  ‚Ä¢ CTEs for complex multi-step business logic\")\n",
    "print(\"  ‚Ä¢ CASE statements for business rule implementation\")\n",
    "print(\"  ‚Ä¢ Subqueries for dynamic threshold calculations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SQL vs Pandas: When to Use Each Approach\n",
    "\n",
    "Let's compare the strengths of SQL versus pandas for different types of data operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sql_vs_pandas_approaches():\n",
    "    \"\"\"\n",
    "    Compare SQL and pandas approaches for different types of analysis.\n",
    "    \"\"\"\n",
    "    print(\"‚ö° SQL vs Pandas: Strategic Comparison\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Example 1: Simple aggregation comparison\n",
    "    print(\"\\nüìä Example 1: Simple Aggregation\")\n",
    "    print(\"Task: Count records by category\")\n",
    "    \n",
    "    if sales_tables and state_cols:\n",
    "        print(\"\\nüóÑÔ∏è SQL Approach:\")\n",
    "        sql_agg_query = f\"\"\"\n",
    "        SELECT \n",
    "            \"{state_cols[0]}\" as category,\n",
    "            COUNT(*) as record_count,\n",
    "            ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "        FROM \"{sales_tables[0]}\"\n",
    "        WHERE \"{state_cols[0]}\" IS NOT NULL\n",
    "        GROUP BY \"{state_cols[0]}\"\n",
    "        ORDER BY record_count DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            sql_result = db.execute_query(sql_agg_query)\n",
    "            print(\"‚úÖ SQL Result:\")\n",
    "            display(sql_result)\n",
    "            \n",
    "            print(\"\\nüêº Pandas Equivalent (conceptual):\")\n",
    "            print(\"\"\"\n",
    "            # If we had the data in a pandas DataFrame:\n",
    "            pandas_result = (\n",
    "                df.groupby('category')['record_id']\n",
    "                .count()\n",
    "                .sort_values(ascending=False)\n",
    "                .head(5)\n",
    "            )\n",
    "            \"\"\")\n",
    "            \n",
    "            # Now let's actually demonstrate with the SQL result\n",
    "            if len(sql_result) > 0:\n",
    "                print(\"\\nüîÑ Converting SQL result to pandas for further analysis:\")\n",
    "                # Calculate additional statistics using pandas\n",
    "                total_records = sql_result['record_count'].sum()\n",
    "                avg_records = sql_result['record_count'].mean()\n",
    "                std_records = sql_result['record_count'].std()\n",
    "                \n",
    "                print(f\"  ‚Ä¢ Total records: {total_records:,}\")\n",
    "                print(f\"  ‚Ä¢ Average per category: {avg_records:.1f}\")\n",
    "                print(f\"  ‚Ä¢ Standard deviation: {std_records:.1f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå SQL aggregation failed: {e}\")\n",
    "    \n",
    "    # Example 2: When SQL excels\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"\\nüìÖ Example 2: When SQL Excels - Date Operations\")\n",
    "    \n",
    "    if date_cols:\n",
    "        print(\"\\nüóÑÔ∏è SQL Approach (Superior for date functions):\")\n",
    "        sql_date_query = f\"\"\"\n",
    "        SELECT \n",
    "            EXTRACT(YEAR FROM \"{date_cols[0]}\") as year,\n",
    "            EXTRACT(QUARTER FROM \"{date_cols[0]}\") as quarter,\n",
    "            COUNT(*) as quarterly_records\n",
    "        FROM \"{sales_tables[0]}\"\n",
    "        WHERE \"{date_cols[0]}\" IS NOT NULL\n",
    "        GROUP BY EXTRACT(YEAR FROM \"{date_cols[0]}\"), EXTRACT(QUARTER FROM \"{date_cols[0]}\")\n",
    "        ORDER BY year, quarter\n",
    "        LIMIT 8\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            sql_date_result = db.execute_query(sql_date_query)\n",
    "            print(\"‚úÖ SQL Date Analysis:\")\n",
    "            display(sql_date_result)\n",
    "            \n",
    "            print(\"\\nüí° SQL Advantage: Date extraction and grouping in one step\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå SQL date analysis failed: {e}\")\n",
    "    \n",
    "    # Analysis summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\\nüéØ When to Use SQL vs Pandas:\")\n",
    "    \n",
    "    print(\"\\nüóÑÔ∏è Use SQL when:\")\n",
    "    print(\"  ‚Ä¢ Working with large datasets (millions of rows)\")\n",
    "    print(\"  ‚Ä¢ Need complex JOINs across multiple tables\")\n",
    "    print(\"  ‚Ä¢ Performing set operations (UNION, INTERSECT, EXCEPT)\")\n",
    "    print(\"  ‚Ä¢ Using window functions for analytics\")\n",
    "    print(\"  ‚Ä¢ Implementing business logic with CASE statements\")\n",
    "    print(\"  ‚Ä¢ Need database-level performance optimization\")\n",
    "    \n",
    "    print(\"\\nüêº Use Pandas when:\")\n",
    "    print(\"  ‚Ä¢ Dataset fits comfortably in memory\")\n",
    "    print(\"  ‚Ä¢ Need statistical analysis (correlation, regression)\")\n",
    "    print(\"  ‚Ä¢ Data cleaning and transformation tasks\")\n",
    "    print(\"  ‚Ä¢ Creating visualizations\")\n",
    "    print(\"  ‚Ä¢ Machine learning feature engineering\")\n",
    "    print(\"  ‚Ä¢ Iterative data exploration and experimentation\")\n",
    "    \n",
    "    print(\"\\nüîÑ Best Practice: Hybrid Approach\")\n",
    "    print(\"  1. Use SQL for data extraction and initial processing\")\n",
    "    print(\"  2. Use pandas for analysis, statistics, and visualization\")\n",
    "    print(\"  3. Leverage each tool's strengths for optimal performance\")\n",
    "    \n",
    "    return \"SQL excels at data processing, pandas excels at analysis\"\n",
    "\n",
    "# Run the comparison\n",
    "comparison_insights = compare_sql_vs_pandas_approaches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Handling and Best Practices\n",
    "\n",
    "Production database applications require robust error handling and connection management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_error_handling():\n",
    "    \"\"\"\n",
    "    Demonstrate proper error handling techniques for database operations.\n",
    "    \"\"\"\n",
    "    print(\"üõ°Ô∏è Database Error Handling and Best Practices\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Example 1: Handling SQL syntax errors\n",
    "    print(\"\\n‚ùå Example 1: SQL Syntax Error Handling\")\n",
    "    try:\n",
    "        # Intentional syntax error\n",
    "        result = db.execute_query(\"\"\"\n",
    "            SELCT * FROM \"non_existent_table\"  -- Missing 'E' in SELECT\n",
    "            WHERE some_column = 'value'\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úÖ Caught SQL syntax error: {type(e).__name__}\")\n",
    "        print(f\"   Error message: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Example 2: Handling table/column not found\n",
    "    print(\"\\nüîç Example 2: Table/Column Not Found Error\")\n",
    "    try:\n",
    "        if sales_tables:\n",
    "            result = db.execute_query(f\"\"\"\n",
    "                SELECT customer_id, nonexistent_column \n",
    "                FROM \"{sales_tables[0]}\"\n",
    "                LIMIT 5\n",
    "            \"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úÖ Caught column error: {type(e).__name__}\")\n",
    "        print(f\"   Error message: {str(e)[:100]}...\")\n",
    "    \n",
    "    # Example 3: Parameterized queries (SQL injection prevention)\n",
    "    print(\"\\nüîí Example 3: Safe Parameterized Queries\")\n",
    "    \n",
    "    def safe_data_lookup(table_name, column_name, value):\n",
    "        \"\"\"\n",
    "        Safely query data using parameterized queries.\n",
    "        Note: Table and column names can't be parameterized, so validate them first.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Validate table exists\n",
    "            if table_name not in db_info:\n",
    "                raise ValueError(f\"Table {table_name} not found\")\n",
    "            \n",
    "            # Validate column exists\n",
    "            if column_name not in db_info[table_name]['columns']:\n",
    "                raise ValueError(f\"Column {column_name} not found in {table_name}\")\n",
    "            \n",
    "            # Use parameterized query for the value\n",
    "            query = f\"\"\"\n",
    "                SELECT COUNT(*) as record_count\n",
    "                FROM \"{table_name}\" \n",
    "                WHERE \"{column_name}\" = %(search_value)s\n",
    "            \"\"\"\n",
    "            result = db.execute_query(query, params={'search_value': value})\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Query failed: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # Test safe query\n",
    "    if sales_tables and state_cols:\n",
    "        # Get a real state value first\n",
    "        sample_query = f'SELECT DISTINCT \"{state_cols[0]}\" FROM \"{sales_tables[0]}\" WHERE \"{state_cols[0]}\" IS NOT NULL LIMIT 1'\n",
    "        try:\n",
    "            sample_state = db.execute_query(sample_query)\n",
    "            if len(sample_state) > 0:\n",
    "                test_value = sample_state.iloc[0, 0]\n",
    "                safe_result = safe_data_lookup(sales_tables[0], state_cols[0], test_value)\n",
    "                if len(safe_result) > 0:\n",
    "                    print(f\"‚úÖ Safe query returned {safe_result.iloc[0, 0]} records for '{test_value}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not test safe query: {e}\")\n",
    "    \n",
    "    # Example 4: Connection management with context managers\n",
    "    print(\"\\nüîå Example 4: Proper Connection Management\")\n",
    "    \n",
    "    class SafeDatabaseQuery:\n",
    "        \"\"\"\n",
    "        Context manager for safe database operations.\n",
    "        \"\"\"\n",
    "        def __init__(self, engine):\n",
    "            self.engine = engine\n",
    "            self.connection = None\n",
    "        \n",
    "        def __enter__(self):\n",
    "            self.connection = self.engine.connect()\n",
    "            return self.connection\n",
    "        \n",
    "        def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "            if self.connection:\n",
    "                self.connection.close()\n",
    "            if exc_type:\n",
    "                print(f\"‚ùå Database error occurred: {exc_type.__name__}: {exc_val}\")\n",
    "            return False  # Don't suppress exceptions\n",
    "    \n",
    "    # Use context manager for safe operations\n",
    "    try:\n",
    "        with SafeDatabaseQuery(db.engine) as conn:\n",
    "            result = pd.read_sql(\n",
    "                text(\"SELECT 'Connection test successful' as message\"), \n",
    "                conn\n",
    "            )\n",
    "            print(f\"‚úÖ Context manager query successful: {result.iloc[0, 0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Context manager caught error: {e}\")\n",
    "    \n",
    "    # Example 5: Data validation\n",
    "    print(\"\\n‚úÖ Example 5: Data Validation Best Practices\")\n",
    "    \n",
    "    def validate_query_result(df, expected_columns=None, min_rows=0):\n",
    "        \"\"\"\n",
    "        Validate query results meet business requirements.\n",
    "        \"\"\"\n",
    "        validations = []\n",
    "        \n",
    "        # Check if DataFrame is empty\n",
    "        if df.empty:\n",
    "            validations.append(\"‚ùå Query returned no data\")\n",
    "        else:\n",
    "            validations.append(f\"‚úÖ Query returned {len(df):,} rows\")\n",
    "        \n",
    "        # Check minimum row count\n",
    "        if len(df) < min_rows:\n",
    "            validations.append(f\"‚ö†Ô∏è Row count ({len(df)}) below minimum ({min_rows})\")\n",
    "        \n",
    "        # Check expected columns\n",
    "        if expected_columns:\n",
    "            missing_cols = set(expected_columns) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                validations.append(f\"‚ùå Missing columns: {missing_cols}\")\n",
    "            else:\n",
    "                validations.append(\"‚úÖ All expected columns present\")\n",
    "        \n",
    "        # Check for null values in key columns\n",
    "        if not df.empty:\n",
    "            null_counts = df.isnull().sum()\n",
    "            if null_counts.any():\n",
    "                validations.append(f\"‚ö†Ô∏è Null values found: {dict(null_counts[null_counts > 0])}\")\n",
    "            else:\n",
    "                validations.append(\"‚úÖ No null values detected\")\n",
    "        \n",
    "        return validations\n",
    "    \n",
    "    # Test validation\n",
    "    if sales_tables:\n",
    "        test_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM \"{sales_tables[0]}\"\n",
    "            LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            test_data = db.execute_query(test_query)\n",
    "            validations = validate_query_result(\n",
    "                test_data, \n",
    "                expected_columns=list(test_data.columns)[:3],  # Check first 3 columns\n",
    "                min_rows=5\n",
    "            )\n",
    "            \n",
    "            print(\"Query validation results:\")\n",
    "            for validation in validations:\n",
    "                print(f\"  {validation}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Validation test failed: {e}\")\n",
    "    \n",
    "    return validations\n",
    "\n",
    "# Run error handling demonstration\n",
    "error_handling_results = demonstrate_error_handling()\n",
    "\n",
    "print(\"\\nüìö Database Best Practices Summary:\")\n",
    "print(\"  üîí Always use parameterized queries to prevent SQL injection\")\n",
    "print(\"  üõ°Ô∏è Implement comprehensive error handling for all database operations\")\n",
    "print(\"  üîå Use connection context managers to ensure proper resource cleanup\")\n",
    "print(\"  ‚úÖ Validate query results before processing in business logic\")\n",
    "print(\"  üìä Log query performance for optimization opportunities\")\n",
    "print(\"  üîÑ Implement retry logic for transient connection issues\")\n",
    "print(\"  üìù Document query patterns and business logic for team maintenance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways and Next Steps\n",
    "\n",
    "### What We've Accomplished:\n",
    "\n",
    "1. **PostgreSQL Database Connection**\n",
    "   - Connected to Supabase cloud PostgreSQL database\n",
    "   - Established professional connection patterns with SQLAlchemy\n",
    "   - Implemented proper resource management and error handling\n",
    "\n",
    "2. **SQL Query Execution from Python**\n",
    "   - Basic data exploration and filtering\n",
    "   - Complex business intelligence with JOINs\n",
    "   - Advanced analytics with window functions and CTEs\n",
    "\n",
    "3. **Real-World Data Integration**\n",
    "   - Worked with actual Olist e-commerce and marketing datasets\n",
    "   - Adapted queries to real schema structures\n",
    "   - Handled data quality issues and missing values\n",
    "\n",
    "4. **Production-Ready Practices**\n",
    "   - Error handling and validation\n",
    "   - Parameterized queries for security\n",
    "   - Connection pooling and resource management\n",
    "\n",
    "### Business Value:\n",
    "\n",
    "- **Real-time Analysis**: Connect directly to live business systems\n",
    "- **Scalability**: Handle enterprise-scale datasets efficiently\n",
    "- **Performance**: Leverage database engines for heavy computation\n",
    "- **Security**: Proper authentication and query sanitization\n",
    "- **Collaboration**: Multiple analysts accessing the same cloud data source\n",
    "\n",
    "### When to Use SQL vs Pandas:\n",
    "\n",
    "**Use SQL for:**\n",
    "- Data extraction from large datasets\n",
    "- Complex joins across multiple tables\n",
    "- Window functions and analytical queries\n",
    "- Business logic implementation with CASE statements\n",
    "- Database-level performance optimization\n",
    "\n",
    "**Use Pandas for:**\n",
    "- Statistical analysis and modeling\n",
    "- Data cleaning and transformation\n",
    "- Visualization preparation\n",
    "- Machine learning feature engineering\n",
    "- Iterative data exploration\n",
    "\n",
    "### Next Session Preview:\n",
    "In our next sessions, we'll explore:\n",
    "- Advanced SQL patterns for business intelligence\n",
    "- Real-time data pipeline automation\n",
    "- Combining SQL analytics with interactive visualizations\n",
    "- Building automated reporting systems\n",
    "\n",
    "**üéâ You now have the fundamental skills to connect Python to cloud databases and perform enterprise-level data analysis!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practice Exercise\n",
    "\n",
    "**Your Challenge! üöÄ**\n",
    "\n",
    "**Business Scenario**: The Olist analytics team wants to understand the relationship between their marketing efforts and customer behavior. Your task is to create an analysis that bridges the marketing and sales datasets.\n",
    "\n",
    "**Your Task**: Create a comprehensive analysis that combines both datasets to answer business questions.\n",
    "\n",
    "**Requirements**:\n",
    "1. **Data Exploration**: Explore both olist_sales_data_set and olist_marketing_data_set\n",
    "2. **Schema Analysis**: Document the structure and relationships between datasets\n",
    "3. **Business Intelligence**: Create queries that provide actionable insights\n",
    "4. **Error Handling**: Implement proper error handling for your queries\n",
    "5. **Best Practices**: Use parameterized queries and validation\n",
    "\n",
    "**Specific Questions to Answer**:\n",
    "- What is the structure of each dataset?\n",
    "- How can these datasets be connected?\n",
    "- What insights can we derive about customer acquisition and behavior?\n",
    "- Which marketing channels or strategies show the most promise?\n",
    "\n",
    "**Deliverable**: A comprehensive analysis with SQL queries, data validation, and business insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your practice exercise solution here\n",
    "\n",
    "def comprehensive_business_analysis():\n",
    "    \"\"\"\n",
    "    Your challenge: Create a comprehensive analysis bridging marketing and sales data.\n",
    "    \n",
    "    Business Goal: Understand the relationship between marketing efforts and customer behavior.\n",
    "    \n",
    "    Implementation steps:\n",
    "    1. Explore both datasets thoroughly\n",
    "    2. Identify connection points between datasets\n",
    "    3. Create business intelligence queries\n",
    "    4. Validate results and handle errors\n",
    "    5. Generate actionable insights\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üéØ Comprehensive Business Analysis Challenge\")\n",
    "    print(\"üìä Goal: Bridge marketing and sales data for business insights\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Step 1: Dataset Exploration\n",
    "    print(\"\\nüìã Step 1: Dataset Structure Analysis\")\n",
    "    \n",
    "    # TODO: Explore olist_sales_data_set structure\n",
    "    # Think about: What columns are available? What do they represent?\n",
    "    \n",
    "    # TODO: Explore olist_marketing_data_set structure  \n",
    "    # Think about: How does this relate to sales data?\n",
    "    \n",
    "    # Step 2: Connection Analysis\n",
    "    print(\"\\nüîó Step 2: Identify Dataset Relationships\")\n",
    "    \n",
    "    # TODO: Find common columns or keys between datasets\n",
    "    # Think about: How can we join these datasets?\n",
    "    \n",
    "    # Step 3: Business Intelligence Queries\n",
    "    print(\"\\nüíº Step 3: Business Intelligence Analysis\")\n",
    "    \n",
    "    # TODO: Create queries that answer business questions:\n",
    "    # - Which marketing channels are most effective?\n",
    "    # - What's the customer journey from lead to purchase?\n",
    "    # - How do marketing efforts correlate with sales performance?\n",
    "    \n",
    "    # Step 4: Advanced Analytics\n",
    "    print(\"\\nüìà Step 4: Advanced Business Insights\")\n",
    "    \n",
    "    # TODO: Use advanced SQL features:\n",
    "    # - Window functions for trend analysis\n",
    "    # - CTEs for complex business logic\n",
    "    # - Statistical functions for performance metrics\n",
    "    \n",
    "    # Step 5: Validation and Error Handling\n",
    "    print(\"\\n‚úÖ Step 5: Data Validation and Quality Checks\")\n",
    "    \n",
    "    # TODO: Implement proper error handling and data validation\n",
    "    \n",
    "    # Step 6: Business Recommendations\n",
    "    print(\"\\nüéØ Step 6: Strategic Business Recommendations\")\n",
    "    \n",
    "    # TODO: Synthesize findings into actionable business insights\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"üí° Hints for Your Analysis:\")\n",
    "print(\"  ‚Ä¢ Start by examining the schema of both datasets\")\n",
    "print(\"  ‚Ä¢ Look for common identifiers (seller_id, customer_id, etc.)\")\n",
    "print(\"  ‚Ä¢ Use SQL JOINs to combine datasets where appropriate\")\n",
    "print(\"  ‚Ä¢ Focus on metrics that matter to business decision-makers\")\n",
    "print(\"  ‚Ä¢ Always validate your results and handle potential errors\")\n",
    "\n",
    "print(\"\\nüîç Analysis Framework:\")\n",
    "print(\"  1. Data Discovery: Understand what data is available\")\n",
    "print(\"  2. Relationship Mapping: How datasets connect\")\n",
    "print(\"  3. Business Metrics: What KPIs can we calculate?\")\n",
    "print(\"  4. Trend Analysis: How do metrics change over time?\")\n",
    "print(\"  5. Insights Generation: What actions should the business take?\")\n",
    "\n",
    "print(\"\\nüìä Expected Deliverables:\")\n",
    "print(\"  ‚Ä¢ Dataset structure documentation\")\n",
    "print(\"  ‚Ä¢ Relationship mapping between datasets\")\n",
    "print(\"  ‚Ä¢ Business intelligence SQL queries\")\n",
    "print(\"  ‚Ä¢ Data quality assessment\")\n",
    "print(\"  ‚Ä¢ Strategic recommendations based on findings\")\n",
    "\n",
    "# Uncomment to run your solution:\n",
    "# comprehensive_analysis_results = comprehensive_business_analysis()\n",
    "\n",
    "# Remember to clean up database connection when done\n",
    "# db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up database connection\n",
    "print(\"üîí Closing database connection...\")\n",
    "db.close()\n",
    "print(\"‚úÖ Session complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}